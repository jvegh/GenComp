
%https://creativecommons.org/licenses/by-nc/3.0/us/
%https://creativecommons.org/licenses/by-nc/2.5/hu/
%https://creativecommons.org/licenses/by-nc/3.0/us/deed.hu
 

@article{EinsteinSpecial:1905,
	author = {Albert Einstein},
	title = {{On the Electrodynamics of Moving Bodies}},
	journal = {Annalen der Physik (in German)},
	volume = {10},
	number = {17},
	year = {1905},
	pages = {891-921},
		doi = {10.1002/andp.19053221004}
	
}
 
@article{Minkowski:1908,
	author = {{Hermann Minkowski}},
	title = {{Die Grundgleichungen f\"ur die electromagnetischen Vorg\"ange in bewegten K\"orpern}},
	journal = {Nachrichten von der K\"oniglichen Gesellschaft der Wissenschaften zu G\"ottingen (in German)},
	year = {1908},
	pages = {53–111}
}

@article{NyquistTelegraphSpeed:1924,
	author = {Nyquist, H.},
    title = {{Certain Topics in Telegraph Transmission Theory}},
	journal = "A.I.E.E. Trans.", 
	volume = {47},
	year = {1928},
	page = {617}
}
   % title = “Certain Factors Affecting Telegraph Speed",
%    journal = ” Bell System Technical Journal", April 1924, p. 324;  

@article{HartleyNoise:1928,
	author = {Hartley, R. V. L.},
	title = {{Transmission of Information}},
	journal = {{Bell System Technical Journal}}, 
	year = {1928},
	page = {535}
}

 
@techreport{EDVACEckertMauchly,
author = {J. P. Eckert, Jr. and J. W. Mauchly},
title = {{Automatic High-Speed Computing: A Progress Report on the EDVAC}},
institution = {Moore School Library, University of Pennsylvania, Philadephia},
year = {1945},
number = {of Work under Contract No. W-670-ORD-4926, Supplement No 4},
howpublished = {\url{ https://archive.org/details/eckert-mauchly-prog-rep-edvac-moore-sch-1945}}
}


@ARTICLE{EDVACreport1945,	
	author={von Neumann, J.},	
	journal={IEEE Annals of the History of Computing}, 	
	title={{First draft of a report on the EDVAC}}, 	
	year={1993},	
	volume={15},	
	number={4},
	pages={27-75},	
	doi={10.1109/85.238389},
howpublished = {
\url{https://archive.org/details/vnedvac/}
}
}
 
 
 @ARTICLE{ShannonOriginal:1948,	
 	author={Shannon, C. E.},
 	journal={The Bell System Technical Journal},  	
 	title={A mathematical theory of communication},  	
 	year={1948},	
 	volume={27}, 	
 	number={3},	
 	pages={379-423},	
 	doi={10.1002/j.1538-7305.1948.tb01338.x}
 }
 
@article{ShannonBandwagon:1956,
	title={{The Bandwagon}},
	author={Shannon, C. E. },
	journal={IRE Transactions in 	Information Theory},
	year={1956},
	volume={2},
	issue={1},
	pages={3},
	howpublished={http://csc.ucdavis.edu/~cmg/papers/Shannon.IRETransInfoTh1956b.pdf}
}

@book{vonNeumannCollectedVI:1963,
	editor = {{Taub, A. H.}},
	title = {{Volume VI: Theory of Games, Astrophysics, Hydrodynamics and Meteorology}}, 
	booktitle = {{Collected Works}}, 
	publisher = {Oxford: Pergamon Press},
	year = {1963},
}

 
 
@book{vonNeumannBrain,
	author = {{von Neumann, John}},
	title = {{John von Neumann and the Origins of Modern Computing}}, 
	booktitle = {{The Computer and the Brain}}, 
	publisher = {Yale University Press},
	year = {2012},
	isbn = {978-0300181111}
}



@InProceedings{AmdahlSingleProcessor67,
	author= {Amdahl, G. M.},
	year = {1967},
	title = {{Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities}},
	booktitle = {AFIPS Conference Proceedings},
	volume = {30},
	pages = {483-485},
	doi = {10.1145/1465482.1465560}
}


@article{MemristorChua:1971,
	title={Memristor-The missing circuit element},
	author={Leon Ong Chua},
	journal={IEEE Transactions on Circuit Theory},
	year={1971},
	volume={18},
	pages={507-519}
}

@article{FirstComputerProgramKnuthNeumann:1970,
	author = {Knuth, Donald E.},
	title = {{Von Neumann's First Computer Program}},
	journal = {Computing Surveys},
	volume = {2},
	year = {1970},
	pages = {247-260},
}

@article{DijkstraSemaphore1972,
author = {E.W. Dijkstra},
title = {{Information Streams Sharing a Finite Buffer}},
journal = {Information Processing Letters},
volume = {1},
year = {1972},
pages = {179-180}
}

@article{MoreIsDifferent1972,
	author = {P. W. Anderson},
	title = {{More Is Different}},
	journal = {Science},
	volume = {177},
	year = {1972},
	pages = {393-396},
		doi = {10.1126/science. 177.4047.393}	
}

@article{EinsteinMinkowskiRelativity:1977,
	author={Pyenson, Lewis},
	year={1977},
	title={{Hermann Minkowski and Einstein's special theory of relativity}},
	journal={{Archive for History of Exact Sciences}},
	volume={17},
	issue={1},
	pages={71-95},
	doi={10.1007/BF00348403}
}

@article{BackusNeumannProgrammingStyle,
author = {J. Backus},
title = {{Can Programming Languages Be liberated from the von Neumann Style? A Functional Style and its Algebra of Programs}},
journal = {Communications of the ACM},
volume = {21},
year = {1978},
pages = {613-641}
}

@inproceedings{GodfreyGeographic:1982,
	author = {Godfrey, D. Michael},
	title = {Cartographic Computing Techology},
	booktitle = {Euro-Carto 1. Oxford, },
	year = {1981},
	location = {Oxford, UK},
	numpages = {8},
	howpublished = {\url{http://www,researchgate.net/profile/Micheal_Godfrey4/publicaltions}}

} 

@article{CacheMemories:1982,
 author = {Smith, Alan Jay},
 title = {Cache Memories},
 journal = {ACM Comput. Surv.},
 issue_date = {Sept. 1982},
 volume = {14},
 number = {3},
 month = sep,
 year = {1982},
 issn = {0360-0300},
 pages = {473--530},
 numpages = {58},
 url = {http://doi.acm.org/10.1145/356887.356892},
 doi = {10.1145/356887.356892},
 acmid = {356892},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{UsingCacheMemory:1983,
 author = {Goodman, James R.},
 title = {Using Cache Memory to Reduce Processor-memory Traffic},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 1983},
 volume = {11},
 number = {3},
 month = jun,
 year = {1983},
 issn = {0163-5964},
 pages = {124--131},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1067651.801647},
 acmid = {801647},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{NicolauFischer1984,
author = {Alexandru Nicolau and Joseph A. Fisher},
title = {{Measuring the parallelism available for very long instruction word architectures}},
journal = {IEEE Transactions on Computers},
volume = {C-33},
year = {1984},
number = {11},
pages = {968-976}
}

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{GodfreyArchitecture1986,
	author = {M. D. Godfrey},
	title = {{Innovation in Computational Architecture and Design}},
	journal = {ICL Technical Journal},
	volume = {5},
	year = {1986},
	pages ={18--31},
}

%	isbn = {0-387-18923-8},
%	url = {http://dl.acm.org/citation.cfm?id=52797.52802},
@inproceedings{IannucciIssues:1988,
	author = {Arvind and Iannucci, Robert A.},
	title = {Two Fundamental Issues in Multiprocessing},
	booktitle = {4th International DFVLR Seminar on Foundations of Engineering Sciences on Parallel Computing in Science and Engineering, Bonn, Germany},
	year = {1988},
	pages = {61--88},
	numpages = {28},
	acmid = {52802},
	publisher = {Springer-Verlag New York, Inc.},
	address = {New York, NY, USA},
} 

@article{Gustafson:1988,
	author = {Gustafson, John L.},
	title = {{Reevaluating Amdahl's Law}},
	journal = {Commun. ACM},
	issue_date = {May 1988},
	volume = {31},
	number = {5},
	year = {1988},
	pages = {532--533},
	numpages = {2},
	doi = {10.1145/42411.42415},
	acmid = {42415},
	publisher = {ACM},
	address = {New York, NY, USA},
} 



@book{RISCarchitecture:1989,
	editor = {Furber, Stephen Bo},
	series = {Advances in Computers, Volume 96},
	title = {{VLSI RISC Architecture and Organization}},
	year = {1989},
	publisher = {CRC Press},
}

@inbook{vonNeumannOrigins,
	author = {Aspray, W.},
	editor = {Bernard Cohen and William Aspray},
	title = {{John von Neumann and the Origins of Modern Computing}}, 
	booktitle = {{John von Neumann and the Origins of Modern Computing}}, 
	publisher = { MIT Press, Cambridge},
	pages = {34--48},
	year = {1990},
}

%  isbn = {0-89391-369-3},
@incollection{NeumannPreliminary:1989,
 author = {Burks, Arthur W. and Goldstine, Herman H. and von Neumann, John},
 chapter = {Preliminary Discussion of the Logical Design of an Electronic Computing Instrument (1946)},
 title = {Perspectives on the Computer Revolution},
 editor = {Pylyshyn, Zenon W. and Bannon, Liam J.},
 year = {1989},
 pages = {39--48},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=98326.98337},
 acmid = {98337},
 publisher = {Ablex Publishing Corp.},
 address = {Norwood, NJ, USA},
} 

% url = {http://doi.acm.org/10.1145/78607.78614},
% issn = {0001-0782},

@article{Karp:parallelperformance1990,
 author = {Karp, Alan H. and Flatt, Horace P.},
 title = {{Measuring Parallel Processor Performance}},
 journal = {Commun. ACM},
 issue_date = {May 1990},
 volume = {33},
 number = {5},
 month = may,
 year = {1990},
 pages = {539--543},
 numpages = {5},
 doi = {10.1145/78607.78614},
 acmid = {78614},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallel performance},
} 

  
@article{PriorityInheritance:1990,
	author = {{L. Sha and R. Rajkumar and J.P. Lehoczky}},
	title = {{Priority inheritance protocols: an approach to real-time synchronization}},
	journal = { IEEE Transactions on Computers },
	volume = {39},
	number = {9},
	month = sep,
	year = {1990},
	pages = {1175--1185},
	numpages = {1},
	doi = { 10.1109/12.57058},
	publisher = {IEEE},
} 

@misc{Ousterhout90,
author = {John K. Ousterhout},
title = {Why Aren't Operating Systems Getting Faster As Fast As Hardware?},
booktitle = {USENIX Summer Conference},
year = {1990},
howpublished = {\url{http://www.stanford.edu/~ouster/cgi-bin/papers/osfaster.pdf}},
				note = {Accessed: 2023-09-10}

}

@article{Sun:BetterPerformanceMetric1991,
 author = {Sun, Xian-He and Gustafson, John L.},
 title = {Paper: Toward a Better Parallel Performance Metric},
 journal = {Parallel Comput.},
 issue_date = {December, 1991},
 volume = {17},
 number = {10-11},
 month = dec,
 year = {1991},
 issn = {0167-8191},
 pages = {1093--1109},
 numpages = {17},
 url = {http://dx.doi.org/10.1016/S0167-8191(05)80028-6},
 doi = {10.1016/S0167-8191(05)80028-6},
 acmid = {1746189},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Parallel processing, parallel speedup, performance measurement, scaled speedup, sizeup}
} 

@techreport{DongarraPerformance:1992,
author = {Jack Dongarra},
title = {{Performance of Various Computers Using Standard Linear Equations Software}},
institution = {Computer Science Dept., Univ. of Tennessee, Knoxville},
year = {1992},
number = {TN 37996-1301},
}


@inproceedings{Mahlke:1992:LimitedRegisters,
author = {Mahlke, S.A. and Chen, W.Y. and Chang, P.P. and Hwu, W.-M.W.},
title = {Scalar program performance on multiple-instruction-issue processors with a limited number of registers},
booktitle = {Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences},
date = {7-10 Jan 1992},
year = {1992},
volume = {1},
pages = {34 - 44},
doi = {10.1109/HICSS.1992.183141},
}


@article{FateofEDVAC1993,
 author = {Williams, Michael R.},
 title = {{The Origins, Uses, and Fate of the EDVAC}},
 journal = {IEEE Ann. Hist. Comput.},
 issue_date = {January 1993},
 volume = {15},
 number = {1},
 month = jan,
 year = {1993},
 issn = {1058-6180},
 pages = {22--38},
 numpages = {17},
 doi = {10.1109/85.194089},
 acmid = {612518},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
} 
% url = {http://dx.doi.org/10.1109/85.194089},

@book{ModernRelativity:1993,
	author = { Anadijiban Das},
	title = {{The special theory of relativity: a mathematical exposition}},
	publisher = {Springer-Verlag New York},
	isbn = {978–0–387–94042–7},
	edition = {1},
	year = {1993}
}

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{GodfreyIEEE1993,
	author = {M. D. Godfrey and D. F. Hendry},
	title = {{The Computer as von Neumann Planned It}},
	journal = {IEEE Annals of the History of Computing},
	volume = {15},
	year = {1993},
	pages ={11-21},
	number = {1},	
	howpublished = {\url{https://archive.org/details/vonneumann_computer/}}
}

@article{PriorityInversion:1993,
	abstract = "A priority inversion occurs when a low-priority task causes the execution of a higher-priority task to be delayed. The possibility of priority inversions complicates the analysis of systems that use priority-based schedulers because priority inversions invalidate the assumption that a task can be delayed by only higher-priority tasks. This paper formalizes priority inversion and gives sufficient conditions as well as some new protocols for preventing priority inversions.",
	author = "Babaoglu, Ozalp and Marzullo, Keith and Schneider, Fred B.",
	day = "01",
	doi = "10.1007/BF01088832",
	issn = "1573-1383",
	journal = "Real-Time Systems",
	month = "Oct",
	number = "4",
	pages = "285–303",
	title = "A formalization of priority inversion",
	url = "https://doi.org/10.1007/BF01088832",
	volume = "5",
	year = "1993"
}
@MISC{WallLimitsOfILP:1993,
 author = {Wall, David W.},
 title = {Limits of Instruction-level Parallelism},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {Apr. 1991},
 volume = {25},
 number = {Special Issue},
 month = "apr",
 year = {1991},
 issn = {0163-5980},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106991},
 doi = {10.1145/106974.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
%howpublished = {\url{http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-93-6.pdf}}

% issn = {0018-9162},
%url = {http://dx.doi.org/10.1109/MC.1993.274941},
@article{ScalingParallel:1993,
 author = {Singh, Jaswinder Pal and Hennessy, John L. and Gupta, Anoop},
 title = {Scaling Parallel Programs for Multiprocessors: Methodology and Examples},
 journal = {Computer},
 issue_date = {July 1993},
 volume = {26},
 number = {7},
 year = {1993},
 pages = {42--50},
 numpages = {9},
 doi = {10.1109/MC.1993.274941},
 acmid = {619634},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 


@MISC{CharmIntroduction93,
	author = {L. V. Kale and S. Krisnan},
	title = {{CHARM++: A Portable Concurrent Object Oriented System Based on C++}},
	year = {1993},
	howpublished = {\url{http://http://charm.cs.illinois.edu/newPapers/93-02/paper.pdf}}
}


@INPROCEEDINGS{CellularNeuralNetworks93,
	author = {Valerio Cimagalli and Marco Balsi},
	title = {Cellular Neural Networks: A Review},
	booktitle = {Proc. 6th Italian Workshop on Parallel Architectures and Neural Networks, Vietri sul Mare, Italy},
	year = {1993},
	pages = {12--14},
	note = {ISBN: 9789814534604},
	publisher = {World Scientific}
}

@CONFERENCE{ThekkathException94,
author = {C. A. Thekkath and H. M. Levy},
title = {Hardware and software support for efficient exception handling},
booktitle = {ASPLOS-VI Proceedings},
year = {1994},
note = {DOI: 10.1145/195470.195515},
url = {http://www.thekkath.org/Documents/traps.pdf}
}


@article{Wulf:MemoryWall:1995,
	author = {Wm. A. Wulf and 	Sally A. McKee},
	title = {{Hitting the memory wall: implications of the obvious}},
	journal = {ACM SIGARCH Computer Architecture News},
	volume = {23},
	year = {1995},
	pages ={20-24},
	number = {1},
	doi = {10.1145/216585.216588}
}



@MISC{AmdahlVsGustafson96,
	author = {Shi,Yuan},
	title = {{Reevaluating Amdahl's Law and Gustafson's Law}},
	year = {1996},
	howpublished = {\url{https://www.researchgate.net/publication/ 228367369\_Reevaluating\_Amdahl's\_law\_and \_Gustafson's\_law}}
}



@book{CPPBoook:1997,
	author = {Stroustrup, Bjarne},
	title = {{The C++ Programming Language}},
	year = {1997},
	publisher = {Addison-Wesley},
}



@MISC{SpawnJoinArchitectureVishkin:1998,
	author = {Uzi Y. Vishkin},
	title = {{Spawn-join instruction set architecture for providing explicit multithreading }},
	year = {1998},
	howpublished = {\url{https://patents.google.com/patent/US6463527B1/en}}
}

@article{AddressEvent:1999,
	author = {K.   Boahen},
	title = {{Point-to-Point   Connectivity   Between   Neuromorphic   Chips   UsingAddress Events}},
	journal = {IEEE Trans. on Circuits and Systems Part},
	volume = {47},
	year = {2000},
	pages = {416-434},
}

@inproceedings{Pollack:ThermalWall:1999,
 author = {Pollack, Fred J.},
 title = {New Microarchitecture Challenges in the Coming Generations of CMOS Process Technologies (Keynote Address)(Abstract Only)},
 booktitle = {Proceedings of the 32Nd Annual ACM/IEEE International Symposium on Microarchitecture, Haifa, Israel},
 series = {MICRO 32},
 year = {1999},
 isbn = {0-7695-0437-X},
 page = {2},
 url = {http://cs.nyu.edu/courses/spring12/CSCI-GA.3033-012/lecture12.pdf},
 acmid = {320082},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

 
@inproceedings{ClockVsIPC2000,
	author = {Agarwal, Vikas and Hrishikesh, M. S. and Keckler, Stephen W. and Burger, Doug},
	title = {{Clock Rate Versus IPC: The End of the Road for Conventional Microarchitectures}},
	booktitle = {Proceedings of the 27th Annual International Symposium on Computer Architecture},
	series = {ISCA '00},
	year = {2000},
	isbn = {1-58113-232-8},
	location = {Vancouver, British Columbia, Canada},
	pages = {248--259},
	numpages = {12},
	url = {http://doi.acm.org/10.1145/339647.339691},
howpublished = {\url{www.cs.utexas.edu/~skeckler/pubs/isca00.pdf}},
	doi = {10.1145/339647.339691},
	acmid = {339691},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@phdthesis{borph2000,
	author = {Hayden Kwok-Hay So},
	title = {{BORPH: An Operating System for FPGA-Based Reconfigurable Computers}},
	school = {University of California, Berkeley},
	year = {2000},
}

@article{EPIC:2000,
	author = {Schlansker, M.S. and Rau, B.R.},
	title = {{EPIC: Explicitly Parallel Instruction Computing}},
	journal = {Computer},
	issue_date = {February 2000},
	volume = {33},
	number = {2},
	year = {2000},
	pages = {37--45},
	publisher = {IEEE},
} 
%	doi = {10.1109/2.820037},

@article{UsesAbusesAmdahl:2001,
 author = {Krishnaprasad, S.},
 title = {{Uses and Abuses of Amdahl's Law}},
 journal = {J. Comput. Sci. Coll.},
 issue_date = {December 2001},
 volume = {17},
 number = {2},
 month = dec,
 year = {2001},
 issn = {1937-4771},
 pages = {288--293},
 numpages = {6},
 publisher = {Consortium for Computing Sciences in Colleges},
 address = {USA},
} 
% url = {http://dl.acm.org/citation.cfm?id=775339.775386},
% acmid = {775386},

%     PHDTHESIS{citation_key,
%                required_fields [, optional_fields] }
%Required fields: author, title, school, year
%
%Optional fields: address, month, note, key
@PHDTHESIS{Akesson01,
author = {Johan F. Akesson},
title = {{Interprocess Communication Utilising Special Purpose Hardware}},
school = {Uppsala University},
year = {2001},
url = {http://www.it.uu.se/research/publications/lic/2001-016/2001-016.pdf}
}

@article{ComptonReconfigurableComputing:2002,
 author = {Compton, Katherine and Hauck, Scott},
 title = {Reconfigurable Computing: A Survey of Systems and Software},
 journal = {ACM Comput. Surv.},
 issue_date = {June 2002},
 volume = {34},
 number = {2},
 month = jun,
 year = {2002},
 issn = {0360-0300},
 pages = {171--210},
 numpages = {40},
 url = {http://doi.acm.org/10.1145/508352.508353},
 doi = {10.1145/508352.508353},
 acmid = {508353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Automatic design, FPGA, field-programmable, manual design, reconfigurable architectures, reconfigurable computing, reconfigurable systems},
} 

@inproceedings{ScratchpadMemory:2002,
 author = {Banakar, Rajeshwari and Steinke, Stefan and Lee, Bo-Sik and Balakrishnan, M. and Marwedel, Peter},
 title = {Scratchpad Memory: Design Alternative for Cache On-chip Memory in Embedded Systems},
 booktitle = {Proceedings of the Tenth International Symposium on Hardware/Software Codesign},
 series = {CODES '02},
 year = {2002},
 isbn = {1-58113-542-4},
 location = {Estes Park, Colorado},
 pages = {73--78},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/774789.774805},
 doi = {10.1145/774789.774805},
 acmid = {774805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inbook{TheoryOfInformation:2002,
	author = {Robert J. McEliece},
	title = {{The theory of information and coding}}, 
	publisher = {Cambridge University Press},
	year = {2002},
	isbn = {0-521-00095-5}
}


@ARTICLE{Vishkin:FineGrainedProgramming,
	AUTHOR = "Dorit Naishlos and Joseph Nuzman and Chau-Wen Tseng and Uzi Vishkin",
	TITLE = "{Towards a First Vertical Prototyping of an Extremely Fine-Grained Parallel Programming Approach}",
	JOURNAL = "Theory of Computing Systems",
	VOLUME = {36},
	NUMBER = {5},
	PAGES = {521-552},
	MONTH = "September",
	DOI = {10.1007/s00224-003-1086-6},
	YEAR = {2003}	}


@misc{IntelHyperTechnol:2003,
author = {Intel},
title = {{Intel®	Hyper-Threading Technology}},
year = {2003},
howpublished = {\url{https://www.utdallas.edu/~edsha/parallel/2010S/Intel-HyperThreads.pdf}}
}


@INPROCEEDINGS{Zhao2003,
	author = {Jianjun Zhao},
	title = {Data-flow-based unit testing of aspect-oriented programs},
	booktitle = {Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC 2003)},
	year = {2003},
	pages = {188--197},
	publisher = {IEEE Computer Society},
	doi = {http://ieeexplore.ieee.org/iel5/8813/27898/01245340.pdf?isnumber=27898\&prod=STD\&arnumber=1245340\&arnumber=1245340\&arSt=+188\&ared=+197\&arAuthor=Jianjun+Zhao},
	location = {Dallas, USA},
}


@ARTICLE{DynamicSleep:2003,
	AUTHOR = {J. W. Tschanz and S.G. Narendra and Y. Ye and B.A. Bloechel and S. Borkar and V. De},
	TITLE = "{Dynamic sleep transistor and body bias
	for active leakage power control of microprocessors}",
	JOURNAL = "IEEE Journal of Solid State Circuits",
	VOLUME = {38},
	NUMBER = {11},
	PAGES = {1838 - 1845},
	MONTH = "November",
	YEAR = {2003}	}


@book{LiljaComputerPerformance:2004,
	author = {David J. Lilja},
	title = {{Measuring Computer Performance: A practitioner's guide}},
	year = {2004},
	publisher = {Cambridge University Press},
	ISBN = {0-521-64105-5}
}


@misc{ARMdualport2004,
author = {ARM},
title = {{Dual port DMA-capable RAM example}},
year = {2004},
howpublished = {\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0213e/Cihbchdd.html}}
}


@ARTICLE{ThreeStateUnidirectional:2004,
	AUTHOR = {B. Linder and J. Garcia-Ojalvo and A. Neiman and L. Schimansky-Geier},
	TITLE = "{Effects of noise in excitable systems}",
	JOURNAL = "Physics reports",
	VOLUME = {392},
	NUMBER = {6},
	PAGES = {321-424},
	YEAR = {2004}
}

@article{MarkovianIonChannel:2005,
	title = {{Non-Markovian stochastic resonance: Three-state model of ion channel gating}},
	author = {Goychuk, Igor and H\"anggi, Peter and Vega, Jose L. and Miret-Art\'es, Salvador},
	journal = {Phys. Rev. E},
	volume = {71},
	issue = {6},
	pages = {061906},
	numpages = {11},
	year = {2005},
	month = {Jun},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevE.71.061906},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.71.061906}
}
 
@misc{DeBenedictis_zettaflops:2005,
author =  {{ERIK P. DeBenedictis}},
title = {{Petaflops, Exaflops, and Zettaflops for Science and Defense}},
year = {2005},
howpublished = {\url{http://debenedictis.org/erik/SAND-2005/SAND2005-2690-CUG2005-B.pdf}}
}



@MISC{Curriculum2005,
	author = {{ACM}},
	title = {{Computing Curricula 2005 - The Overview Report}},
	year = {2006},
	url ={http://www.acm.org/education/education/curric_vols/CC2005-March06Final.pdf}
}

@ARTICLE{SynergisticCell:2006,
	AUTHOR = "{M. Kistler and M. Perrone and F. Petrini}",
	TITLE = "{Cell multiprocessor communication network: Built for speed}",
	JOURNAL = "IEEE Micro",
	VOLUME = {26},
	NUMBER = {3},
	PAGES = {10-23},
	MONTH = "May",
	YEAR = {2006}	}


@book{PerformanceEvaluation2006,
	editor = {Lizy Kurian John and Lieven Eeckhout},
	title = {{Performance Evaluation and Benchmarking}},
	year = {2006},
	isbn = {0849336228, 9780849336225},
	publisher = {CRC, Taylor and Francis},
}


@book{QTProgramming,
author={Blanchette, Jasmin and Summerfield, Mark},
year = {2006},
title = {{A Brief History of Qt. C++ GUI Programming with Qt 4}},
publisher = {Prentice-Hall}
}


@book{HennessyArchitecture2007,
	author = {John L. Hennessy and  David A. Patterson},
	title = {{Computer Architecture: A Quantitative Approach}}, 
	publisher = {Morgan Kaufmann Publishers},
	isbn = {978-0-12-370490-0},
	year = {2007}
}


@article{ParallelPanic:2006,
	author = {O'Hanlon, C},
	title = {{A conversation with John Hennessy and David Patterson.}},
	journal = {Queue},
	volume = {10},
	number = {4},
	month = jan,
	year = {2006},
	pages = {14--22},
} 

@article{QuantumLogic:1936,
	author = {{Birkhoff, G.  and Von Neumann, J.}},
	title = {{The logic of quantum mechanics}},
	journal = {Annals of mathematics},
	year = {1936},
	pages = {823-843},
} 

@inproceedings{EmergingNonNeumann:2006,
	author = {{Poznanovic, D. S.}},
	title = {{The emergence of non-von Neumann processors}},
	booktitle = {{In International Workshop on Applied Reconfigurable Computing}},
	year = {2006},
	pages = {243-254},
	publisher = {Springer, Berlin, Heidelberg},
} 

@article{HWcontrolledthreadsMahesri:2007,
	author = {Mahesri, Aqeel and Wang, Nicholas J. and Patel, Sanjay J.},
	title = {{Hardware Support for Software Controlled Multithreading}},
	journal = {SIGARCH Comput. Archit. News},
	issue_date = {March 2007},
	volume = {35},
	number = {1},
	month = mar,
	year = {2007},
	issn = {0163-5964},
	pages = {3--12},
	numpages = {10},
	doi = {10.1145/1241601.1241606},
	acmid = {1241606},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@CONFERENCE{ReinventingComputing2007,
	author = {B. Smith},
	title = {Reinventing computing},
	booktitle = {International Supercomputing Conference},
	year = {2007},
	url = {http://www.cct.lsu.edu/~estrabd/LACSI2006/Smith.pdf}
}

@article{TooManyCores2007,
author = {Avi Mendelson},
title= {{How many cores are too many cores? }},
publisher = {IBM Research},
	year = {2007},
howpublished = {Last accessed July 7, 2017 [Online].
	\url{https://www.research.ibm.com/haifa/Workshops/compiler2007/present/ avi\_mendelson.pdf}
	}
}

@MISC{VishkinHome2007,
	author = {{Uzi Vishkin}},
	title = {{Explicit Multi-Threading (XMT): A PRAM-On-Chip Vision --
	A Desktop Supercomputer}},
	year = {2007},
	howpublished = {Last accessed Dec. 12, 2015 [Online].
	\url{http://www.umiacs.umd.edu/users/vishkin/XMT/index.shtml}}
}


@inbook{Hartenstein07,
author = {Reiner Hartenstein},
editor = {K.L.M. Bertels and S.D. Cotofana and G.N. Gaydadjiev and K.G.W. Goossens and S. Hamdioui and B.H.H. Juurlink and A.J. van Genderen and S. Wong},
title = {The Future of Computing, essays in memory of Stamatis Vassiliadis}, 
publisher = {Computer Engineering Laboratory, TU Delft},
year = {2007}, 
url = {http://hartenstein.de/4SlashdotDec07.pdf}
}

@inproceedings{Borkar:ThousandCore:2007,
author = {Shekhar Borkar},
title = {{Thousand Core Chips—A Technology Perspective}},
 booktitle = {{Proceedings of the ACM/IEEE 44th Design Automation Conf.(DAC)}},
series = {{DAC'07}},
year = {2007},
publisher = {ACM Press},
pages = {746-749},
}


@Article{AmdalsLaw-Paul2007,
author="Paul, JoAnn M.
and Meyer, Brett H.",
title="{Amdahl's Law Revisited for Single Chip Systems}",
journal="International Journal of Parallel Programming",
year="2007",
month="Apr",
day="01",
volume="35",
number="2",
pages="101--123",
abstract="Amdahl's Law is based upon two assumptions -- that of boundlessness and homogeneity -- and so it can fail when applied to single chip heterogeneous multiprocessor designs, and even microarchitecture. We show that a performance increase in one part of the system can negatively impact the overall performance of the system, in direct contradiction to the way Amdahl's Law is instructed. Fundamental assumptions that are consistent with Amdahl's Law are a heavily ingrained part of our computing design culture, for research as well as design. This paper points in a new direction. We motivate that emphasis should be made on holistic, system level views instead of divide and conquer approaches. This, in turn, has relevance to the potential impacts of custom processors, system-level scheduling strategies and the way systems are partitioned. We realize that Amdahl's Law is one of the few, fundamental laws of computing. However, its very power is in its simplicity, and if that simplicity is carried over to future systems, we believe that it will impede the potential of future computing systems.",
}
% issn="1573-7640",
% doi="10.1007/s10766-006-0028-8",
% url="https://doi.org/10.1007/s10766-006-0028-8"


@inproceedings{Tsafrir:2007,
 author = {Tsafrir, Dan},
 title = {The Context-switch Overhead Inflicted by Hardware Interrupts (and the Enigma of Do-nothing Loops)},
 booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science, San Diego, California},
 series = {ExpCS '07},
 year = {2007},
 isbn = {978-1-59593-751-3},
 articleno = {4},
 pages = {3--3},
 acmid = {1281704},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock interrupts, operating system noise, ticks},
} 
% url = {http://doi.acm.org/10.1145/1281700.1281704},
% doi = {10.1145/1281700.1281704},

@inproceedings{armContextSwitching:2007,
	author = {David, Francis M. and Carlyle, Jeffrey C. and Campbell, Roy H.},
	title = {{Context Switch Overheads for Linux on ARM Platforms}},
	booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science, San Diego, California},
	series = {ExpCS '07},
	year = {2007},
	isbn = {978-1-59593-751-3},
	articleno = {3},
	url = {http://doi.acm.org/10.1145/1281700.1281703},
	doi = {10.1145/1281700.1281703},
	acmid = {1281703},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {context switch overhead, operating system},
} 



@article{EnergyProportional2007,
author = {Luiz André Barroso and Urs Hölzle},
title = {{The Case for Energy-Proportional Computing}},
journal = {Computer},
volume = {40},
year = {2007},
pages = {33--37}
}


@article{Congy:CoreSpilling:2007,
	author = {J. Congy and et al},
	title = {{Accelerating Sequential Applications on CMPs Using Core Spilling}},
	journal = {Parallel and Distributed Systems},
	volume = {18},
	year = {2007},
	pages = {1094--1107},
} 

@article{MissingMemristor:2008,
	author={Strukov, Dmitri B. and Snider, Gregory S. and Stewart, Duncan R. and Williams, R. Stanley},
	year ={2008},
	title = {{The missing memristor found}},
	journal = {Nature},
	volume = {453/7191},
	pages = {80--83},
}


@CONFERENCE{ComputingDensity2008,
author = {J. Williams and A. D. George and J. Richardson and K. Gosrani and S. Suresh},
title = {Computational density of fixed and reconfigurable multi-core devices for application acceleration},
booktitle = {Proceedings of Reconfigurable Systems Summer Institute, Urbana, IL, Jul. 2008.},
year = {2008}
}

@INPROCEEDINGS{VishkinFPGA2008,
	author = {	Xingzhi Wen	and	Uzi Vishkin},
	title = "{FPGA-based prototype of a pram-on-chip processor}",
	booktitle = {Proceedings of the 5th conference on Computing frontiers, CF'08},
	year = {2008},
	address = {New York, NY, USA},
	pages = {55-66},
	publisher = {ACM},
	DOI = {10.1145/1366230.1366240},
	ISBN={978-1-60558-077-7}
}


@misc{IntelHyper:2008,
author = {Intel},
title = {{Hyper-Threading Technology Architecture and Microarchitecture}},
year = {2008},
howpublished = {\url{http://noggin.intel.com/technology-journal/2002/61/hyper-threading-technology}}
}


@article{HillMulticoreAmdahl2008,
	author = {Hill, M. D.  and Marty, M. R. },
	title = {{Amdahl's Law in the Multicore Era}},
	journal = {IEEE Computer },
	volume = {41},
	year = {2008},
	pages ={33-38},
	number = {7}
}



@inproceedings{Qthreads:2008,
	author = {{Wheeler, K.B.  and Murphy, R.C. and Thain, D.}},
	title = {{Qthreads: An API for programming with millions of lightweight threads}},
	booktitle = {Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on},
	year = {2008},
	pages = {1--8},
	doi = {10.1109/IPDPS.2008.4536359},
}


@inproceedings{ChandyParallelism:2009,
author = {Chandy, John A. and Singaraju, Janardhan},
title = {Hardware Parallelism vs. Software Parallelism},
booktitle = {Proceedings of the First USENIX Conference on Hot Topics in Parallelism},
series = {HotPar'09},
year = {2009},
location = {Berkeley, California},
pages = {2--2},
numpages = {1},
acmid = {1855593},
publisher = {USENIX Association},
address = {Berkeley, CA, USA},
} 

@article{Minkowski100:2008,
	author = {Scott Walter},
	title = {{Hermann Minkowski and the scandal of spacetime}},
	journal = {ESI News},
	volume = {1},
	number = {3},
	year = {2008},
	pages = {6--8},
	howpublished = {https://halshs.archives-ouvertes.fr/halshs-00319209/document},
} 

@article{ForceWithFiniteSpeed:2008,
	doi = {10.1209/0295-5075/84/20002},
	year = {2008},
	volume = {84},
	number = {2},
	pages = {20002},
	author = {Rousseaux, G.},
	title = {On the electrodynamics of Minkowski at low velocities},
	journal = {Europhysics Letters},
	abstract = {The Galilean constitutive equations for the electrodynamics of moving media are derived for the first time. They explain all the historic and modern experiments which were interpreted so far in a relativistic framework assuming the constant light celerity principle. Here, we show the latter to be sufficient but not necessary.}
}
@article{WilliamsRoofline:2009,
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
	journal = {Commun. ACM},
	issue_date = {April 2009},
	volume = {52},
	number = {4},
	year = {2009},
	issn = {0001-0782},
	pages = {65--76},
	publisher = {ACM},
	address = {New York, NY, USA},
} 
%	url = {http://doi.acm.org/10.1145/1498765.1498785},
%doi = {10.1145/1498765.1498785},
%acmid = {1498785},

@inbook{Ferreira09,
title={RTOS Hardware Coprocessor Implementation in VHDL},
url={http://aveiro.academia.edu/documents/0054/9267/OReK\_CoProcessor.pdf},
booktitle={RTOS Hardware Coprocessor Implementation in VHDL},
publisher={Intellectual Property / Embedded Systems Conference (IP/ESC)},
author={Ferreira, Carlos Miguel and Oliveira, Arnaldo S R}, year={2009}, pages={6}
}

@ARTICLE{AsanovicParallelCACM:2009,
	AUTHOR = "Krste Asanovic and Rastislav Bodik and James Demmel and Tony
	Keaveny and Kurt Keutzer and John Kubiatowicz and Nelson Morgan
	and David Patterson and Koushik Sen and John Wawrzynek and
	David Wessel and Katherine Yelick",
	lab = {University of California, Berkeley},
	TITLE = "{A View of the Parallel Computing Landscape}",
	JOURNAL = "Comm. ACM",
	VOLUME = {52},
	NUMBER = {10},
	PAGES = {56-67},
	YEAR = {2009}	}

% isbn = {978-1-60558-406-5},
%url = {http://doi.acm.org/10.1145/1508244.1508246},
%doi = {10.1145/1508244.1508246},
@inproceedings{Gebhart:TRIPS:2009,
 author = {Gebhart, Mark and Maher, Bertrand A. and Coons, Katherine E. and Diamond, Jeff and Gratz, Paul and Marino, Mario and Ranganathan, Nitya and Robatmili, Behnam and Smith, Aaron and Burrill, James and Keckler, Stephen W. and Burger, Doug and McKinley, Kathryn S.},
 title = {An Evaluation of the TRIPS Computer System},
 booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems, Washington, DC, USA},
 series = {ASPLOS XIV},
 year = {2009},
 pages = {1--12},
 numpages = {12},
 acmid = {1508246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {trips},
} 


@BOOK{ArtOfConcurrencyBook:2009,
	author = {Clay Breshears},
	title = {The Art of Concurrency},
	year = {2009},
	publisher = {O'Reilly Media, Inc.},
	isbn = {9780596802424}
}

@BOOK{RussellNorvigAIBook:2009,
	author = {Russell, Stuart J. and Norvig, Peter },
	title = {Artificial Intelligence: A Modern Approach},
	year = {2009},
	publisher = {Upper Saddle River, New Jersey: Prentice Hall},
	isbn = {ISBN 978-0-13-604259-4}
}

@BOOK{EnglanderBook,
	author = {Irv Englander},
	title = {The Architecture of COMPUTER HARDWARE, SYSTEMS SOFTWARE AND NETWORKING
	An Information Technology Approach},
	year = {2010},
	publisher = {John Wiley \& Sons, Inc. },
	edition = {Fourth},
	place = {Lawrence, Kansas, USA},
	isbn = {978-0-470-40028-9}
}

@BOOK{SystemCBook:2010,
	author = {David C. Black and Jack Donovan and Bill Bunton and Anna Keist},
	title = {SystemC: From the Ground Up},
	year = {2010},
	publisher = {Springer},
	edition = {second},
	place = {New York Dordrecht Heidelberg London},
	isbn = {978-0-387-69957-8}
}

@inbook{Hartenstein2010,
author = {Hartenstein, R.},
title = {{The Grand Challenge To Reinvent Computing}}, 
publisher = {XXX Congress of the SBC},
year = {20-23 July, 2010},
place = {Belo Horizonte, MG, Brazil}, 
url = {http://hartenstein.de/4SlashdotDec07.pdf}
}


%% http://www.cslab.pepperdine.edu/warford/cosc330/

@article{ParallelEfficiency:Orii:2010,
title = "Metrics for evaluation of parallel efficiency toward highly parallel processing ",
journal = "Parallel Computing ",
volume = "36",
number = "1",
pages = "16 - 25",
year = "2010",
note = "",
issn = "0167-8191",
doi = "http://dx.doi.org/10.1016/j.parco.2009.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167819109001227",
author = "Shigeo Orii",
keywords = "Parallel performance evaluation",
abstract = "I studied novel performance metrics of parallel processing efficiency, determined simply by measuring timing data in a parallel process. The metrics, which consider load-imbalance, are called the parallel efficiency, load-balancing, and parallel impediment metrics. The relationships between these three can be unified into one expression that makes it possible to conduct detailed performance evaluations of parallel processing. The parallel efficiency metric also makes it possible to estimate the acceleration potential of the parallel process. "
}


@BOOK{Warford2010,
	author = {Stanley J. Warford},
	title = {Computer Systems},
	year = {2010},
	publisher = {Jones and Bartlett},
	place = {Sudbury, Massachusetts, USA},
	isbn = { 0-7637-7144-9}
}

%http://www.soos-project.eu/
@MISC{SoOS:2010,
author = {{S(o)OS~project}},
lab={{S(o)OS~project}},
title = {Resource-independent execution support on exa-scale systems},
year = {2010},
howpublished = {\url{http://www.soos-project.eu/index.php/related-initiatives} (Accessed on Dec 14, 2020)}
}

@article{MemristorNanoscale:2010,
	author = {Mazumder, Pinaki and Lu, Wei},
	year = {2010},
	doi = {10.1021/nl904092h},
	title = {Nanoscale Memristor Device as Synapse in Neuromorphic Systems},
	journal = {Nano Letters},
	pages = {1297-1301},
	volume = {10},
	issue = {4},
	publisher = {American Chemical Society},
	abstract = {A memristor is a two-terminal electronic device whose conductance can be precisely modulated by charge or flux through it. Here we experimentally demonstrate a nanoscale silicon-based memristor device and show that a hybrid system composed of complementary metal−oxide semiconductor neurons and memristor synapses can support important synaptic functions such as spike timing dependent plasticity. Using memristors as synapses in neuromorphic circuits can potentially offer both high connectivity and high density required for efficient computing.}
}


@misc{InefficiencyCPUs:2010,
 author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
 title = {Understanding Sources of Inefficiency in General-purpose Chips},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture, Saint-Malo, France},
 series = {ISCA '10},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 pages = {37--47},
 numpages = {11},
 howpublished = {\url{http://doi.acm.org/10.1145/1815961.1815968}},
 doi = {10.1145/1815961.1815968},
 acmid = {1815968},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASIC, chip multiprocessor, customization, energy efficiency, h.264, high performance, tensilica},
} 

@inproceedings{ResourceAwareCompilerVishkin:2010,
 author = {Caragea, George C. and Tzannes, Alexandros and Keceli, Fuat and Barua, Rajeev and Vishkin, Uzi},
 title = {Resource-Aware Compiler Prefetching for Many-Cores},
 booktitle = {Proceedings of the 2010 Ninth International Symposium on Parallel and Distributed Computing},
 series = {ISPDC '10},
 year = {2010},
 isbn = {978-0-7695-4120-4},
 pages = {133--140},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/ISPDC.2010.16},
 doi = {10.1109/ISPDC.2010.16},
 acmid = {1848308},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {parallel architectures, optimizing compilers},
} 

@inproceedings{Lee:GPUvsCPU2010,
 author = {Lee, Victor W. and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
 title = {{Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU}},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 series = {ISCA '10, Saint-Malo, France},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 pages = {451--460},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1815961.1816021},
 doi = {10.1145/1815961.1816021},
 acmid = {1816021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cpu architecture, gpu architecture, performance analysis, performance measurement, software optimization, throughput computing},
} 


@article{ReconfigurableMulticoresWilliams:2010,
 author = {Williams, Jason and Massie, Chris and George, Alan D. and Richardson, Justin and Gosrani, Kunal and Lam, Herman},
 title = {Characterization of Fixed and Reconfigurable Multi-Core Devices for Application Acceleration},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {November 2010},
 volume = {3},
 number = {4},
 month = {nov},
 year = {2010},
 issn = {1936-7406},
 pages = {19:1--19:29},
 articleno = {19},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/1862648.1862649},
 doi = {10.1145/1862648.1862649},
 acmid = {1862649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Computational density per watt, internal memory bandwidth},
} 

@ARTICLE{Ungerer_MERASA:2010,
	AUTHOR = "T. Ungerer",
	TITLE = "{Multi-core execution of hard real-time applications supporting analyzability}",
	JOURNAL = "IEEE Micro",
	VOLUME = {99},
	PAGES = {66--75},
	YEAR = {2010}
}

@misc{ScienceExascaleRace:2010,
author = {{US DOE}},
title = {{The Opportunities and Challenges of Exascale Computing}},
year = {2010},
howpublished = {\url{https://science.energy.gov/~/media/ascr/ascac/pdf/reports/Exascale\_subcommittee\_report.pdf}},
				note = {Accessed: 2023-09-10}

}


@article{CriticalSectionAmdahlEyerman:2010,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {{Modeling Critical Sections in Amdahl's Law and Its Implications for Multicore Design}},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 2010},
 volume = {38},
 number = {3},
 month = jun,
 year = {2010},
 pages = {362--370},
 numpages = {9},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Amdahl's law, analytical performance modeling, critical sections, synchronization},
} 


@InProceedings{InformationToKnowledge:2011,
	author="Carbone, John N.
	and Crowder, James A.",
	editor="Suh, Sang C.
	and Gurupur, Varadraj P.
	and Tanik, Murat M.",
	title="THE GREAT MIGRATION: INFORMATION CONTENT TO KNOWLEDGE USING COGNITION BASED FRAMEWORKS",
	booktitle="Biomedical Engineering",
	year="2011",
	publisher="Springer New York",
	address="New York, NY",
	pages="17--46",
	abstract="Research shows that generating new knowledge is accomplished via natural human means: mental insights, scientific inquiry process, sensing, actions, and experiences, while context is information, which characterizes the knowledge and gives it meaning. This knowledge is acquired via scientific research requiring the focused development of an established set of criteria, approaches, designs, and analysis, as inputs into potential solutions.",
	isbn="978-1-4614-0116-2"
}

@book{
	InformationTheoryForMemoryless:2011,
	author = {Imre Csisz\'ar and J\'anos K\"orner},
	title = {{Information Theory: Coding Theorems for Discrete Memoryless Systems}},
	publisher = {Cambridge Universiy Press},
	year = {2011}
}



@INPROCEEDINGS{TrueNorth:2016,
	author={J. {Sawada~et~al}},
	booktitle={SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Salt Lake City, UT, USA; 13-18 Nov. 2016)},
	title={{TrueNorth Ecosystem for Brain-Inspired Computing: Scalable Systems, Software, and Applications}}, year={2016}, volume={}, number={}, pages={130-141},
} 
%and F. {Akopyan} and A. S. {Cassidy} and B. {Taba} and M. V. {Debole} and P. {Datta} and R. %{Alvarez-Icaza} and A. {Amir} and J. V. {Arthur} and A. {Andreopoulos} and R. {Appuswamy} and H. %{Baier} and D. {Barch} and D. J. {Berg} and C. {Di Nolfo} and S. K. {Esser} and M. {Flickner} %and T. A. {Horvath} and B. L. {Jackson} and J. {Kusnitz} and S. {Lekuch} and M. {Mastro} and T. %{Melano} and P. A. {Merolla} and S. E. {Millman} and T. K. {Nayak} and N. {Pass} and H. E. %{Penner} and W. P. {Risk} and K. {Schleupen} and B. {Shaw} and H. {Wu} and B. {Giera} and A. T. %{Moody} and N. {Mundhenk} and B. C. {Van Essen} and E. X. {Wang} and D. P. {Widemann} and Q. %{Wu} and W. E. {Murphy} and J. K. {Infantolino} and J. A. {Ross} and D. R. {Shires} and M. M. %{Vindiola} and R. {Namburu} and D. S. {Modha}




@ARTICLE{NeuralScaling2017,
	AUTHOR={Ippen, Tammo and Eppler, Jochen M. and Plesser, Hans E. and Diesmann, Markus},   
	TITLE={{Constructing Neuronal Network Models in Massively Parallel Environments}},      
	JOURNAL={Frontiers in Neuroinformatics},      
	VOLUME={11},      
	PAGES={30},     
	YEAR={2017},      
	ABSTRACT={Recent advances in the development of data structures to represent spiking neuron network models enable us to exploit the complete memory of petascale computers for a single brain-scale network simulation. In this work, we investigate how well we can exploit the computing power of such supercomputers for the creation of neuronal networks. Using an established benchmark, we divide the runtime of simulation code into the phase of network construction and the phase during which the dynamical state is advanced in time. We find that on multi-core compute nodes network creation scales well with process-parallel code but exhibits a prohibitively large memory consumption. Thread-parallel network creation, in contrast, exhibits speedup only up to a small number of threads but has little overhead in terms of memory. We further observe that the algorithms creating instances of model neurons and their connections scale well for networks of ten thousand neurons, but do not show the same speedup for networks of millions of neurons. Our work uncovers that the lack of scaling of thread-parallel network creation is due to inadequate memory allocation strategies and demonstrates that thread-optimized memory allocators recover excellent scaling. An analysis of the loop order used for network construction reveals that more complex tests on the locality of operations significantly improve scaling and reduce runtime by allowing construction algorithms to step through large networks more efficiently than in existing code. The combination of these techniques increases performance by an order of magnitude and harnesses the increasingly parallel compute power of the compute nodes in high-performance clusters and supercomputers.}
}

%	URL={https://www.frontiersin.org/article/10.3389/fninf.2017.00030},       
% DOI={10.3389/fninf.2017.00030},      
% ISSN={1662-5196},   

% issn = {0163-5964},
% url = {http://doi.acm.org/10.1145/1816038.1816011},
% doi = {10.1145/1816038.1816011},
% acmid = {1816011},

@article{CognitiveComputationPICCININI:2017,
	title = {{Computation vs. information processing: why their difference matters to cognitive science}},
	journal = {Studies in History and Philosophy of Science Part A},
	volume = {41},
	number = {3},
	pages = {237-246},
	year = {2010},
	note = {Computation and cognitive science},
	issn = {0039-3681},
	doi = {https://doi.org/10.1016/j.shpsa.2010.07.012},
	url = {https://www.sciencedirect.com/science/article/pii/S0039368110000440},
	author = {Gualtiero Piccinini and Andrea Scarantino},
	keywords = {Computation, Information processing, Computationalism, Computational theory of mind, Cognitivism},
	abstract = {Since the cognitive revolution, it has become commonplace that cognition involves both computation and information processing. Is this one claim or two? Is computation the same as information processing? The two terms are often used interchangeably, but this usage masks important differences. In this paper, we distinguish information processing from computation and examine some of their mutual relations, shedding light on the role each can play in a theory of cognition. We recommend that theorists of cognition be explicit and careful in choosing notions of computation and information and connecting them together.}
}
	
	
	

@inproceedings{SpiNNakerNeuromimetic:2010,
	author = {Rast, A. D. and Jin, X. and Galluppi, F. and Plana, L. A. and Patterson, C. and Furber, S.},
	title = {{Scalable Event-driven Native Parallel Processing: The SpiNNaker Neuromimetic System}},
	booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers, Bertinoro, Italy},
	series = {CF '10},
	year = {2010},
	isbn = {978-1-4503-0044-5},
	pages = {21--30},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/1787275.1787279},
	doi = {10.1145/1787275.1787279},
	acmid = {1787279},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {asynchronous, event-driven, universal neural processor},
} 


@article{GaluzziInstructionSetExtension:2011,
 author = {Galuzzi, Carlo and Bertels, Koen},
 title = {The Instruction-Set Extension Problem: A Survey},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {May 2011},
 volume = {4},
 number = {2},
 month = may,
 year = {2011},
 issn = {1936-7406},
 pages = {18:1--18:28},
 articleno = {18},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/1968502.1968509},
 doi = {10.1145/1968502.1968509},
 acmid = {1968509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {HW/SW codesign, Instruction-set, customization, instruction generation, instruction selection, instruction-set extension, reconfigurable architecture},
} 


@BOOK{ReconfigurableComputing2011,
	author = {J. M. P. Cardoso and M. H\"ubner},
	title = {Reconfigurable Computing},
	year = {2011},
	publisher = {Springer, Inc. },
	place = {Lawrence, Kansas, USA},
	isbn = {978-1-4614-0060-8},
	doi= {10.1007/978-1-4614-0061-5}
}


@article{YanPingXian11,
author = {Yan Li and Ping-Ping Gu and Xian-Xhan Wang},
title = {{The implementation of Semaphore Management in Hardware Real Time Operating System}},
journal = {Information Technology Journal},
volume = {10},
year = {2011},
pages = {158-163},
number = {1}
}

@article{Borkar:FutureOfUp:2011,
author = {Shekhar Borkar and Andrew A. Chien},
title = {{The Future of Microprocessors}},
journal = {{Communications of the ACM}},
volume = {54},
year = {2011},
pages = {67-77},
number = {5}
}



@CONFERENCE{NASAhyper2011,
	author= {{Saini, S. and Haoqiang Jin and Hood, R. and Barker, D. and	more authors}},
	year = {2011},
	title = {{The impact of hyper-threading on processor resource utilization in production applications}},
	booktitle = { 2011 18th International Conference on High Performance Computing (HiPC) (Bengaluru, India; 18-21 Dec. 2011)},
	pages = {1 -- 10},
	doi = {10.1109/HiPC.2011.6152743}
}



@misc{ARM:big.LITTLE:2011,
	author = {{ARM}},
	title = {{big.LITTLE technology}},
	year = {2011},
	url = {https://developer.arm.com/technologies/big-little}
}


@ARTICLE{Vishkin:AbstractionCACM,
	AUTHOR = "Uzi Vishkin",
	TITLE = "{Using Simple Abstraction to Reinvent Computing for Parallelism}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {54},
	NUMBER = {1},
	PAGES = {75-85},
	MONTH = "January",
	YEAR = {2011}	}

@INPROCEEDINGS{XMTtoolchain2011,
	author = {Keceli, F. and Tzannes, A. and Caragea G.C. and Barua, R. and et al},
	title = "{Toolchain for Programming, Simulating and Studying the XMT Many-Core Architecture}",
	booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on},
	year = {2011},
	publisher = {IEEE},
	DOI = {10.1109/IPDPS.2011.270},
	ISBN={978-1-61284-425-1}
}


@incollection{InvasiveComputing:2011,
	author = {J. Teich and J. Henkel and A. Herkersdorf and D. Schmitt-Landsiedel and W. Schr\"oder-Preikschat and G. Snelting},
	booktitle   = {Multiprocessor System-on-Chip},
	title = {{Invasive Computing: An Overview}},
	editor      = {M. H\"ubner and		J. Becker},
	year = {2011},
	isbn = {978-1-4419-6459-5},
	pages = {241-268},
	publisher   = {Springer},
}
%%howpublished = {\url{http://invasic.informatik.uni-erlangen.de/en/index.php}}

@ARTICLE{ComputingPerformance:2011,
	author = {S.~H.~Fuller and  L.~I.~Millett},
	title = {{Computing Performance: Game Over or Next Level?}},
	journal = {Computer},
	volume = {44},
	Issue = {1},
	pages = {31-38},
	year = {2011}
}

@INPROCEEDINGS{EfficiacyAPU:2011,
author={M. Daga and A. M. Aji and W. c. Feng},
booktitle={2011 Symposium on Application Accelerators in High-Performance Computing},
title={{On the Efficacy of a Fused CPU+GPU Processor (or APU) for Parallel Computing}},
year={2011},
volume={},
number={},
pages={141-149},
abstract={The graphics processing unit (GPU) has made significant strides as an accelerator in parallel computing. However, because the GPU has resided out on PCIe as a discrete device, the performance of GPU applications can be bottlenecked by data transfers between the CPU and GPU over PCIe. Emerging heterogeneous computing architectures that "fuse" the functionality of the CPU and GPU, e.g., AMD Fusion and Intel Knights Ferry, hold the promise of addressing the PCIe bottleneck. In this paper, we empirically characterize and analyze the efficacy of AMD Fusion, an architecture that combines general-purpose x86 cores and programmable accelerator cores on the same silicon die. We characterize its performance via a set of micro-benchmarks (e.g., PCIe data transfer), kernel benchmarks(e.g., reduction), and actual applications (e.g., molecular dynamics). Depending on the benchmark, our results show that Fusion produces a 1.7 to 6.0-fold improvement in the data-transfer time, when compared to a discrete GPU. In turn, this improvement in data-transfer performance can significantly enhance application performance. For example, running a reduction benchmark on AMD Fusion with its mere 80 GPU cores improves performance by 3.5-fold over the discrete AMD Radeon HD 5870 GPU with its 1600 more powerful GPU cores.},
keywords={computer graphic equipment;coprocessors;parallel processing;AMD Fusion;AMD Radeon HD 5870;APU;Intel Knights Ferry;data transfer;fused CPU+GPU Processor;graphics processing unit;parallel computing;programmable accelerator;x86 core acclerator;Acceleration;Benchmark testing;Engines;Graphics processing unit;Instruction sets;Multicore processing;AMD Fusion;APU;GPGPU;GPU;OpenCL;accelerated processing unit;benchmarking;graphics processing unit;heterogeneous computing;performance evaluation},
doi={10.1109/SAAHPC.2011.29},
ISSN={2166-5133},
month={July},}

@inbook{ComputingPerformanceBook:2011,
	author = {S.~H.~Fuller and  L.~I.~Millett},
	title = {{The Future of Computing Performance: Game Over or Next Level?}},
	lab = {National Research Council},
	publisher = {National Academies Press, Washington},
	isbn = {978-0-309-15951-7},
	doi= {10.17226/12980},
	year = {2011}
}

@article{Pingali:2011:TaoOfParallelism,
	author = { Keshav Pingali and  Donald Nguyen and  Milind Kulkarni and Martin Burtscher and M. Amber Hassaan  and  Rashid Kaleem and  Tsung-Hsien Lee and  Andrew Lenharth and  Roman Manevich and Mario M{\'e}ndez-Lojo and Dimitrios Prountzos and  Xin Sui},
	title = {{The Tao of Parallelism in Algorithms}},
	journal = {SIGPLAN Not.},
	issue_date = {June 2011},
	volume = {46},
	number = {6},
	month = jun,
	year = {2011},
	issn = {0362-1340},
	pages = {12--25},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/1993316.1993501},
	doi = {10.1145/1993316.1993501},
	acmid = {1993501},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {amorphous data-parallelism, galois system, irregular programs, operator formulation, tao-analysis},
} 

%	howpublished = {\url{http://invasic.informatik.uni-erlangen.de/en/index.php}}


@Inbook{MemoryWallMcKee:2011,
author="McKee, Sally A.
and Wisniewski, Robert W.",
editor="Padua, David",
title="Memory Wall",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="1110--1116",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_234",
url="http://dx.doi.org/10.1007/978-0-387-09766-4_234"
}


@MISC{GameOverYelick:2011,
	author = {{US National Research Council}},
	lab = {{US National Research Council}},
	title = {{The Future of Computing Performance: Game Over or Next Level?}},
	year = {2011},
	url ={http://science.energy.gov/~/media/ascr/ascac/pdf/meetings/mar11/Yelick.pdf}
}

@article{QuantumSupercomputingCircuits:2011,
	author = {Mariantoni, M. and Wang, H. and Yamamoto, T. and Neeley, M. and Bialczak, R. C. and Chen, Y. and \dots  and Martinis, J. M.},
	title = {{Implementing the quantum von Neumann architecture with superconducting circuits}},
	journal = {Science},
	volume = {334},
	number = {6052},
	year = {2011},
	pages = {61-65},
} 

@phdthesis{HSWscalable2012,
	author = {Daniel Sanchez Martin},
	title = {{HARDWARE AND SOFTWARE TECHNIQUES FOR SCALABLE
	THOUSAND-CORE SYSTEMS}},
	school = {Stanford University, Berkeley},
	year = {2012},	 
	howpublished = {\url{http://purl.stanford.edu/mz572jk7876}}
}



@INPROCEEDINGS{EfficiencyMetric:2012,
	author = { Chung-Hsing Hsu and Jeffery Alan Kuehn  and	Stephen W Poole},
	title = {{Towards efficient supercomputing: searching for the right efficiency metric}},
booktitle={ Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
year={2012},
volume={},
number={},
pages={1157–162},
url = {https://doi.org/10.1145/2188286.2188309},
				note = {Accessed: 2023-09-10}
} 

@misc{NSA_Secret_Report:2012,
	author = {{US National Security Agency}},
	title = {{NSA Outs Top-Secret Report That Missed the Future of Supercomputing}},
	year = {2012},
	url = {https://www.wired.com/2012/11/top-secret-nsa-report/},
					note = {Accessed: 2023-09-10}
	
}




@article{DarkSilicon2012,
	author = {Esmaeilzadeh, H. and  Blem, E. and St. Amant, R. and Sankaralingam, K. and Burger, D},
	title = {{Dark Silicon and the End of Multicore Scaling}},
	journal = {IEEE Micro},
	volume = {32},
	year = {2012},
	pages = {122-134},
	number = {3}
}


@ARTICLE{InherentSequentiality:2012,
   author = {F. Ellen and D. Hendler and N. Shavit},
   title = {{On the Inherent Sequentiality of Concurrent Objects}}, 
   doi = {10.1137/08072646X},
   journal = {SIAM J. Comput.},
     year = {2012},
     volume = {43},
     number = {3},
     pages = {519–536}
}


@MISC{XeonHyperthread2013,
	author = {{Intel}},
	title = {{Software-Hardware Interface for Multi-Many-Core}},
	year = {2013},
	howpublished = {\url{https://software.intel.com/en-us/forums/topic/515522}},
					note = {Accessed: 2023-09-10}
	
}

@CONFERENCE{EmaniRuntimeInfo2013,
author = {Emani, M.K. and Wang, Zheng  and O'Boyle, M.F.P.},
title = {{Smart, adaptive mapping of parallelism in the presence of external workload}},
booktitle = {Code Generation and Optimization (CGO), 2013 IEEE/ACM International Symposium on},
pages = {1--10},
year = {2013},
}

%howpublished = {\url{www.cs.utexas.edu/~skeckler/pubs/isca00.pdf}}


@CONFERENCE{SynchronizationEverything2013,
	author = {David,	T.  and	Guerraoui, R. and	Trigonakis, V.},
	title = {{Everything you always wanted to know about synchronization but were afraid to ask}},
	booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13)},
	year = {2013},
	pages = {33-48},
	doi = {10.1145/2517349.2522714}
}

@misc{RebootingComputing2013,
	author = {{IEEE}},
	title = {{IEEE Rebooting Computing}},
	year = {2013},
	howpublished = {\url{http://rebootingcomputing.ieee.org/}}
}
//http://www.acm.org/education/CS2013-final-report.pdf

@MISC{SHIMM13:Multicore,
	author = {{The Multicore Association.}},
	title = {{Software-Hardware Interface for Multi-Many-Core}},
	year = {2013},
	howpublished = {\url{http://www.multicore-association.org/workgroup/shim.php}}
}

@article{FutureIoT2013,
	author = {Jayavardhana Gubbi and Rajkumar Buyya and Slaven Marusic and Marimuthu Palaniswami },
	title = {{Internet of Things (IoT): A vision, architectural elements, and future directions}},
	journal = {Future Generation Computer Systems},
	volume = {29},
	year = {2013},
	pages = {1645-1660},
	doi = {doi:10.1016/j.future.2013.01.010}
}


@misc{Curriculum2013,
	author = {{ACM}},
	title = {{Computer Science	Curricula 2013 - Curriculum Guidelines for
	Undergraduate Degree Programs	in Computer Science}},
	year = {2014},
	url = {Last accessed Dec. 12, 2015[Online].
	http://www.acm.org/education/CS2013-final-report.pdf}
}

@techreport{Adaptiveresourcecontrolinmulticoresystems2013,
author = {F. Xia and A. Mokhov and A. Yakovlev and A. Iliasov and A. Rafiev and A. Romanovsky},
title = {{Adaptive resource control in multi-core systems}},
institution = {Newcastle University, School of Electrical and Electronic Engineering, Philadephia},
year = {2013},
number = {NCL-EEE-MICRO-TR-2013-183},
howpublished = {\url{http://async.org.uk/tech-reports/NCL-EEE-MICRO-TR-2013-183.pdf}}
}



@INPROCEEDINGS{PerformanceCounter2013,
author={Weaver, V.M. and Terpstra, D. and Moore, S.},
booktitle={2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)  21-23 April 2013; Austin, TX, USA},
title={Non-determinism and overcount on modern hardware performance counter implementations},
year={2013},
pages={215-224},
keywords={computer architecture;multiprocessing systems;performance evaluation;ARM;IA64 system;PMU implementations;POWER system;SPARC system;architectural redesign;counter event;deterministic replay library implementation;deterministic threading library implementation;hardware performance counter implementation;ideal hardware performance counters;nondeterminism;overcount;performance monitoring unit;run-to-run variation;x86_64 CPU implementations;Assembly;Benchmark testing;Hardware;Kernel;Phasor measurement units;Radiation detectors},
doi={10.1109/ISPASS.2013.6557172},}

@inproceedings {NeumannRetire:2013,
	author = {Aleksander Budzynowski and Gernot Heiser},
	title = {The von Neumann Architecture Is Due for Retirement},
	booktitle = {Presented as part of the 14th Workshop on Hot Topics in Operating Systems},
	year = {2013},
	address = {Santa Ana Pueblo, NM},
	url = {https://www.usenix.org/conference/hotos13/von-neumann-architecture-due-retirement},
	publisher = {USENIX},
}


@MISC{InterCoreShared:2013,
	author = {GlobalFoundries Inc},
	title = {{Administering inter-core communication via shared memory}},
	year = {2013},
	howpublished = {\url{https://patents.google.com/patent/US9223505
	}}
}




@ARTICLE{SpiNNaker:2013,
	author = {S. B. Furber and D. R. Lester and L. A. Plana and J. D. Garside and E. Painkras and S. Temple and A. D. Brown},
	journal = {{IEEE Transactions on Computers}},
	title = {{Overview of the SpiNNaker System Architecture}},
	year = {2013},
	volume = {62},
	number = {12},
	pages = {2454-2467},
	keywords={Network architecture;Program processors;Biological system modeling;Computer architecture;Neural networks},
}
%	doi = {10.1109/TC.2012.142},
% url = {doi.ieeecomputersociety.org/10.1109/TC.2012.142},
% ISSN = {0018-9340},
% month={Dec.}


@inproceedings {AsynchronParadigm:2013,
	author = { S. Kumar and et al},
	title = {{Acceleration of an Asynchronous Message Driven Programming Paradigm on IBM Blue Gene/Q}},
	booktitle = {2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
	year = {2013},
	address = {Boston},
	url = {https://https://ieeexplore.ieee.org/abstract/document/ 6569854},
	publisher = {IEEE},
}


@ARTICLE{CompilerInfrastructure:2014,
author = {W. Sheng and S. Sch\"urmans and M. Odendahl and M. Bertsch and V. Volevach and R. Leupers and G. Ascheid},
title = {{A compiler infrastructure for embedded heterogeneous MPSoCs}},
journal = {Parallel Computing},
volume = {40},
Issue = {2},
pages = {51-68},
year = {2014},
}

@misc{DeBenedictis_supercomputing:2014,
author = {{Machine Intelligence Research Institute}},
title = {{Erik DeBenedictis on supercomputing}},
lab ={SAND Number 2014-2679P},
year = {2014},
howpublished = {\url{https://intelligence.org/2014/04/03/erik-debenedictis/} (Accessed on Oct 23, 2021)}

}





@misc{Intel10GHz:2014,
author = {Intel},
title = {{Why has CPU frequency ceased to grow?}},
year = {2014},
url = {https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow}
}



%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{YavitsMulticoreAmdahl2014,
	author = { Yavits, L. and  Morad, A. and Ginosar, R.  },
	title = {{The effect of communication and synchronization on Amdahl's law in multicore systems}},
	journal = {Parallel Computing},
	volume = {40},
	year = {2014},
	pages ={1-16},
	number = {1}
}


@MISC{Epiphany2014,
author = {Adapteva},
title = {{Epiphany Architecture Reference}},
year = {2014},
howpublished = {\url{http://adapteva.com/docs/epiphany_arch_ref.pdf}}
}


%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{ReliableParallel2014,
	author = {Junfeng Yang and Heming Cui and Jingyue Wu and Yang Tang and Gang Hu},
	title = {{Making Parallel Programs Reliable with Stable Multithreading}},
	journal = {Communications of the ACM},
	volume = {57},
	year = {2014},
	pages = {58-69},
	number = {3},
	doi = {10.1145/2500875}
}

@book{hallaron,
	author = {{Randal E. Bryant and  David R. O'Hallaron}},
	title = {{Computer Systems: A Programmer's Perspective}},
	year = {2014},
	publisher = {Pearson},
	place = {Edinburgh Gate, Harlow, Essex CM20 2JE},
	isbn = {978-1-292-02584-1}
}

@misc{TorwaldsParallel2014,
	author = {Linus Torwalds},
title = {{Linus: The whole "parallel computing is the future" is a bunch of crock}},
	year = {2014},
	url = {http://highscalability.com/blog/2014/12/31/linus-the-whole-parallel-computing-is-the-future-is-a-bunch.html}
}




@MISC{MIPSUserManual:2014,
	author = {{Imagination Technologies Ltd.}},
	title = {{MIPS32\raisebox{4pt}{\textregistered} \ 
	microAptiv\textsuperscript{TM} UP Processor	Core Family Software User's Manual}},
	year = {2014},
	howpublished = {\url{http://community.imgtec.com/university/}}
}


@ARTICLE{Vishkin:ViewpointProgrammingMulticore,
	AUTHOR = "Uzi Vishkin",
	TITLE = "{Is Multicore Hardware for	General-Purpose Parallel Processing Broken?}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {57},
	NUMBER = {4},
	PAGES = {35},
	DOI = {10.1145/2580945},
	YEAR = {2014}	}

@misc{FastAwake:lachwani2014,
  title={Fast awake from low power mode},
  author={Lachwani, M. and Puckett, J.M. and Berbessou, D. and Bowen, J.S. and Isbister, D.J.},
  url={https://www.google.com/patents/US8766919},
  year={2014},
  month=jul # "~1",
  publisher={Google Patents},
  note={US Patent 8,766,919}
}


@ARTICLE{MarkovLimitsOfLimits:2014,
   author = {I.L.~Markov},
   title = {{Limits on fundamental limits to computation}},
   journal = {Nature},
   volume = {512},
   Issue = {7513},
   pages = {147-154},
   note = {\url{http://download.nap.edu/cart/download.cgi?\&record_id=12980}},
   year = {2014}
}


 
@ARTICLE{HybridDataflowNeumann:2014,
   author = {F. Yazdanpanah and C. Alvarez-Martinez and D. Jimenez-Gonzalez and Y. Etsion},
   title = {{Hybrid Dataflow/von-Neumann Architectures}},
   journal = {{IEEE Transactions on Parallel and Distributed Systems}},
   volume = {25},
   Issue = {6},
   pages = {1489 - 1509},
   note = {\url{https://pdfs.semanticscholar.org/f1af/0791836175668598d8112df42ca091d9600b.pdf}},
   year = {2014}
}
 
@ARTICLE{ImpreciseSupercomputers:2015,
	author = { Palmer, Tim},
	year = {2015},
	title = {Modelling: Build imprecise supercomputers},
	journal = {Nature},
	page = {32-33},
	volume = {526},
	isue = {7571},
	abstract = {Energy-optimized hybrid computers with a range of processor accuracies will advance modelling in fields from climate change to neuroscience,
	says Tim Palmer.},
	doi = {10.1038/526032a},
}
 
@book{CommunicatingSerialProcesses:2015,
	editor = {Abdallah, Ali E. and Jones, Cliff and Sanders, Jeff W.},
	title = {{Communicating Sequential Processes. The First 25 Years}},
	year = {2005},
	isbn = {978-3-540-32265-87},
	publisher = {Springer},
	doi ={10.1007/b136154}
}

 
@book{HPCFPGA:2014,
	editor = {Wim Vanderbauwhede and Khaled Benkrid},
	title = {{High-Performance Computing Using FPGAs}},
	year = {2014},
	publisher = {Springer},
}



@Book{ArpaciDusseau14-Book,
  author =       {Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau},
  title =        "{Operating Systems: Three Easy Pieces}",
  publisher =    {Arpaci-Dusseau Books},
  month =        "May",
  year =         {2015},
  edition =      {0.91},
}

@ARTICLE{Larus:ProgrammingMulticoreCACM,
	AUTHOR = "James Larus",
	TITLE = "{Programming Multicore Computers}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {58},
	NUMBER = {5},
	PAGES = {76},
	MONTH = "May",
	DOI = {10.1145/2580945},
	YEAR = {2015}	}
	
	@MISC{DeepLearningSingularity:2015,
	author = {Tim Dettmers},
	title = {{The Brain vs Deep Learning Part I: Computational Complexity -- Or Why the Singularity Is Nowhere Near}},
	year = {2015},
	howpublished = {\url{http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/}}
	}
	
	
	

@inproceedings{Esmaeilzadeh:2015,
	author = {H. Esmaeilzadeh},
	title = {Approximate Acceleration: A Path Through the Era of Dark Silicon and Big Data},
	booktitle = {Proceedings of the 2015 International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
	series = {CASES '15},
	year = {2015},
	isbn = {978-1-4673-8320-2},
	location = {Amsterdam, The Netherlands},
	pages = {31--32},
	numpages = {2},
} 
%	url = {http://dl.acm.org/citation.cfm?id=2830689.2830693},
% acmid = {2830693},
% publisher = {IEEE Press},
% address = {Piscataway, NJ, USA},

@article{NinjaPerformanceGap:2015:CACM,
	author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
	title = "{Can Traditional Programming Bridge the Ninja Performance Gap for Parallel Computing Applications?}",
	journal = {Commun. ACM},
	issue_date = {May 2015},
	volume = {58},
	number = {5},
	month = apr,
	year = {2015},
	issn = {0001-0782},
	pages = {77--86},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2742910},
	doi = {10.1145/2742910},
	acmid = {2742910},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


% issn = {1544-3566},
%url = {http://doi.acm.org/10.1145/2686874},
%doi = {10.1145/2686874},

@article{Matheou:2015:ASD:2695583.2686874,
 author = {Matheou, George and Evripidou, Paraskevas},
 title = {Architectural Support for Data-Driven Execution},
 journal = {ACM Trans. Archit. Code Optim.},
 issue_date = {January 2015},
 volume = {11},
 number = {4},
 month = jan,
 year = {2015},
 pages = {52:1--52:25},
 articleno = {52},
 numpages = {25},
 acmid = {2686874},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data-Driven Multithreading, FPGA},
} 

@MISC{Cypress15,
	author = {Cypress},
	title = {{CY7C026A: 16K x 16 Dual-Port Static RAM}},
	year = {2015},
	howpublished = {\url{http://www.cypress.com/documentation/datasheets/cy7c026a-16k-x-16-dual-port-static-ram}}
}

@book{Dataflow:2015,
	editor = {Ai Hurson and Veljko Milutinovic},
	series = {Advances in Computers, Volume 96},
	title = {{Dataflow processing}},
	year = {2015},
	isbn = { 9780128021347},
	publisher = {Elsevier},
}

@book{Information:2012,
	author = {Luciano Floridi},
	title = {{Information - A Very Short Introduction.}},
	year = {2010},
	isbn = {978-0-19-160954-1},
	publisher = {Oxford University Press},
}


@book{ClockDistribution:2012,
	editor = {Rainer Waser },
	lab = {Aachen University},
	series = {Nanoelectronics and Information Technology},
	title = {{Advanced Electronics Materials and Novel Devices}},
	year = {2012},
	isbn = { 9783-527-40927-3},
	publisher = {Wiley-VCH},
}


@INPROCEEDINGS{PerformancePenaltySpinnaker:2013,
	author={Diehl, P.U. and Cook, M.},
	booktitle={In 2014 International Joint Conference on Neural Networks (IJCNN)}, 
	title={{Efficient implementation of STDP rules on SpiNNaker neuromorphic hardware}}, 
	year={2014},
	volume={},
	number={},	
	pages={4288–4295},
}
% https://doi.org/10.1109/IJCNN.2014.6889876.


@MISC{IntelMemorywall2015,
author = {Holger Pirk},
publisher = {Intel},
title = {{Memory Wall? What Memory Wall?}},
year = {2015},
url = {http://istc-bigdata.org/index.php/memory-wall-what-memory-wall/}
}

@Article{HPCGB:2015,
	author = {Jack Dongarra and Michael A Heroux and Piotr Luszczek},
title = {{High-performance conjugate-gradient benchmark: A new metric for ranking high-performance computing systems}},
journal = {{The International Journal of High Performance Computing Applications}},
year={2015},
url = {\url{https://doi.org/10.1177/1094342015593158}}
}


@misc{NeuromorphicComputing:2015,
author = {{US DOE Office of Science}},
title = {{Report of a Roundtable Convened to 
Consider Neuromorphic Computing 
Basic Research Needs}},
year = {2015},
howpublished = {\url{https://science.osti.gov/-/media/ascr/pdf/programdocuments/docs/Neuromorphic-Computing-Report\_FNLBLP.pdf}}
}

@Article{CooperativeComputing2015,
author="Zheng, Fang
and Li, Hong-Liang
and Lv, Hui
and Guo, Feng
and Xu, Xiao-Hong
and Xie, Xiang-Hui",
title="Cooperative Computing Techniques for a Deeply Fused and Heterogeneous Many-Core Processor Architecture",
journal="Journal of Computer Science and Technology",
year="2015",
day="01",
volume="30",
number="1",
pages="145--162",
abstract="Due to advances in semiconductor techniques, many-core processors have been widely used in high performance computing. However, many applications still cannot be carried out efficiently due to the memory wall, which has become a bottleneck in many-core processors. In this paper, we present a novel heterogeneous many-core processor architecture named deeply fused many-core (DFMC) for high performance computing systems. DFMC integrates management processing elements (MPEs) and computing processing elements (CPEs), which are heterogeneous processor cores for different application features with a unified ISA (instruction set architecture), a unified execution model, and share-memory that supports cache coherence. The DFMC processor can alleviate the memory wall problem by combining a series of cooperative computing techniques of CPEs, such as multi-pattern data stream transfer, efficient register-level communication mechanism, and fast hardware synchronization technique. These techniques are able to improve on-chip data reuse and optimize memory access performance. This paper illustrates an implementation of a full system prototype based on FPGA with four MPEs and 256 CPEs. Our experimental results show that the effect of the cooperative computing techniques of CPEs is significant, with DGEMM (double-precision matrix multiplication) achieving an efficiency of 94{\%}, FFT (fast Fourier transform) obtaining a performance of 207 GFLOPS and FDTD (finite-difference time-domain) obtaining a performance of 27 GFLOPS.",
}
%issn="1860-4749",
%doi="10.1007/s11390-015-1510-9",
%url="https://doi.org/10.1007/s11390-015-1510-9"


@MISC{MentorMooreFuture2016,
author = {Mentor Graphics},
title = {{Moore's Law and the Future of Solid-State Electronics}},
year = {2016},
howpublished = {\url{http://blogs.scientificamerican.com/guest-blog/moore-s-law-and-the-future-of-solid-state-electronics/}}
}

% issn = {1544-3566},
%url = {http://doi.acm.org/10.1145/2890506},
%doi = {10.1145/2890506},
%acmid = {2890506},
@article{Braak:2016:RRG:2899032.2890506,
 author = {Braak, Gert-Jan Van Den and Corporaal, Henk},
 title = {{R-GPU: A Reconfigurable GPU Architecture}},
 journal = {ACM Trans. Archit. Code Optim.},
 issue_date = {April 2016},
 volume = {13},
 number = {1},
 month = mar,
 year = {2016},
 pages = {12:1--12:24},
 articleno = {12},
 numpages = {24},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, reconfigurable architecture},
} 


@techreport{DongarraSunwaySystem:2016,
author = {Jack Dongarra},
title = {{Report on the Sunway TaihuLight System}},
institution = {University of Tennessee
Department of Electrical Engineering and Computer Science},
year = {2016},
   url = {\url{http://www.netlib.org/utk/people/JackDongarra/PAPERS/sunway-report-2016.pdf}},
number = {Tech Report UT-EECS-16-742},
}

@Article{FuSunwaySystem2016,
author="Fu, Haohuan
and Liao, Junfeng
and Yang, Jinzhe
and Wang, Lanning
and Song, Zhenya
and Huang, Xiaomeng
and Yang, Chao
and Xue, Wei
and Liu, Fangfang
and Qiao, Fangli
and Zhao, Wei
and Yin, Xunqiang
and Hou, Chaofeng
and Zhang, Chenglong
and Ge, Wei
and Zhang, Jian
and Wang, Yangang
and Zhou, Chunbo
and Yang, Guangwen",
title={{The Sunway TaihuLight supercomputer: system and applications}},
journal="{Science China Information Sciences}",
year="2016",
volume="59",
number="7",
pages="1--16",
abstract="The Sunway TaihuLight supercomputer is the world's first system with a peak performance greater than 100 PFlops. In this paper, we provide a detailed introduction to the TaihuLight system. In contrast with other existing heterogeneous supercomputers, which include both CPU processors and PCIe-connected many-core accelerators (NVIDIA GPU or Intel Xeon Phi), the computing power of TaihuLight is provided by a homegrown many-core SW26010 CPU that includes both the management processing elements (MPEs) and computing processing elements (CPEs) in one chip. With 260 processing elements in one CPU, a single SW26010 provides a peak performance of over three TFlops. To alleviate the memory bandwidth bottleneck in most applications, each CPE comes with a scratch pad memory, which serves as a user-controlled cache. To support the parallelization of programs on the new many-core architecture, in addition to the basic C/C++ and Fortran compilers, the system provides a customized Sunway OpenACC tool that supports the OpenACC 2.0 syntax. This paper also reports our preliminary efforts on developing and optimizing applications on the TaihuLight system, focusing on key application domains, such as earth system modeling, ocean surface wave modeling, atomistic simulation, and phase-field simulation.",
}
%issn="1869-1919",
%doi="10.1007/s11432-016-5588-7",
%url="http://dx.doi.org/10.1007/s11432-016-5588-7"

@MISC{FileSystemHierarchy:2016,
author = {The Linux fundations},
editor = {Yeoh, Christopher; Russell, Rusty; Quinlan, Daniel, },
title = {{Filesystem Hierarchy Standard}},
year = {2016},
howpublished = {\url{https://refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf}}
}


   
@inbook{ReconfigurableAdaptive2016,
author = {Luiza de Macedo Mourelle and Nadia Nedjah and
Fábio Goncalves Pessanha},
title = {{Reconfigurable and Adaptive Computing: Theory and Applications}}, 
	editor = {Nadia Nedjah and Chao Wang},
	chapter = {5: Interprocess Communication via Crossbar for Shared Memory Systems-on-chip},
publisher = {CRC press},
year = {2016},
isbn = {978-1-4987-3176-8},
doi={10.1201/b19157-7}
}


@book{HwangParallelism:2016,
	author = {Kai Hwang and Naresh Jotwani},
	title = {{Advanced Computer Architecture: Parallelism, Scalability, Programmability}}, 
	publisher = {Mc Graw Hill},
	isbn = {978-93-392-2093-8},
	edition = {3},
	year = {2016}
}

%	abstract=
%	{
%	https://books.google.hu/books?hl=en&lr=&id=grz-CwAAQBAJ&oi=fnd&pg=PT13&ots=fCadzG5Psv&sig=sKh_UGATcrAVaHNDewH6VMMjyG0&redir_esc=y#v=onepage&q&f=false	
%	}


@MISC{IntelFPGAcloud:2016,
author = {{cloudcomputing-news.net}},
title = {{How Intel’s acquisition of Altera could transform IoT and the data center}},
year = {2016},
howpublished = {\url{http://www.cloudcomputing-news.net/news/2016/apr/07/intel-acquires-altera-what-their-fpga-capabilities-could-mean-for-data-centers-and-iot//}}
}


 @ARTICLE{QuantumArithmetic:2017,
	author = {Ruiz-Perez, Lidia and Garcia-Escartin, Juan Carlos},
	title = {{Quantum arithmetic with the quantum Fourier transform}},
	journal = {Quantum Information Processing},
	volume = {16},
	issue = {6},
	year = {2017},
	pages = {152},
}


@MISC{IntelDumpsXeonPhi:2017,
	author = {{www.top500.org}},
	title = {Intel Dumps Knights Hill, Future of Xeon Phi Product Line Uncertain},
	year = {2017},
	howpublished = {\url{https://www.top500.org/news/intel-dumps-knights-hill-future-of-xeon-phi-product-line-uncertain///}}
}


@MISC{Arpaci-Dusseau_OS:2016,
	author = {Remzi\&Andrea Arpaci-Dusseau},
	title = {{Operating Systems: Three Easy Pieces}},
	year = {2016},
	month = {July},
	howpublished = {\url{http://www.lulu.com/shop/remzi-arpaci-dusseau-and-andrea-arpaci-dusseau/operating-systems-three-easy-pieces-softcover-version-091/paperback/product-22796750.html}}
}


@MISC{NSA_DOE_HPC_Report_2016,
author = {{US Government NSA and DOE}},
title = {{A Report from the NSA-DOE Technical Meeting on High Performance Computing}},
year = {2016},
howpublished = {https://www.nitrd.gov/ \\nitrdgroups/images/b/b4/NSA\_DOE\_HPC\_TechMeetingReport.pdf}
}


@MISC{HPCG_List:2016,
author = {{HPCG Benchmark}},
title = {{HPCG Benchmark}},
year = {2016},
howpublished = {\url{http://www.hpcg-benchmark.org/}}
}


@misc{EUActionPlan:2016,
author = {{European Commission}},
title = {{Implementation of the Action Plan for the European High-Performance Computing strategy}},
year = {2016},
howpublished = {\url{https://eur-lex.europa.eu/legal-content/en/ALL/?uri=CELEX%3A52016SC0106}}
}

   
%    doi = {DOI:10.1145/2976758},
 @ARTICLE{ExponentialLawsComputing:2017,
    author = {P. J. Denning and T.G. Lewis},
     title = {{Exponential Laws of Computing Growth}},
   journal = {Communications of the ACM},
      year = 2017,
      pages = {54-65},
     month = Jan,
 }

 @article{Post-Moore_2017,
author = {J. S. Vetter and E. P. DeBenedictis and T. M. Conte },
title = {{Architectures for the Post-Moore Era}},
journal = "IEEE Micro",
year = {2017},
volume = {37},
issue = {4},
pages = {6--8}
}´
%10.1109/MM.2017.3211127
 
 @article{AngeloLearningBioinformatics:2014, 
 author = {D'Angelo, G. and Rampone, S.},
 title = {{Towards a HPC-oriented parallel implementation of a learning algorithm for bioinformatics applications}},
 journal = {{BMC Bioinformatics}},
 volume =  {15},
 issue = {S2},
 year = {2014},
 howpublished = {\url{https://doi.org/10.1186/1471-2105-15-S5-S2}}
 }

@article{Computing_Dark_Silicon_2017,
title = "{Computing in the dark silicon era: Current trends and research challenges}",
abstract = "Power density has become the major constraint for many on-chip designs. As an introduction to the Special Issue on Dark Silicon, the authors provide the newest trends and a survey on the topic that has valuable information for novices and experts alike.",
author = "Muhammad Shafique and Siddharth Garg",
year = "2017",
month = "4",
doi = "10.1109/MDAT.2016.2633408",
volume = "34",
pages = "8--23",
journal = "IEEE Design and Test",
issn = "2168-2356",
publisher = "IEEE Computer Society",
number = "2",
}
%{\url{http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862952}},

@article{Lifetime_Dark_Silicon_2017,
title = "
{Can Dark Silicon Be Exploited to Prolong System Lifetime?} ",
abstract = "Besides stringent power and thermal constraints, a dark silicon chip is also subjected to various reliability threats. This article illustrates how the dark silicon can be exploited to improve the chip's lifetime through efficient utilization of computational resources and power budget, while still performing in a similar way.",
author = "Mohammad-Hashem Haghbayan ; Amir M. Rahmani ; Pasi Liljeberg ; Axel Jantsch ; Antonio Miele ; Cristiana Bolchini ; Hannu Tenhunen",
year = "2017",
month = "4",
doi = "10.1109/MDAT.2016.2633408",
volume = "34",
pages = "51-59",
journal = "IEEE Design and Test",
issn = "2168-2356",
publisher = "IEEE Computer Society",
number = "2",
}


%doi={10.1109/JSSC.2016.2638459},
%ISSN={0018-9200},
@ARTICLE{KilocoreChip:2017,
author={B. Bohnenstiehl and A. Stillmaker and J. J. Pimentel and T. Andreas and B. Liu and A. T. Tran and E. Adeagbo and B. M. Baas},
journal={IEEE Journal of Solid-State Circuits},
title={{KiloCore: A 32-nm 1000-Processor Computational Array}},
year={2017},
volume={52},
number={4},
pages={891-902},
abstract={A processor array containing 1000 independent processors and 12 memory modules was fabricated in 32-nm partially depleted silicon on insulator CMOS. The programmable processors occupy 0.055 mm2 each, contain no algorithmspecific hardware, and operate up to an average maximum clock frequency of 1.78 GHz at 1.1 V. At 0.9 V, processors operating at an average of 1.24 GHz dissipate 17 mW while issuing one instruction per cycle. At 0.56 V, processors operating at an average of 115 MHz dissipate 0.61 mW while issuing one instruction per cycle, resulting in an energy consumption of 5.3 pJ/instruction. On-die communication is performed by complementary circuit and packet-based networks that yield a total array bisection bandwidth of 4.2 Tb/s. Independent memory modules handle data and instructions and operate up to an average maximum clock frequency of 1.77 GHz at 1.1 V. All processors, their packet routers, and the memory modules contain unconstrained clock oscillators within independent clock domains that adapt to large supply voltage noise. Compared with a variety of Intel i7s and Nvidia GPUs, the KiloCore at 1.1 V has geometric mean improvements of 4.3× higher throughput per area and 9.4× higher energy efficiency for AES encryption, 4095-b low-density parity-check decoding, 4096-point complex fast Fourier transform, and 100-B record sorting applications.},
keywords={CMOS digital integrated circuits;UHF integrated circuits;UHF oscillators;clocks;energy conservation;integrated circuit noise;memory architecture;multiprocessing systems;network routing;network-on-chip;parallel processing;power aware computing;power consumption;silicon-on-insulator;100-B record sorting applications;1000-processor computational array;4095-b low-density parity-check decoding;4096-point complex fast Fourier transform;AES encryption;KiloCore;NoC;complementary circuit;data handling;energy consumption;energy efficiency;geometric mean improvements;memory modules;on-die communication;packet routers;packet-based networks;partially depleted silicon on insulator CMOS;programmable processors;size 32 nm;supply voltage noise;unconstrained clock oscillators;voltage 0.56 V to 1.1 V;Clocks;Memory management;Oscillators;Parallel processing;Pipelines;Ports (Computers);Throughput;Globally asynchronous locally synchronous (GALS);NoC;many core;multicore;parallel processor},
}
 

@misc{IntelArchitectureOptimization2016,
author = {Intel},
title = {{Intel® 64 and IA-32 Architectures
Optimization Reference Manual}},
year = {2016},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf}}
}


@inproceedings{IBMasynchronous:2016,
	author = {Akopyan, Filipp},
	title = {{Design and Tool Flow of IBM's TrueNorth: An Ultra-Low Power Programmable Neurosynaptic Chip with 1 Million Neurons}},
	booktitle = {Proceedings of the 2016 on International Symposium on Physical Design},
	series = {ISPD '16},
	year = {2016},
	isbn = {978-1-4503-4039-7},
	location = {Santa Rosa, California, USA},
	pages = {59--60},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/2872334.2878629},
	doi = {10.1145/2872334.2878629},
	acmid = {2878629},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {asynchronous circuits, asynchronous communication, custom tool flow, design automation, design methodology, image recognition, logic design, low-power electronics, neural network hardware, neural networks, neuromorphics, parallel architectures, real-time systems, synchronous circuits, very large-scale integration},
} 



@misc{WhyNotExascale:2014,
	author = {Simon, Horst},
	title = {{Why we need Exascale and why we won't get there by 2020}},
	booktitle = {Exascale Radioastronomy Meeting},
	series = {AASCTS2},
	year = {2014},
	location = {Monterey, California, USA},
	numpages = {60},
howpublished = {\url{https://www.researchgate.net/publication/261879110\\ \_Why\_we\_need\_Exascale\_and\_why\_we\_won't\_get \_there\_by\_2020}},
				note = {Accessed: 2023-09-10}

} 

@misc{PEZY2048cores:2017,
author = {PEZY},
title = {{2048 core chip}},
year = {2017},
howpublished = {\url{https://www.top500.org/green500/lists/ 2017/11/}}
}

 
@misc{HP_MemoryDriven:2017,
author = {Hewlett Packard Enterprise (HPE)},
title = {{HP unleashes "The Machine" memory-centric supercomputer prototype}},
year = {2017},
howpublished = {\url{https://newatlas.com/hewlett-packard-enterprise-the-machine-big-data/49561/}},
				note = {Accessed: 2023-09-10}

}


@INPROCEEDINGS{PerformancePenaltySpinnaker:2013,
	author={Diehl, P.U. and Cook, M.},
	booktitle={In 2014 International Joint Conference on Neural Networks (IJCNN)}, 
	title={{Efficient implementation of STDP rules on SpiNNaker neuromorphic hardware}}, 
	year={2014},
	volume={},
	number={},	
	pages={4288–4295},
}
% https://doi.org/10.1109/IJCNN.2014.6889876.

@misc{NaturalComputation:2018,
	author ={Chu, Dominique and Prokopenko, Mikhail and Ray, J. Christian},
	title = {Computation by natural systems},
	journal = {Interface Focus}, 
	page = {820180058},
	volume = {8},
	doi = {10.1098/rsfs.2018.0058},
	howpublished = {\url{https://www.researchgate.net/publication/328398755_Computation_by_natural_systems}},
	note = {Accessed: 2023-08-30}
	
}

@book{
	MemristorBook:2018,
	editor = {James, Alex Pappachen},
	title = {{memristor and Memristive Neural Networks}},
	publisher = {InTech, Rijeka, Croatia},
	year = {2018},
	isbn = {978-953-51-3947-8}
}


@article{IntelLoihi:2018,
	author = {{M. Davies, et al}},
	title = {{Loihi:
	{\small A Neuromorphic Manycore Processor with On-Chip Learning}}},
	journal = { IEEE Micro},
	year = {2018},
	volume = {38},
	issue = {1},
	pages ={82--99},
	keywords = {neuromorphic computing,machine learning,artificial intelligence},
}
%        doi = {https://doi.org/10.1109/MM.2018.112130359}




@INPROCEEDINGS{IntelLoihiAsynchronous:2018,	
	author={Lines, Andrew and Joshi, Prasad and Liu, Ruokun and McCoy, Steve and Tse, Jonathan and Weng, Yi-Hsin and Davies, Mike},	
	booktitle={24th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC) 13-16 May 2018; Vienna, Austria}, 	
	title={Loihi Asynchronous Neuromorphic Research Chip}, 	
	year={2018},	
	volume={},
	number={},	
	pages={32-33},	
	doi={10.1109/ASYNC.2018.00018},
}


@article{PhysicsOfBrainNetwork:2019,
	author= {Lynn, Christopher W. and Bassett, Danielle S.},
	year = {2019},
	title = {The physics of brain network structure, function and control},
	journal = { Nature Reviews Physics},
	pages = {318-332},
	volume = {1},
	issue = {5},
	abstract = { The brain is characterized by heterogeneous patterns of structural connections supporting unparalleled feats of cognition and a wide range of behaviours. New non-invasive imaging techniques now allow comprehensive mapping of these patterns. However, a fundamental challenge remains to understand how the brain’s structural wiring supports cognitive processes, with major implications for personalized mental health treatments. Here, we review recent efforts to meet this challenge, drawing on physics intuitions, models and theories, spanning the domains of statistical mechanics, information theory, dynamical systems and control. We first describe the organizing principles of brain network architecture instantiated in structural wiring under constraints of spatial embedding and energy minimization. We then survey models of brain network function that stipulate how neural activity propagates along structural connections. Finally, we discuss perturbative experiments and models for brain network control; these use the physics of signal transmission along structural connections to infer intrinsic control processes that support goal-directed behaviour and to inform stimulation-based therapies for neurological and psychiatric disease. Throughout, we highlight open questions that invite the creative efforts of pioneering physicists.},
	doi = {10.1038/s42254-019-0040-8},
}


@article{RecipeMemristor:2020,
	author = {Chicca,E.  and Indiveri,G. },
	title = {{A recipe for creating ideal hybrid memristive-CMOS neuromorphic processing systems}},
	journal = {Applied Physics Letters},
	volume = {116},
	number = {12},
	pages = {120501},
	year = {2020},
	doi = {10.1063/1.5142089},
	
	URL = { 
	https://doi.org/10.1063/1.5142089},
	ABSTRACT={The development of memristive device technologies has reached a level of maturity to enable the design and fabrication of complex and large-scale hybrid memristive-Complementary Metal-Oxide Semiconductor (CMOS) neural processing systems. These systems offer promising solutions for implementing novel in-memory computing architectures for machine learning and data analysis problems. We argue that they are also ideal building blocks for integration in neuromorphic electronic circuits suitable for ultra-low power brain-inspired sensory processing systems, therefore leading to innovative solutions for always-on edge-computing and Internet-of-Things applications. Here, we present a recipe for creating such systems based on design strategies and computing principles inspired by those used in mammalian brains. We enumerate the specifications and properties of memristive devices required to support always-on learning in neuromorphic computing systems and to minimize their power consumption. Finally, we discuss in what cases such neuromorphic systems can complement conventional processing ones and highlight the importance of exploiting the physics of both the memristive devices and the CMOS circuits interfaced to them.}
	
}
%	eprint = { 
	%	https://doi.org/10.1063/1.5142089	
	%},

@misc{HP_MemoryDrivenMemristor:2020,
	author = {Hewlett Packard Enterprise (HPE)},
	title = {{Memristors and HPE Machine Focused Computer Idea Seems Almost Dead}},
	year = {2020},
	howpublished = {\url{https://www.nextbigfuture.com/2020/11/memristors-and-hpe\\ -machine-focused-computer-research-seems- almost-dead.html}},
					note = {Accessed: 2023-09-10}
	
}

@article{PhysicsForNeuromorhicComputing:2020,
	author = {Danijela Markovic and Alice Mizrahi and Damien Querlioz and Julie Grollier},
	title = {{Physics for neuromorphic computing}},
	journal = {Nature Reviews Physics},
	volume = {2},
	year = {2020},
	pages = {499--510},
	doi = "https://www.nature.com/articles/s42254-020-0208-2.pdf"
}



@misc{HPCGPerformanceList:2017,
author = {{HPCG Benchmark}},
title = {{7th HPCG Performance List}},
year = {2017},
howpublished = {\url{http://www.hpcg-benchmark.org/custom/index.html?lid=154\&slid=292}}
}


@misc{AIPowerSystems:2018,
	author = {{MDPI}},
	title = {{The Artificial Intelligence Technologies for Electric Power Systems}},
	year = {2018},
	howpublished = {\url{https://www.mdpi.com/journal/energies/special_issues/AI_EPS}},
					note = {Accessed: 2023-09-10}
	
}


@misc{DongarraExascaleRace:2017,
author = {J. Dongarra},
title = {{The Global Race for Exascale High Performance Computing}},
year = {2017},
howpublished = {\url{http://ec.europa.eu/newsroom/document.cfm?doc_id=45647}}
}

 
 @misc{Gyoukou:2017,
 author = {TOP500},
 title = {{November 2017 list of supercomputers}},
 year = {2017},
 howpublished = {\url{https://www.top500.org/lists/2017/11/}}
 }
 
@book{RISCVarchitecture:2017,
	editor = {D.A. Patterson and J.L. Hennessy},
	title = {{Computer Organization and design. RISC-V Edition}},
	year = {2017},
	publisher = {Morgan Kaufmann},
}


@inproceedings{BenchmarkingClouds:2017,
	author = {Mohammadi, Mohammad and Bazhirov, Timur},
	title = {{Comparative Benchmarking of Cloud Computing Vendors with High Performance Linpack}},
	booktitle = {Proceedings of the 2Nd International Conference on High Performance Compilation, Computing and Communications, Hong Kong, Hong Kong},
	series = {HP3C},
	year = {2018},
	isbn = {978-1-4503-6337-2},
	pages = {1--5},
	numpages = {5},
	acmid = {3195613},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {benchmarking, cloud computing, high performance computing, linpack},
} 

%	url = {http://doi.acm.org/10.1145/3195612.3195613},
% doi = {10.1145/3195612.3195613},

@misc{PricePerformanceClouds:2017,
author = {E. Wustenhoff and T. S. E. Ng},
lab = {""},
title = {{Cloud Computing Benchmark}},
year = {2017},
howpublished = {\url{https://www.burstorm.com/price-performance-benchmark/1st-Continuous-Cloud-Price-Performance-Benchmarking.pdf} (Accessed on Oct 24, 2021)}
}


@misc{DifferentBenchmarks:2017,
author = {{IEEE Spectrum}},
title = {{Two Different Top500 Supercomputing Benchmarks Show Two Different Top Supercomputers}},
year = {2017},
howpublished = {\url{https://spectrum.ieee.org/tech-talk/computing/hardware/two-different-top500-supercomputing- benchmarks-show\ -two -different-top-supercomputers}}
}

@misc{TOP500,
author = {TOP500},
title = {{Top500 list of supercomputers}},
year = {2021},
howpublished = {\url{https://www.top500.org/lists/top500/} (Accessed on Oct 24, 2021)}
}

@misc{DOEAurora:2017,
author = {{Top500.org}},
title = {{Retooled Aurora Supercomputer Will Be America’s First Exascale System}},
year = {2017},
howpublished = {\url{https://www.top500.org/news/retooled-aurora-supercomputer-will-be -americas- first-exascale-system/}}
}

@misc{DOEAuroraMistery:2017,
author = {{Inside HPC}},
title = {{Is Aurora Morphing into an Exascale AI Supercomputer?}},
year = {2017},
 howpublished = {\url{https://insidehpc.com/2017/06/told-aurora-morphing-novel-architecture-ai-supercomputer/}}
}

@misc{SupercomputersCosmicRay:2018,
	author = {wired.com},
	title = {{Cosmic Ray Showers Crash Supercomputers. Here's What to Do About It}},
	year = {2018},
	howpublished = {\url{https://www.wired.com/story/cosmic-ray-showers-crash-supercomputers -heres-what-to-do-about-it/}}
}



@misc{PEZY-SC2:2017,
author = {{fuse.wikichip.org}},
title = {{The 2,048-core PEZY-SC2 sets a Green500 record}},
year = {2017},
 howpublished = {\url{https://fuse.wikichip.org/news/191/the-2048-core-pezy-sc2-sets-a-green500-record/}}
 }

@InProceedings{AmdahlsRefutation_Devai2017,
author="D{\'e}vai, Ferenc",
editor="Gervasi, Osvaldo
and Murgante, Beniamino
and Misra, Sanjay
and Borruso, Giuseppe
and Torre, Carmelo M.
and Rocha, Ana Maria A.C.
and Taniar, David
and Apduhan, Bernady O.
and Stankova, Elena
and Cuzzocrea, Alfredo",
title="{The Refutation of Amdahl's Law and Its Variants}",
booktitle="Computational Science and Its Applications -- ICCSA 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="480--493",
abstract="Amdahl's law, imposing a restriction on the speedup achievable by a multiple number of processors, based on the concept of sequential and parallelizable fractions of computations, has been used to justify, among others, asymmetric chip multiprocessor architectures and concerns of ``dark silicon''. This paper demonstrates flaws in Amdahl's law that (i) in theory no inherently sequential fractions of computations exists (ii) sequential fractions appearing in practice are inherently different from parallelizable fractions and therefore usually have different growth rates and that (iii) the time requirement of sequential fractions can be proportional to the number of processors. However, mathematical analyses are also provided to demonstrate that sequential fractions have negligible effect on speedup if the growth rate of the parallelizable fraction is higher than that of the sequential fraction. Examples from computational geometry are given that Amdahl's law and its variants fail to represent limits to parallel computation. In particular, Gustafson's law, claimed to be a refutation of Amdahl's law by some authors, is shown to contradict established theoretical results. We can conclude that no simple formula or law governing concurrency exists.",
isbn="978-3-319-62395-5"
}

@misc{GyoukouFraud:2017,
author = {{The Japan Times}},
title = {{Chief of firm behind world’s fourth-fastest supercomputer arrested in Tokyo for alleged fraud}},
year = {2017},
howpublished = {\url{https://www.japantimes.co.jp/news/2017/12/05/national/crime-legal/chief-firm-behind-worlds-fourth-fastest-supercomputer-arrested-tokyo-alleged-fraud/\#.WmQ-KXRG3CI}}
}

@article{StrechingSupercomputers:2017,
	author = {K. Bourzac},
	title = {{Streching supercomputers to the limit}},
	journal = {Nature},
	volume = {551},
	year = {2017},
	pages = {554-556},
}


@misc{AuroraEarlyScience:2017,
author = {{Top500.org}},
title = {{DOE Witholds Details of First Exascale Supercomputer, Even as it Solicits Researchers to Apply for Early Access}},
year = {2018},
howpublished = {\url{https://www.top500.org/news/doe-witholds-details-of-first-exascale -supercomputer- even-as-it-solicits-researchers -to-apply-for-early-access/}}
}

@misc{ExaScaleRace:2018,
	author = {{Top500.org}},
	title = {{The race to exascale}},
	year = {2018},
	howpublished = {\url{https://sciencenode.org/feature/the-race-to-exascale.php}}
}

@article{GordonBellPrize:2017,
	abstract = { The Gordon Bell Prize is awarded each year by the Association for Computing Machinery to recognize outstanding achievement in high-performance computing (HPC). The purpose of the award is to track the progress of parallel computing with particular emphasis on rewarding innovation in applying HPC to applications in science, engineering, and large-scale data analytics. Prizes may be awarded for peak performance or special achievements in scalability and time-to-solution on important science and engineering problems. Financial support for the US\$10,000 award is provided through an endowment by Gordon Bell, a pioneer in high-performance and parallel computing. This article examines the evolution of the Gordon Bell Prize and the impact it has had on the field. },
	author = {Gordon Bell and David H Bailey and Jack Dongarra and Alan H Karp and Kevin Walsh},
	journal = {The International Journal of High Performance Computing Applications},
	number = {6},
	pages = {469-484},
	title = {{A look back on 30 years of the Gordon Bell Prize}},
	url = { https://doi.org/10.1177/1094342017738610},
	volume = {31},
	year = {2017}
}
%	doi = {10.1177/1094342017738610},
% eprint = {https://doi.org/10.1177/1094342017738610 },


@MISC{SystemChome:2017,
author = {IEEE/Accellera},
title = {{Systems initiative}},
year = {2017},
howpublished = {\url{http://www.accellera.org/downloads/standards/systemc}}
}



@article{Scienceexascale:2018,
	author = {Robert F. Service},
	title = {{Design for U.S. exascale computer takes shape}},
	journal = {Science},
	volume = {359},
	issue={6376},
	year = {2018},
	pages = {617--618},
}

%  volume    = {abs/1708.01462},
%  url       = {http://arxiv.org/abs/1708.01462},
%  archivePrefix = {arXiv},
%  eprint    = {1708.01462},
%  timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
%  biburl    = %{http://dblp.org/rec/bib/journals/corr/abs-1708-01462},
%  bibsource = {dblp computer science bibliography, http://dblp.org}

@article{FogCloudComputing:2018,
	author = {J. Du and L. Zhao and J. Feng and X. Chu},
	title = {{Computation Offloading and Resource Allocation in Mixed Fog/Cloud Computing Systems With Min-Max Fairness Guarantee}},
	journal = {IEEE Transactions on Communications},
	volume = {66},
	issue={4},
	year = {2018},
	pages = {1594--1608},
}

@book{FeynmanComputation:2018,
	author = {Feynman, R. P.},
	title = {Feynman Lectures on Computation},
	publisher = {CRC Press},
	year = {2018},
	isbn ={9780429500442}
}

%issn = "0167-8191",
%doi = "https://doi.org/10.1016/j.parco.2018.03.001",

@ARTICLE{SpikingPetascale2014,	
	AUTHOR={Kunkel, Susanne and Schmidt, Maximilian and Eppler, Jochen M. and Plesser, Hans E. and Masumoto, Gen and Igarashi, Jun and Ishii, Shin and Fukai, Tomoki and Morrison, Abigail and Diesmann, Markus and Helias, Moritz},   	
	TITLE={Spiking network simulation code for petascale computers},      	
	JOURNAL={Frontiers in Neuroinformatics},      	
	VOLUME={8},      	
	PAGES={78},     	
	YEAR={2014},      	
	DOI={10.3389/fninf.2014.00078},      	
	ISSN={1662-5196},   	
	ABSTRACT={Brain-scale networks exhibit a breathtaking heterogeneity in the dynamical properties and parameters of their constituents. At cellular resolution, the entities of theory are neurons and synapses and over the past decade researchers have learned to manage the heterogeneity of neurons and synapses with efficient data structures. Already early parallel simulation codes stored synapses in a distributed fashion such that a synapse solely consumes memory on the compute node harboring the target neuron. As petaflop computers with some 100,000 nodes become increasingly available for neuroscience, new challenges arise for neuronal network simulation software: Each neuron contacts on the order of 10,000 other neurons and thus has targets only on a fraction of all compute nodes; furthermore, for any given source neuron, at most a single synapse is typically created on any compute node. From the viewpoint of an individual compute node, the heterogeneity in the synaptic target lists thus collapses along two dimensions: the dimension of the types of synapses and the dimension of the number of synapses of a given type. Here we present a data structure taking advantage of this double collapse using metaprogramming techniques. After introducing the relevant scaling scenario for brain-scale simulations, we quantitatively discuss the performance on two supercomputers. We show that the novel architecture scales to the largest petascale supercomputers available today.}
}    

@misc{PruningNeuralNetworks:2015,	
	AUTHOR={ Han, Song and  Pool, Jeff and Tran, John  and  Dally, William J.},   	
	TITLE={{Learning both Weights and Connections for Efficient
	Neural Networks}},      	
	JOURNAL={ArXiv},      	
	YEAR={2015},      	
	ABSTRACT={Neural networks are both computationally intensive and memory intensive, making
	them difficult to deploy on embedded systems. Also, conventional networks fix
	the architecture before training starts; as a result, training cannot improve the
	architecture. To address these limitations, we describe a method to reduce the
	storage and computation required by neural networks by an order of magnitude
	without affecting their accuracy by learning only the important connections. Our
	method prunes redundant connections using a three-step method. First, we train
	the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the
	remaining connections. On the ImageNet dataset, our method reduced the number
	of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without
	incurring accuracy loss. Similar experiments with VGG-16 found that the total
	number of parameters can be reduced by 13×, from 138 million to 10.3 million,
	again with no loss of accuracy.},
	howpublished={\url{https://arxiv.org/pdf/1506.02626.pdf}}
}    


@ARTICLE{DeepLearning:2015,
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
   year = {2015},
	title = {Deep learning},
 	journal = {Nature},
 	pages = {436--444},
 	volume = {521},
	issue = {7553},
	abstract = { Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
 	doi = {10.1038/nature14539}
}

%URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00291},       
%DOI={10.3389/fnins.2018.00291},      
%ISSN={1662-453X},   

@inproceedings{DeepNeuralNetworkTraining:2016,
	author = { Janis Keuper and Franz-Josef Pfreundt},
	title = {{Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability}},
	booktitle = {2nd Workshop on Machine Learning in HPC Environments (MLHPC)},
	year = {2016},
	pages = {1469--1476},
	numpages = {11},
	publisher = {IEEE},
	doi = {10.1109/MLHPC.2016.006},
	url = {https://www.researchgate.net/publication/308457837}
} 




@MISC{Saturating:2017,
	author = {Stefan Hamminga},
	title = {{C++ saturating arithmetic functions and types}},
	year = {2017},
	howpublished = {\url{https://github.com/StefanHamminga/saturating}}
}


@inproceedings{HalfPrecisionArithmetic:2017,
	author = {Haidar, Azzam and Wu, Panruo and Tomov, Stanimire and Dongarra, Jack},
	title = {{Investigating Half Precision Arithmetic to Accelerate Dense Linear System Solvers}},
	booktitle = {{Proceedings of the 8th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems, Denver, Colorado}},
	series = {ScalA '17},
	year = {2017},
	pages = {10:1--10:8},
	articleno = {10},
	numpages = {8},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {GPGPU, HPC, half precision, mixed-precision iterative refinement},
} 
%	isbn = {978-1-4503-5125-6},
%	url = {http://doi.acm.org/10.1145/3148226.3148237},
% doi = {10.1145/3148226.3148237},
% acmid = {3148237},


@MISC{IBMAsynchronousAPI2017,
	author = {Gohil, P and  Horn, J and He, J and  Papageorgiou, A and Poole,C},
	title = {{IBM CICS Asynchronous API: Concurrent Processing Made Simple}},
	year = {2017},
	howpublished = {
	\url{http://www.redbooks.ibm.com/redbooks/pdfs/sg248411.pdf
	}
	}
}


@misc{AIrobustness:2018,
	title={Robust Physical-World Attacks on Deep Learning Models}, 
	author={Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Song},
	year={2018},
	eprint={1707.08945},
	archivePrefix={arXiv},
	primaryClass={cs.CR}
}
@article{patent:IBMNeural2018,
	title     = " GRAPH PARTITIONING AND PLACEMENT FOR MULTI - CHIP NEUROSYNAPTIC NETWORKS",
	number    = "0260682",
	author    = "Arnon Amir and Pallab Datta  and Myron D . Flickner  and Dharmendra S . Modha and  Tapan K . Nayak",
	year      = "2017",
	month     = "Mar",
	url       = "https://patentimages.storage.googleapis.com/8e/51/4a/204809be525db9/US20180260682A1.pdf"
}

@article{patent:FacebookNeural2018,
	title     = "  SEQUENCE - TO - SEQUENCE CONVOLUTIONAL ARCHITECTURE",
	number    = "0261214",
	author    = "Jonas Gehring and Michael Auli and Yann Nicolas Dauphin and David G . Grangier and Dzianis Yarats)",
	year      = "2017",
	month     = "Dec",
	url       = "https://https://patentimages.storage.googleapis.com/6a/67/30/1d5b7f99f4b1f8/US20180261214A1.pdf"
}

@article{patent:GoogleNeural2018,
	title     = "  SEQUENCE - TO - SEQUENCE CONVOLUTIONAL ARCHITECTURE",
	number    = "0260220",
	author    = "William  Lacy and Gregory Michael Thorson and Christopher Aaron Clark and Norman Paul Jouppi and Thomas Norrie and Andrew Everett Phelps",
	year      = "2017",
	month     = "Mar",
	url       = "https://patentimages.storage.googleapis.com/ae/ec/c7/b6c9311ebee07b/US20180260220A1.pdf"
}

@article{RejectingMemristor:2018,
	author = {Abraham, I},
	title = {{The case for rejecting the memristor as a fundamental circuit element}},
	journal = {Nature Scientific Reports},
	volume = {8},
	year = {2018},
	pages = {10972},
	publisher = {ACM},
	doi = {10.1038/s41598-018-29394-7},
} 


@article{TaihulightHPCG:2018,
	author = {Ao, Yulong and Yang, Chao and Liu, Fangfang and Yin, Wanwang and Jiang, Lijuan and Sun, Qiao},
	title = {{Performance Optimization of the HPCG Benchmark on the Sunway TaihuLight Supercomputer}},
	journal = {ACM Trans. Archit. Code Optim.},
	issue_date = {April 2018},
	volume = {15},
	number = {1},
	month = mar,
	year = {2018},
	pages = {11:1--11:20},
	articleno = {11},
	numpages = {20},
	acmid = {3182177},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {HPCG, Sunway TaihuLight, heterogeneous many-core processor, performance optimization},
} 


@article{patent:20180189231,
	title     = "PROCESSORS, METHODS, AND SYSTEMS WITH A CONFIGURABLE SPATIAL ACCELERATOR",
	number    = "20180189231",
	author    = "Fleming Jr. and Kermin E.  and Glossop, Kent D.  and Steely Jr., Simon C.  and Tang, Jinjie  and Gara, Alan G.",
	year      = "2018",
	month     = "July",
	url       = "http://www.freepatentsonline.com/y2018/0189231.html",
		note = {Accessed: 2023-09-10}
	
}


@MISC{CosmicRaysSupercomputers:2018,
	author = {wired,com},
	title = {{Cosmic Ray Showers Crash Supercomputers. Here's What to Do About It}},
	year = {2018},
	howpublished = {\url{https://www.wired.com/story/cosmic-ray-showers-crash-supercomputers-heres-what-to-do-about-it/
	}}
}

@MISC{IntelSectretExascale:2017,
	author = {nextplatform.com},
	title = {{Looking Ahead to Intel’s Secret Exascale Architecture}},
	year = {2017},
	howpublished = {\url{https://www.nextplatform.com/2017/11/14/looking-ahead-intels-secret-exascale-architecture/
	}}
}

@MISC{KeepingComputerIndustryInUS:2017,
	author = {T. M. Conte and E. P. Debenedictis and R. S. William sand  M. D. Hill},
	title = {{Challenges to Keeping the Computer Industry Centered in the US}},
	year = {2017},
	howpublished = {\url{https://arxiv.org/abs/1706.10267
	}}
}



@MISC{IntelSwitchableTopology:2018,
	author = {Intel},
	title = {{Switchable topology machine}},
	year = {2016},
	howpublished = {\url{https://patents.google.com/patent/US20180113838A1
	}}
}


@MISC{NokiaParallelMulticore:2013,
	author = {Nokia},
	title = {{Method, apparatus, and computer program product for parallel functional units in multicore processors}},
	year = {2013},
	howpublished = {\url{https://patents.google.com/patent/US20130151817A1/
	}}
}

@MISC{HumanBrainProject:2018,
	author = {Human Brain Project, EU},
	title = {{Human Brain Project}},
	year = {2018},
	howpublished = {\url{https://www.humanbrainproject.eu/en/
	}}
}


@MISC{QuantumComputing:2018,
	author = {M. Dyakonov},
	title = {{The Case Against Quantum Computing}},
	year = {2018},
	howpublished = {\url{https://spectrum.ieee.org/computing/hardware/the-case-against-quantum-computing
	}}
}

@MISC{IntelDropsX86:2018,
	author = {nextplatform.com},
	title = {{Intel’s Exascale Dataflow Engine Drops X86 And Von Neumann}},
	year = {2018},
	howpublished = {\url{https://www.nextplatform.com/2018/08/30/intels-exascale-dataflow-engine-drops-x86-and-von-neuman/
	}}
}

@MISC{IntelDataflowPatent:2018,
	author = {Intel},
	title = {{Processors, methods and systems with a configurable spatial accelerator}},
	year = {2018},
	howpublished = {\url{http://www.freepatentsonline.com/y2018/0189231.html}},
					note = {Accessed: 2023-09-10}
	
}

@MISC{JapanExascale:2018,
author = {Extremtech},
	year = "2018",
	title = {{ Japan Tests Silicon for Exascale Computing in 2021.}},
howpublished="https://www.extremetech.com/computing/
	272558-japan-tests-silicon-for-exascale-computing -in-2021"
}

@ARTICLE{VeghMendedCores:2018,
	author = {J. V\'egh and J. V\'as\'arhelyi},
	title =  {{Can Broken Multicore Hardware be Mended?}},
	journal = {	Global Journal of Researches in Engineering },
	year = {2018},
	volume = {18},
	issue = {1},
	pages = {15--21}
}
%ISSN: 2249-4596


@article{ChinaExascale:2018,
	abstract = "High-performance computing (HPC) is essential for both traditional and emerging scientific fields, enabling scientific activities to make progress. With the development of high-pThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	erformance computing, it is foreseeable that exascale computing will be put into practice around 2020. As Moore's law approaches its limit, high-performance computing will face severe challenges when moving from exascale to zettascale, making the next 10 years after 2020 a vital period to develop key HPC techniques. In this study, we discuss the challenges of enabling zettascale computing with respect to both hardware and software. We then present a perspective of future HPC technology evolution and revolution, leading to our main recommendations in support of zettascale computing in the coming future.",
	author = "{Liao, Xiang-ke et al}",
	doi = "10.1631/FITEE.1800494",
	issn = "2095-9230",
	journal = "Frontiers of Information Technology {\&} Electronic Engineering",
	number = "10",
	pages = "1236--1244",
	title = "Moving from exascale to zettascale computing: challenges and techniques",
	url = "https://doi.org/10.1631/FITEE.1800494",
	volume = "19",
	year = "2018"
}

%	author = "{Liao, Xiang-ke and Lu, Kai and Yang, Can-qun and Li, Jin-wen and Yuan, Yuan and Lai, Ming-che and Huang, Li-bo and Lu, Ping-jing and Fang, Jian-bin and Ren, Jing and Shen, Jie}",

   

%https://www.zdnet.com/article/neuromorphic-computing-and-the-brain-that-wouldnt-die/
%https://www.zdnet.com/article/the-rise-fall-and-rise-of-the-supercomputer-in-the-cloud-era
%https://www.zdnet.com/article/supercomputing-and-ai-meets-the-cloud/
%https://features.slashdot.org/story/99/05/27/0126232/the-power-of-deep-computing
%

@article{CommunicationCollapse:2018,
	year = 2018,
	month = {oct},
	publisher = {{IOP} Publishing},
	volume = {52},
	number = {1},
	pages = {014003},
	author = {Saber Moradi and Rajit Manohar},
	title = {{The impact of on-chip communication on memory technologies for neuromorphic systems}},
	journal = {Journal of Physics D: Applied Physics},
	abstract = {Emergent nanoscale non-volatile memory technologies with high integration density offer a promising solution to overcome the scalability limitations of CMOS-based neural networks architectures, by efficiently exhibiting the key principle of neural computation. Despite the potential improvements in computational costs, designing high-performance on-chip communication networks that support flexible, large-fanout connectivity remains as daunting task. In this paper, we elaborate on the communication requirements of large-scale neuromorphic designs, and point out the differences with the conventional network-on-chip architectures. We present existing approaches for on-chip neuromorphic routing networks, and discuss how new memory and integration technologies may help to alleviate the communication issues in constructing next-generation intelligent computing machines.}
}
% https://arxiv.org/pdf/1809.06016.pdf
%	doi = {10.1088/1361-6463/aae641},
%url = {https://doi.org/10.1088\%2F1361-6463\%2Faae641},

@article{ScienceQuantumComputers:2018,
	year = 2018,
	volume = {364},
	number = {6447},
	pages = {1218-1219},
	author = {Adrian Cho},
	title = {{Tests measure progress of quantum computers}},
	journal = {Science},
	doi = {10.1126/science.364.6447.1218}
}


@article{LiaoZettaScale:2018,
	abstract = "High-performance computing (HPC) is essential for both traditional and emerging scientific fields, enabling scientific activities to make progress. With the development of high-pThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	erformance computing, it is foreseeable that exascale computing will be put into practice around 2020. As Moore's law approaches its limit, high-performance computing will face severe challenges when moving from exascale to zettascale, making the next 10 years after 2020 a vital period to develop key HPC techniques. In this study, we discuss the challenges of enabling zettascale computing with respect to both hardware and software. We then present a perspective of future HPC technology evolution and revolution, leading to our main recommendations in support of zettascale computing in the coming future.",
	author = "Liao, Xiang-ke and Lu, Kai and Yang, Can-qun and Li, Jin-wen and Yuan, Yuan and Lai, Ming-che and Huang, Li-bo and Lu, Ping-jing and Fang, Jian-bin and Ren, Jing and Shen, Jie",
	day = "01",
	doi = "10.1631/FITEE.1800494",
	issn = "2095-9230",
	journal = "Frontiers of Information Technology {\&} Electronic Engineering",
	number = "10",
	pages = "1236–1244",
	title = "Moving from exascale to zettascale computing: challenges and techniques",
	url = "https://doi.org/10.1631/FITEE.1800494",
	volume = "19",
	year = "2018"
}

@inproceedings{MixedPrecisionHPL:2018,
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	title = {{Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed Up Mixed-precision Iterative Refinement Solvers}},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
	series = {SC '18},
	year = {2018},
	location = {Dallas, Texas},
	pages = {47:1--47:11},
	articleno = {47},
	numpages = {11},
	publisher = {IEEE Press},
	keywords = {FP16 arithmetic, GPU computing, half precision, iterative refinement computation, linear algebra, mixed precision solvers},
} 
%	url = {https://doi.org/10.1109/SC.2018.00050},
%doi = {10.1109/SC.2018.00050},
%acmid = {3291719},
%	address = {Piscataway, NJ, USA},

@MISC{ExascaleGrandfatherHPC:2019,
	author = {nextplatform.com},
	title = {{Exascale Is Not Your Grandfather’s HPC}},
	year = {2019},
	howpublished = {\url{https://www.nextplatform.com/2019/10/22/exascale-is-not-your-grandfathers-hpc/}}
}
@MISC{GustafsonFloating:2018,
	author = {John L. Gustafson and Isaac Yonemoto},
	title = {{Beating Floating Point at its Own Game: Posit Arithmetic}},
	doi = {10.14529/jsﬁ170206},
	year = {2018},
	howpublished = {\url{http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf}}
}

@MISC{WiringDominance:2019,
	author = {Waine Luk},
	title = {{Imperial College London, textbook}},
	lab =  {{Imperial College London}},
	year = {2019},
	howpublished = {\url{http://www.imperial.ac.uk/~wl/teachlocal/cuscomp/notes/chapter2.pdf} (Accessed on Dec 14, 2020)}
}

http://www.imperial.ac.uk/~wl/teachlocal/cuscomp/notes/chapter2.pdf

@ARTICLE{SpiNNaker2:2018,
	
	AUTHOR={Liu, Chen and Bellec, Guillaume and Vogginger, Bernhard and Kappel, David and Partzsch, Johannes and Neumärker, Felix and Höppner, Sebastian and Maass, Wolfgang and Furber, Steve B. and Legenstein, Robert and Mayr, Christian G.},   
	TITLE={{Memory-Efficient Deep Learning on a SpiNNaker 2 Prototype}},      
	JOURNAL={Frontiers in Neuroscience},      
	VOLUME={12},      
	PAGES={840},     
	YEAR={2018},      
	URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00840},    	
	DOI={10.3389/fnins.2018.00840},      	
	ISSN={1662-453X},   	
	ABSTRACT={The memory requirement of deep learning algorithms is considered incompatible with the memory restriction of energy-efficient hardware. A low memory footprint can be achieved by pruning obsolete connections or reducing the precision of connection strengths after the network has been trained. Yet, these techniques are not applicable to the case when neural networks have to be trained directly on hardware due to the hard memory constraints. Deep Rewiring (DEEP R) is a training algorithm which continuously rewires the network while preserving very sparse connectivity all along the training procedure. We apply DEEP R to a deep neural network implementation on a prototype chip of the 2nd generation SpiNNaker system. The local memory of a single core on this chip is limited to 64 KB and a deep network architecture is trained entirely within this constraint without the use of external memory. Throughout training, the proportion of active connections is limited to 1.3%. On the handwritten digits dataset MNIST, this extremely sparse network achieves 96.6% classification accuracy at convergence. Utilizing the multi-processor feature of the SpiNNaker system, we found very good scaling in terms of computation time, per-core memory consumption, and energy constraints. When compared to a X86 CPU implementation, neural network training on the SpiNNaker 2 prototype improves power and energy consumption by two orders of magnitude.}
}

@MISC{OpenCAPI:2019,
	author = {IBM},
	title = {{Why IBM sees OpenCAPI and OMI as the future for accelerator-driven computing}},
	year = {2019},
	howpublished = {\url{https://www.techrepublic.com/article/why-ibm-sees-opencapi-and-omi-as-the-future-for-accelerator-driven-computing/}}
}

@MISC{ToolingUpForExascale:2019,
	author = {nextplatform.com},
	title = {{Tooling Up For Exascale}},
	year = {2019},
	howpublished = {\url{https://www.nextplatform.com/2019/11/12/tooling-up-for-exascale/}}
}

%https://sciencenode.org/feature/the-race-to-exascale.php


@inbook{ENIACinaction:2016,
	author = {Haigh, Thomas and Priestley, Mark and Rope, Crispin},
	title = {{ENIAC in Action -- Making and Remaking the Modern Computer}}, 
	booktitle = {{The Computer and the Brain}}, 
	publisher = {MIT Press, Cambridge, Massachusetts},
	year = {2016},
	isbn = {978-0262033985},
}

@article{MemristorTestPershin:2019,
	doi = {10.1088/1361-6463/aae680},
	url = {https://dx.doi.org/10.1088/1361-6463/aae680},
	year = {2018},
	month = {oct},
	publisher = {IOP Publishing},
	volume = {52},
	number = {1},
	pages = {01LT01},
	author = {Y V Pershin and M Di Ventra},
	title = {A simple test for ideal memristors},
	journal = {Journal of Physics D: Applied Physics},
	abstract = {An ideal memristor is defined as a resistor with memory that, when subjected to a time-dependent current, , its resistance RM(q) depends only on the charge q that has flowed through it, so that its voltage response is . It has been argued that a clear fingerprint of these ideal memristors is a pinched hysteresis loop in their I– curves. However, a pinched I– hysteresis loop is not a definitive test of whether a resistor with memory is truly an ideal memristor because such a property is shared also by other resistors whose memory depends on additional internal state variables, other than the charge. Here, we introduce a very simple and unambiguous test that can be utilized to check experimentally if a resistor with memory is indeed an ideal memristor. Our test is based on the duality property of a capacitor-memristor circuit whereby, for any initial resistance states of the memristor and any form of the applied voltage, the final state of an ideal memristor must be identical to its initial state, if the capacitor charge finally returns to its initial value. In actual experiments, a sufficiently wide range of voltage amplitudes and initial states are enough to perform the test. The proposed test can help resolve some long-standing controversies still existing in the literature about whether an ideal memristor does actually exist or it is a purely mathematical concept.}
}



@INPROCEEDINGS{RebootingComputingModels:2019,	
	author={Cadareanu, P. and Reddy C, N. and Almudever, C. G. and Khanna, A. and Raychowdhury, A. and Datta, S. and Bertels, K. and Narayanan, V. and Ventra, M. Di and Gaillardon, P.-E.},	
	booktitle={2019 Design, Automation   Test in Europe Conference   Exhibition (DATE) Florence, Italy; 25-29 March 2019}, 	
	title={Rebooting Our Computing Models}, 	
	year={2019},
	pages={1469-1476},	
	doi={10.23919/DATE.2019.8715167}
}

@INPROCEEDINGS{8715167, author={Cadareanu, P. and Reddy C, N. and Almudever, C. G. and Khanna, A. and Raychowdhury, A. and Datta, S. and Bertels, K. and Narayanan, V. and Ventra, M. Di and Gaillardon, P.-E.}, booktitle={2019 Design, Automation Test in Europe Conference Exhibition (DATE)}, title={Rebooting Our Computing Models}, year={2019}, volume={}, number={}, pages={1469-1476}, doi={10.23919/DATE.2019.8715167}} 

@techreport{DongarraFugakuSystem:2020,
	author = {Jack Dongarra},
	title = {{Report on the Fujitsu Fugaku System}},
	institution = {University of Tennessee
	Department of Electrical Engineering and Computer Science},
	year = {2016},
	number = {Tech Report ICL-UT-20-06},
}
%	url = {\url{http://bit.ly/fugaku-report}},

%https://www.bnl.gov/modsim2019/files/talks/SatoshiMatsuoka.pdf
%https://www.cnet.com/news/fujitsu-supercomputer-simulates-1-second-of-brain-activity/

@article{QuantumPrimeFactorization:2020,
	author = "Wang, B. and Hu, F. and Yao, H. and Wang, C.",
	doi = "10.1038/s41598-020-62802-5",
	journal = "Sci Rep",
	title = "Prime factorization algorithm based on parameter optimization of Ising model",
	volume = {10},
	issue = {7106},
	year = {2021}
}

@article{VerificationBrainScaleS:2020,
	author = {Andreas Grubl and Sebastian Billaudelle and Benjamin Cramer and Vitali Karasenko and Johannes Schemmel 
	},
	title = {{Verification and Design Methods for the BrainScaleS Neuromorphic Hardware System}},
	journal = {Journal of Signal Processing Systems},
	volume = {92},
	year = {2020},
	pages = {1277--1292},
	doi = "10.1007/s11265-020-01558-7"
}
% https://link.springer.com/article/10.1007/s11265-020-01558-7
%Online learning can be implemented in software, which also results in a performance penalty [16]
% https://www.researchgate.net/publication/342408921_Inference_with_Artificial_Neural_Networks_on_the_Analog_BrainScaleS-2_Hardware


@article{BeyondVonNeumann:2020,
	author = "Wang, Cong and Liang, Shi-Jun and Wang, Chen-Yu
	and Yang, Zai-Zheng and Ge, Yingmeng and Pan, Chen and Shen, Xi
	and Wei, Wei and Zhao, Yichen and Zhang, Zaichen and Cheng, Bin
	and Zhang, Chuan and Miao, Feng",
	abstract = {Data-centric computation and the scalability limits of current computing systems call for the developments of alternative to von Neumann architecture.},
	doi = "10.1038/s41565-020-0738-x",
	journal = "Nature Nanotechnology",
	title = "Beyond von Neumann",
	pages ={507},
	url = "https://doi.org/10.1038/s41565-020-0738-x",
	year = "2020"
}


@article{EuroBrainImplosion:2020,
	author = {Nature},
	title = {{Documentary follows implosion of billion-euro brain project}},
	journal = {Nature},
	volume = {588},
	year = {2020},
	pages = {215-216},
	doi = "10.1038/d41586-020-03462-3"
}



@article{AnalogMemristor:2020,
	author = {Krestinskaya, O. and Choubey, B. and James, A. P.},
	year = {2020},
	title = {{Memristive GAN in Analog}},
	journal = {Scientific Reports},
	page = {5838},
	volume = {10},
	issue = {1},
	abstract = {Generative Adversarial Network (GAN) requires extensive computing resources making its implementation in edge devices with conventional microprocessor hardware a slow and difficult, if not impossible task. In this paper, we propose to accelerate these intensive neural computations using memristive neural networks in analog domain. The implementation of Analog Memristive Deep Convolutional GAN (AM-DCGAN) using Generator as deconvolutional and Discriminator as convolutional memristive neural network is presented. The system is simulated at circuit level with 1.7 million memristor devices taking into account memristor non-idealities, device and circuit parameters. The design is modular with crossbar arrays having a minimum average power consumption per neural computation of 47nW. The design exclusively uses the principles of neural network dropouts resulting in regularization and lowering the power consumption. The SPICE level simulation of GAN is performed with 0.18 μm CMOS technology and WOx memristive devices with RON = 40 kΩ and ROFF = 250 kΩ, threshold voltage 0.8 V and write voltage at 1.0 V.},
	url = {https://doi.org/10.1038/s41598-020-62676-7},
	doi = {10.1038/s41598-020-62676-7},
}


@book{AnalogMemristorBook:2021,
	title = {Memristor and Memristive Neural Networks},
	ISBN = {978-1-83968-957-4},
	url = {https://doi.org/10.5772/intechopen.92517},
	DOI = {10.5772/intechopen.92517},
	publisher = {IntechOpen},
	year = {2021},
	month = {Nov},
	address = {Rijeka},
	author = {Yao-Feng Chang},
} 

@article{DAngeloNetwork:2021,
	title = {Network traffic classification using deep convolutional recurrent autoencoder neural networks for spatial–temporal features extraction},
	journal = {Journal of Network and Computer Applications},
	volume = {173},
	pages = {102890},
	year = {2021},
	doi = {https://doi.org/10.1016/j.jnca.2020.102890},
	author = {Gianni D’Angelo and Francesco Palmieri},
	keywords = {Network traffic classification, Features extraction, Long Short-Term Memory, Recurrent neural networks, Convolutional neural networks, ConvLSTM},
	abstract = {The right choice of features to be extracted from individual or aggregated observations is an extremely critical factor for the success of modern network traffic classification approaches based on machine learning. Such activity, usually in charge of the designers of the classification scheme is strongly related to their experience and skills, and definitely characterizes the whole approach, implementation strategy as well as its performance. The main aim of this work is supporting this process by mining new and more expressive, meaningful and discriminating features from the basic ones without human intervention. For this purpose, a novel autoencoder-based deep neural network architecture is proposed where multiple autoencoders are embedded with convolutional and recurrent neural networks to elicit relevant knowledge about the relations existing among the basic features (spatial-features) and their evolution over time (temporal-features). Such knowledge, consisting in new properties that are not immediately evident and better represent the most hidden and representative traffic dynamics can be successfully exploited by machine learning-based classifiers. Different network combinations are analyzed both from a theoretical perspective, and through specific performance evaluation experiments on a real network traffic dataset. We show that the traffic classifier obtained by stacking the autoencoder with a fully-connected neural network, achieves up to a 28% improvement in average accuracy over state-of-the-art machine learning-based approaches, up to a 10% over pure convolutional and recurrent stacked neural networks, and 18% over pure feed-forward networks. It is also able to maintain high accuracy even in the presence of unbalanced training datasets.}
}


% https://www.ncbi.nlm.nih.gov/books/NBK92848/
@ARTICLE{LoihiSurvey:2021,
	author={Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
	journal={Proceedings of the IEEE}, 	
	title={Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook}, 	
	year={2021},	
	volume={109},	
	number={5},
	pages={911-934},	
	doi={10.1109/JPROC.2021.3067593}
}



@article{ContinuousTimeCrossbarMemristor:2021,
	author = "Wang, Cong and Liang, Shi-Jun and Wang, Chen-Yu
	and Yang, Zai-Zheng and Ge, Yingmeng and Pan, Chen and Shen, Xi
	and Wei, Wei and Zhao, Yichen and Zhang, Zaichen and Cheng, Bin
	and Zhang, Chuan and Miao, Feng",
	doi = "10.1038/s41565-021-00943-y",
	journal = "Nature Nanotechnology",
	title = "Scalable massively parallel computing using continuous-time data representation in nanoscale crossbar array",
	url = "https://doi.org/10.1631/FITEE.1800494",
	year = "2021"
}


@inproceedings{GPUalgorithm:2021,
	author = {Fiscale, S. and De Luca, P. and Inno, L. and Marcellino, L. and Galletti, A. and Rotundi, A. and \dots  Quintana, E.},
	title = {{A GPU Algorithm for Outliers Detection in TESS Light Curves}},
	booktitle = {{In International Conference on Computational Science}},
	year = {2021},
	pages = {420-432},
	doi = {10.1007/978-3-030-77977-1\_34},
	publisher = {Springer, Cham},
} 

@ARTICLE{LoihiSurvey:2021,
	author={Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
	journal={Proceedings of the IEEE}, 	
	title={Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook}, 	
	year={2021},	
	volume={109},	
	number={5},
	pages={911-934},	
	doi={10.1109/JPROC.2021.3067593}
}


@Article{ConservationOfInformation:2021,
	AUTHOR = {Cengel, Yunus A.},
	TITLE = {On Entropy, Information, and Conservation of Information},
	JOURNAL = {Entropy},
	VOLUME = {23},
	YEAR = {2021},
	NUMBER = {6},
	ARTICLE-NUMBER = {779},
	URL = {https://www.mdpi.com/1099-4300/23/6/779},
	PubMedID = {34205309},
	ISSN = {1099-4300},
	ABSTRACT = {The term entropy is used in different meanings in different contexts, sometimes in contradictory ways, resulting in misunderstandings and confusion. The root cause of the problem is the close resemblance of the defining mathematical expressions of entropy in statistical thermodynamics and information in the communications field, also called entropy, differing only by a constant factor with the unit ‘J/K’ in thermodynamics and ‘bits’ in the information theory. The thermodynamic property entropy is closely associated with the physical quantities of thermal energy and temperature, while the entropy used in the communications field is a mathematical abstraction based on probabilities of messages. The terms information and entropy are often used interchangeably in several branches of sciences. This practice gives rise to the phrase conservation of entropy in the sense of conservation of information, which is in contradiction to the fundamental increase of entropy principle in thermodynamics as an expression of the second law. The aim of this paper is to clarify matters and eliminate confusion by putting things into their rightful places within their domains. The notion of conservation of information is also put into a proper perspective.},
	DOI = {10.3390/e23060779}
}


@article{FittingElephants:2021,
	author = {Mitra, P.P.},
	title = {Fitting elephants in modern machine learning by statistically consistent interpolation.},
	journal = {Nature Machine Intelligence},
	volume = {3},
	year = {2021},
	page = {378–386},
	doi = {10.1038/s42256-021-00345-8},
}   


@ARTICLE{MemristorNano:2021,	
	AUTHOR={Xu, Weilin and Wang, Jingjuan and Yan, Xiaobing},   	
	TITLE={{Advances in Memristor-Based Neural Networks}},      
	JOURNAL={Frontiers in Nanotechnology},      	
	VOLUME={3},           	
	YEAR={2021},      	
	DOI={10.3389/fnano.2021.645995},      	
	ABSTRACT={The rapid development of artificial intelligence (AI), big data analytics, cloud computing, and Internet of Things applications expect the emerging memristor devices and their hardware systems to solve massive data calculation with low power consumption and small chip area. This paper provides an overview of memristor device characteristics, models, synapse circuits, and neural network applications, especially for artificial neural networks and spiking neural networks. It also provides research summaries, comparisons, limitations, challenges, and future work opportunities.}
}

@misc{QuantumRandom:2021,
	author = {	Moosbrugger, Marcel},
	journal = {Towards Data Science},
	year = {2021},
	howpublished = {\url{
https://towardsdatascience.com/flip-a-coin-on-a-real-quantum-computer-in-python-df51e5f2367b}}
}

@misc{QuantumIBM:2023,
	author = {	IBM},
	journal = {Basics of quantum information},
	year = {2023},
	howpublished = {\url{
	https://docs.quantum-computing.ibm.com/start/hello-world}}
}

@article{DynamicMemristor:2022,
	author = {Park, See-On and Jeong, Hakcheon and Park, Jongyong
	and Bae, Jongmin and Choi, Shinhyun},
	year ={2022},
	title = {Experimental demonstration of highly reliable dynamic memristor for artificial neuron and neuromorphic computing},
	jornal = {Nature Communications},
	page = {2888},
	volume = {13},
	issue = {1},
	abstract = {Neuromorphic computing, a computing paradigm inspired by the human brain, enables energy-efficient and fast artificial neural networks. To process information, neuromorphic computing directly mimics the operation of biological neurons in a human brain. To effectively imitate biological neurons with electrical devices, memristor-based artificial neurons attract attention because of their simple structure, energy efficiency, and excellent scalability. However, memristor’s non-reliability issues have been one of the main obstacles for the development of memristor-based artificial neurons and neuromorphic computings. Here, we show a memristor 1R cross-bar array without transistor devices for individual memristor access with low variation, 100% yield, large dynamic range, and fast speed for artificial neuron and neuromorphic computing. Based on the developed memristor, we experimentally demonstrate a memristor-based neuron with leaky-integrate and fire property with excellent reliability. Furthermore, we develop a neuro-memristive computing system based on the short-term memory effect of the developed memristor for efficient processing of sequential data. Our neuro-memristive computing system successfully trains and generates bio-medical sequential data (antimicrobial peptides) while using a small number of training parameters. Our results open up the possibility of memristor-based artificial neurons and neuromorphic computing systems, which are essential for energy-efficient edge computing devices.},
	doi = {10.1038/s41467-022-30539-6},
}


@ARTICLE{MemristorBasedAnalogue:2022,
	author = {Gao, Bin and Zhou, Ying and Zhang, Qingtian and
	Zhang, Shuanglin and Yao, Peng and Xi, Yue and Liu, Qi and
	Zhao, Meiran and Zhang, Wenqiang and Liu, Zhengwu and
	Li, Xinyi and Tang, Jianshi and Qian, He and
	Wu, Huaqiang},
	year = {2022},
	title = {Memristor-based analogue computing for brain-inspired sound localization with in situ training},
	journal = {Nature Communications},
	page = {2026},
	volume = {13},
	abstract = {The human nervous system senses the physical world in an analogue but efficient way. As a crucial ability of the human brain, sound localization is a representative analogue computing task and often employed in virtual auditory systems. Different from well-demonstrated classification applications, all output neurons in localization tasks contribute to the predicted direction, introducing much higher challenges for hardware demonstration with memristor arrays. In this work, with the proposed multi-threshold-update scheme, we experimentally demonstrate the in-situ learning ability of the sound localization function in a 1K analogue memristor array. The experimental and evaluation results reveal that the scheme improves the training accuracy by ∼45.7% compared to the existing method and reduces the energy consumption by ∼184× relative to the previous work. This work represents a significant advance towards memristor-based auditory localization system with low energy consumption and high performance.},
	doi =  {10.1038/s41467-022-29712-8},
}

@MISC{FrontierHPCwire:2022,
	author = {hpcwire.com},
	title = {{TOP500: Exascale Is Officially Here with Debut of Frontier}},
	journal = {HPCwire},
	year = {2022},
	howpublished = {\url{https://www.hpcwire.com/2022/05/30/top500-exascale-is-officially-here-with-debut-of-frontier/
	}},
		note = {Accessed: 2023-09-10}
} 

@MISC{NextPlatformAMDAldebaran:2022,
	author = {nextplatform.com},
	title = {{The AMD “Aldebaran” GPU That Won Exascale}},
	journal = {NextPlatform},
	year = {2022},
	howpublished = {https://www.nextplatform.com/2021/11/09/the-aldebaran-amd-gpu-that-won-exascale/
	},
} 


@misc{NextPlatformHPCExascaleBarrier:2022,
	title = {{At Long Last, HPC Officially Breaks The Exascale Barrier}},
	author = {nextplatform.com},
	year = {2022},
	howpublished = {\url{https://www.nextplatform.com/2022/05/30/at-long-last-hpc-officially-breaks-the-exascale-barrier/
	}},
} 

@MISC{NextPlatformFrontierStepByStep:2022,
	title = {{Frontier: Step By Step, Over Decades, To Exascale}},
	author = {nextplatform.com},
	year = {2022},
	url = {https://www.nextplatform.com/2022/05/30/frontier-step-by-step-over-decades-to-exascale/
	},
} 


@MISC{HPCWireFrontierStepByStep:2022,
	title = {{Frontier: Step By Step, Over Decades, To Exascale}},
	author = {nextplatform.com},
	year = {2022},
	url = {https://www.nextplatform.com/2022/05/30/frontier-step-by-step-over-decades-to-exascale/
	},
} 

@MISC{NextPlatformFrontierSlingshot:2022,
	title = {{Cray’s Slingshot Interconnect Is At The Heart Of HPE’s HPC And AI Ambitions}},
	author = {nextplatform.com},
	year = {2022},
	url = {https://www.nextplatform.com/2022/01/31/crays-slingshot-interconnect-is-at-the-heart-of-hpes-hpc-and-ai-ambitions/
	},
} 

@MISC{ORNLFrontier:2022,
	title = {{Frontier supercomputer debuts as world's fastest, breaking exascale barrier}},
	author = {ORNL},
	year = {2022},
	url = {https://www.ornl.gov/news/frontier-supercomputer-debuts-worlds-fastest-breaking-exascale-barrier	
	},
} 


@INPROCEEDINGS {MixedPrecisionFrontierSummit:2022,
	author = {H. Lu and M. Matheson and V. Oles and A. Ellis and W. Joubert and F. Wang},
	booktitle = {2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC) (SC)},
	title = {Climbing the Summit and Pushing the Frontier of Mixed Precision Benchmarks at Extreme Scale},
	year = {2022},
	volume = {},
	issn = {2167-4337},
	pages = {1123-1137},
	abstract = {The rise of machine learning (ML) applications and their use of mixed precision to perform interesting science are driving forces behind AI for science on HPC. The convergence of ML and HPC with mixed precision offers the possibility of transformational changes in computational science. The HPL-AI benchmark is designed to measure the performance of mixed precision arithmetic as opposed to the HPL benchmark which measures double precision performance. Pushing the limits of systems at extreme scale is nontrivial —little public literature explores optimization of mixed precision computations at this scale. In this work, we demonstrate how to scale up the HPLAI benchmark on the pre-exascale Summit and exascale Frontier systems at the Oak Ridge Leadership Computing Facility (OLCF) with a cross-platform design. We present the implementation, performance results, and a guideline of optimization strategies employed for delivering portable performance on both AMD and NVIDIA GPUs at extreme scale.},
	keywords = {parallel programming;high performance computing;exascale computing;linear algebra},
	doi = {},
	url = {https://doi.ieeecomputersociety.org/},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}


%For a short period, von Neumann becomes a close collaborator of the team working at the Moore School.
%He arrives too late to have any known influence on the original design of ENIAC, but he studies it closely and is responsible for the problems run on the machine,

%FrontierOakRidge:2022,
https://www.ornl.gov/news/frontier-supercomputer-debuts-worlds-fastest-breaking-exascale-barrier

@MISC{ComputersDoingComputerStuff:2022,
	title = {{Stop Calling Everything A.I. It's Just Computers Doing Computer Stuff}},
	journal = {INC},
	year = {2022},
	howpublished = {\url{https://www.inc.com/jason-aten/stop-calling-everything-ai-its-just-computers-doing-\ computer-stuff.html}},
		note = {Accessed: 2023-09-10}
} 


@article{ExplainableAI:2021,
	title = {Notions of explainability and evaluation approaches for explainable artificial intelligence},
	journal = {Information Fusion},
	volume = {76},
	pages = {89-106},
	year = {2021},
	issn = {1566-2535},
	doi = {https://doi.org/10.1016/j.inffus.2021.05.009},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521001093},
	author = {Giulia Vilone and Luca Longo},
	keywords = {Explainable artificial intelligence, Notions of explainability, Evaluation methods},
	abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models that lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested, coupled with several studies attempting to define the concept of explainability and its evaluation. This systematic review contributes to the body of knowledge by clustering all the scientific studies via a hierarchical system that classifies theories and notions related to the concept of explainability and the evaluation approaches for XAI methods. The structure of this hierarchy builds on top of an exhaustive analysis of existing taxonomies and peer-reviewed scientific material. Findings suggest that scholars have identified numerous notions and requirements that an explanation should meet in order to be easily understandable by end-users and to provide actionable information that can inform decision making. They have also suggested various approaches to assess to what degree machine-generated explanations meet these demands. Overall, these approaches can be clustered into human-centred evaluations and evaluations with more objective metrics. However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps and limitations, and it defines future research directions with explainability as the starting component of any artificial intelligent system.}
}

@MISC{FizikaiGondolkodas:2005,
	author = {Szabó, Gábor},
	title = {{Milyen messzire esett Newton almája?}},
	year = {2005},
	howpublished = {\url{http://real-eod.mtak.hu/977/1/01%20Szab%C3%B3%20007-020.pdf}},
					note = {Accessed: 2023-09-10}
	
} 

@MISC{AISurpassHumanIntelligence:2018,
	author={Fang, J. and Su, H.g and Xiao, Y.},
	title = {Will Artificial Intelligence Surpass Human Intelligence?},
year = {2018},
howpublished = {\url{http://dx.doi.org/10.2139/ssrn.3173876}},
		note = {Accessed: 2023-09-10}

}

@MISC{GenerativeAI:2023,
	title = {There's still a long way to go with generative AI},
	author = {nextplatform.com},
	year = {2023},
	howpublished = {\url{https://www.nextplatform.com/2023/05/05/theres-still-a-long-way-to-go-with-generative-ai/}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{EUdefinicio:2018,
	title = {A közös európai adattér kialakítása felé},
	year = {2018},
	howpublished = {\url{https://eur-lex.europa.eu/legal-content/HU/TXT/HTML/?uri=CELEX:52018DC0237}},
		note = {Accessed: 2023-09-10}
	
}


@Article{TuringNeumann:2022,
	AUTHOR = {Copeland, B. Jack and Fan, Zhao},
	TITLE = {Turing and Von Neumann: From Logic to the Computer},
	JOURNAL = {Philosophies},
	VOLUME = {8},
	YEAR = {2023},
	NUMBER = {2},
	ARTICLE-NUMBER = {22},
	URL = {https://www.mdpi.com/2409-9287/8/2/22},
	ISSN = {2409-9287},
	ABSTRACT = {This article provides a detailed analysis of the transfer of a key cluster of ideas from mathematical logic to computing. We demonstrate the impact of certain of Turing&rsquo;s logico-philosophical concepts from the mid-1930s on the emergence of the modern electronic computer&mdash;and so, in consequence, Turing&rsquo;s impact on the direction of modern philosophy, via the computational turn. We explain why both Turing and von Neumann saw the problem of developing the electronic computer as a problem in logic, and we describe their joint journey from logic to electronic computation. While much has been written about Turing&rsquo;s and von Neumann&rsquo;s individual contributions to the development of the computer, this article investigates less well-known terrain: their interactions and mutual influences. Along the way we argue against &lsquo;logic skeptics&rsquo; and &lsquo;Turing skeptics&rsquo;, who claim that neither logic nor Turing played any significant role in the creation of the modern computer.},
	DOI = {10.3390/philosophies8020022}
}

@MISC{EUbanAI:2023,
	title = {A közös európai adattér kialakítása felé},
	year = {2018},
	howpublished = {\url{https://eur-lex.europa.eu/legal-content/HU/TXT/HTML/?uri=CELEX:52018DC0237}},
		note = {Accessed: 2023-09-10}
	
}
%High-risk AI

%MEPs ensured the classification of high-risk applications will now include AI systems that pose significant harm to people’s health,

@MISC{TrustedDeepLearningAI:2023,
	title = {Deep Learning Can’t Be Trusted, Brain Modeling Pioneer Says},
	year = {2023},
	howpublished = {\url{https://spectrum.ieee.org/deep-learning-cant-be-trusted}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{AIIsTheNumberOnePriority:2023,
	title = {{AMD says AI is the number one priority right now}},
	author = {nextplatform.com},
	year = {2023},
	howpublished = {\url{https://www.nextplatform.com/2023/05/03/amd-says-ai-is-the-number-one-priority-right-now/}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{AIPowerConsumptionExploding:2022,
	author = {semiengineering.com},
	title = {{AI Power Consumption Exploding}},
	year = {2022},
	howpublished = {\url{https://semiengineering.com/ai-power-consumption-exploding/}},
		note = {Accessed: 2023-09-10}
}

@misc{AICarbonEstimating:2023,
	title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model}, 
	author={Luccioni, A. S. and Viguier, S. and Ligozat A-N},
	year={2022},
	eprint={2211.02001},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@MISC{TexasBitcoinRegulation:2023,
	title = {{Texas Senate bill seeks to regulate Bitcoin miners and their electricity consumption}},
	year = {2023},
	howpublished = {\url{https://cbsaustin.com/news/local/texas-senate-bill-seeks-to-regulate-bitcoin-miners-and-\ 
	their-electricity-consumption-riot-platforms-ercot-rockdale}},
		note = {Accessed: 2023-09-10}
	
}


@MISC{GlobalChipShortage:2023,
	title = {{Understanding the global chip shortage}},
	year = {2023},
	howpublished = {\url{https://www.bcs.org/articles-opinion-and-research/understanding\ -the-global-chip-shortage/}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{DisruptiveIOworkload:2019,
	author = {nextplatform.com},
	title = {{CRAY revamps clusterstor for the exascale era}},
	year = {2019},
	howpublished = {\url{https://www.nextplatform.com/2019/10/30/cray-revamps-clusterstor-for-the-exascale-era/}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{HypeCycleAI:2023,
	title = {{Hype Cycle for AI technologies in Business}},
	year = {2023},
	howpublished = {\url{https://omniscien.com/resources/webinars/}},
		note = {Accessed: 2023-09-10}
	
}

@MISC{CompetitiveAI:2023,
	title = {{Hype Cycle for AI technologies in Business}},
	year = {2023},
	howpublished = {\url{External AI R&D labs are becoming a competitive advantage for innovation}},
		note = {Accessed: 2023-09-10}
}

@MISC{SolvingBigComputingProblems:2023,
	author = {nature.com},
	title = {{Solving the big computing problems in the twenty-first century}},
	year = {2023},
	howpublished = {\url{https://www.nature.com/articles/s41928-023-00985-1.epdf}},
		note = {Accessed: 2023-09-10}
}

@misc{IntelRevenueQ2:2023,
title = {{Intel's data center chip sales slide amid bleak Q2 earnings results}},
howpublished = {\url{https://www.protocol.com/bulletins/intel-earnings-results-data-center}},
note = {Accessed: 2023-09-10}
}

@misc{IntelBoostQ2:2023,
	title = {{Intel sees AI boost in Q2}},
howpublished = {\url{https://www.eenewseurope.com/en/intel-sees-ai-boost-in-q2/}},
note = {Accessed: 2023-09-10}
}

@misc{HorovodAI:2023,
	author ={horovod.ai},
	title = {{Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet}},
	howpublished = {\url{https://horovod.ai/}},
	note = {Accessed: 2023-09-10}
}

@misc{IBMDropsQ2:2023,
	title = {{IBM mainframe revenues drop 30\% in Q2 2023}},
	year={2023},
	howpublished={\url{https://www.datacenterdynamics.com/en/news/ibm-mainframe-revenues-drop-30-in-q2-2023/}},
}

@article{MemristorArrayPerception:2023,
	doi = {doi:10.1126/sciadv.adi4083},
	author = {Xuan Pan  and Jingwen Shi  and Pengfei Wang  and Shuang Wang  and Chen Pan  and Wentao Yu  and Bin Cheng  and Shi-Jun Liang  and Feng Miao },
	title = {Parallel perception of visual motion using light-tunable memory matrix},
	journal = {Science Advances},
	volume = {9},
	number = {39},
	pages = {eadi4083},
	year = {2023},
	doi = {10.1126/sciadv.adi4083},
	URL = {https://www.science.org/doi/abs/10.1126/sciadv.adi4083},
	eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adi4083},
	abstract = {Parallel perception of visual motion is of crucial significance to the development of an intelligent machine vision system. However, implementing in-sensor parallel visual motion perception using conventional complementary metal-oxide semiconductor technology is challenging, because the temporal and spatial information embedded in motion cannot be simultaneously encoded and perceived at the sensory level. Here, we demonstrate the parallel perception of diverse motion modes at the sensor level by exploiting light-tunable memory matrix in a van der Waals (vdW) heterostructure array. The optoelectronic characteristics of gate-tunable photoconductivity and light-tunable memory matrix enable devices in the array to realize simultaneous encoding and processing of incoming spatiotemporal light pattern. Furthermore, we implement a visual motion perceptron with the array capable of deciphering multiple motion parameters in parallel, including direction, velocity, acceleration, and angular velocity. Our work opens up a promising venue for the realization of an intelligent machine vision system based on in-sensor motion perception. The concept of in-sensor motion perceptron is proposed based on a gate-tunable van der Waals heterostructure array.}}
