
%https://creativecommons.org/licenses/by-nc/3.0/us/
%https://creativecommons.org/licenses/by-nc/2.5/hu/
%https://creativecommons.org/licenses/by-nc/3.0/us/deed.hu
 

@article{EinsteinSpecial:1905,
	author = {Albert Einstein},
	title = {{On the Electrodynamics of Moving Bodies}},
	journal = {Annalen der Physik (in German)},
	volume = {10},
	number = {17},
	year = {1905},
	pages = {891–921},
		doi = {10.1002/andp.19053221004}
	
}
 
@article{Minkowski:1908,
	author = {{Hermann Minkowski}},
	title = {{Die Grundgleichungen f\"ur die electromagnetischen Vorg\"ange in bewegten K\"orpern}},
	journal = {Nachrichten von der K\"oniglichen Gesellschaft der Wissenschaften zu G\"ottingen (in German)},
	year = {1908},
	pages = {53–111},
}
 
@techreport{EDVACEckertMauchly,
author = {J. P. Eckert, Jr. and J. W. Mauchly},
title = {{Automatic High-Speed Computing: A Progress Report on the EDVAC}},
institution = {Moore School Library, University of Pennsylvania, Philadephia},
year = {1945},
month = {September},
number = {Report of Work under Contract No. W-670-ORD-4926, Supplement No 4},
howpublished = {\url{ https://archive.org/details/eckert-mauchly-prog-rep-edvac-moore-sch-1945}}
}


@MISC{EDVACreport1945,
author = {John~von~Neumann},
title = {{First Draft of a Report on the EDVAC}},
year = {1945},
lab = {Moore School of Electrical Engineering
University of Pennsylvania},
howpublished = {
\url{https://archive.org/details/vnedvac/}
}
}

%\url{https://web.archive.org/web/20130314123032/ http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf}


@CONFERENCE{AmdahlSingleProcessor67,
	author= {Amdahl, G. M.},
	year = {1967},
	title = {{Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities}},
	booktitle = {AFIPS Conference Proceedings},
	volume = {30},
	pages = {483-485},
	doi = {10.1145/1465482.1465560}
}

@article{DijkstraSemaphore1972,
author = {E.W. Dijkstra},
title = {{Information Streams Sharing a Finite Buffer}},
journal = {Information Processing Letters},
volume = {1},
year = {1972},
pages = {179-180}
}

@article{MoreIsDifferent1972,
	author = {P. W. Anderson},
	title = {{More Is Different}},
	journal = {Science},
	volume = {177},
	year = {1972},
	pages = {393-396},
		doi = {10.1126/science. 177.4047.393}	
}


@article{BackusNeumannProgrammingStyle,
author = {J. Backus},
title = {{Can Programming Languages Be liberated from the von Neumann Style? A Functional Style and its Algebra of Programs}},
journal = {Communications of the ACM},
volume = {21},
year = {1978},
pages = {613–-641}
}

@inproceedings{GodfreyGeographic:1982,
	author = {Godfrey, D. Michael},
	title = {Cartographic Computing Techology},
	booktitle = {Euro-Carto 1. Oxford, },
	year = {1981},
	location = {Oxford, UK},
	numpages = {8},
	howpublished = {\url{http://www,researchgate.net/profile/Micheal_Godfrey4/publicaltions}}

} 

@article{CacheMemories:1982,
 author = {Smith, Alan Jay},
 title = {Cache Memories},
 journal = {ACM Comput. Surv.},
 issue_date = {Sept. 1982},
 volume = {14},
 number = {3},
 month = sep,
 year = {1982},
 issn = {0360-0300},
 pages = {473--530},
 numpages = {58},
 url = {http://doi.acm.org/10.1145/356887.356892},
 doi = {10.1145/356887.356892},
 acmid = {356892},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{UsingCacheMemory:1983,
 author = {Goodman, James R.},
 title = {Using Cache Memory to Reduce Processor-memory Traffic},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 1983},
 volume = {11},
 number = {3},
 month = jun,
 year = {1983},
 issn = {0163-5964},
 pages = {124--131},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1067651.801647},
 acmid = {801647},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{NicolauFischer1984,
author = {Alexandru Nicolau and Joseph A. Fisher},
title = {{Measuring the parallelism available for very long instruction word architectures}},
journal = {IEEE Transactions on Computers},
volume = {C-33},
year = {1984},
number = {11},
pages = {968-976}
}

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{GodfreyArchitecture1986,
	author = {M. D. Godfrey},
	title = {{Innovation in Computational Architecture and Design}},
	journal = {ICL Technical Journal},
	volume = {5},
	year = {1986},
	pages ={18--31},
}

%	isbn = {0-387-18923-8},
%	url = {http://dl.acm.org/citation.cfm?id=52797.52802},
@inproceedings{IannucciIssues:1988,
	author = {Arvind and Iannucci, Robert A.},
	title = {Two Fundamental Issues in Multiprocessing},
	booktitle = {4th International DFVLR Seminar on Foundations of Engineering Sciences on Parallel Computing in Science and Engineering},
	year = {1988},
	location = {Bonn, Germany},
	pages = {61--88},
	numpages = {28},
	acmid = {52802},
	publisher = {Springer-Verlag New York, Inc.},
	address = {New York, NY, USA},
} 

@article{Gustafson:1988,
	author = {Gustafson, John L.},
	title = {{Reevaluating Amdahl's Law}},
	journal = {Commun. ACM},
	issue_date = {May 1988},
	volume = {31},
	number = {5},
	month = may,
	year = {1988},
	pages = {532--533},
	numpages = {2},
	doi = {10.1145/42411.42415},
	acmid = {42415},
	publisher = {ACM},
	address = {New York, NY, USA},
} 



@book{RISCarchitecture:1989,
	editor = {Furber, Stephen Bo},
	series = {Advances in Computers, Volume 96},
	title = {{VLSI RISC Architecture and Organization}},
	year = {1989},
	publisher = {CRC Press},
}

@inbook{vonNeumannOrigins,
	author = {Aspray, W.},
	editor = {Bernard Cohen and William Aspray},
	title = {{John von Neumann and the Origins of Modern Computing}}, 
	booktitle = {{John von Neumann and the Origins of Modern Computing}}, 
	publisher = { MIT Press, Cambridge},
	pages = {34--48},
	year = {1990},
}

%  isbn = {0-89391-369-3},
@incollection{NeumannPreliminary:1989,
 author = {Burks, Arthur W. and Goldstine, Herman H. and von Neumann, John},
 chapter = {Preliminary Discussion of the Logical Design of an Electronic Computing Instrument (1946)},
 title = {Perspectives on the Computer Revolution},
 editor = {Pylyshyn, Zenon W. and Bannon, Liam J.},
 year = {1989},
 pages = {39--48},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=98326.98337},
 acmid = {98337},
 publisher = {Ablex Publishing Corp.},
 address = {Norwood, NJ, USA},
} 

% url = {http://doi.acm.org/10.1145/78607.78614},
% issn = {0001-0782},

@article{Karp:parallelperformance1990,
 author = {Karp, Alan H. and Flatt, Horace P.},
 title = {{Measuring Parallel Processor Performance}},
 journal = {Commun. ACM},
 issue_date = {May 1990},
 volume = {33},
 number = {5},
 month = may,
 year = {1990},
 pages = {539--543},
 numpages = {5},
 doi = {10.1145/78607.78614},
 acmid = {78614},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallel performance},
} 

  
@article{PriorityInheritance:1990,
	author = {{L. Sha and R. Rajkumar and J.P. Lehoczky}},
	title = {{Priority inheritance protocols: an approach to real-time synchronization}},
	journal = { IEEE Transactions on Computers },
	volume = {39},
	number = {9},
	month = sep,
	year = {1990},
	pages = {1175--1185},
	numpages = {1},
	doi = { 10.1109/12.57058},
	publisher = {IEEE},
} 

@CONFERENCE{Ousterhout90,
author = {John K. Ousterhout},
title = {Why Aren't Operating Systems Getting Faster As Fast As Hardware?},
booktitle = {USENIX Summer Conference},
year = {1990},
url = {http://www.stanford.edu/~ouster/cgi-bin/papers/osfaster.pdf}
}

@article{Sun:BetterPerformanceMetric1991,
 author = {Sun, Xian-He and Gustafson, John L.},
 title = {Paper: Toward a Better Parallel Performance Metric},
 journal = {Parallel Comput.},
 issue_date = {December, 1991},
 volume = {17},
 number = {10-11},
 month = dec,
 year = {1991},
 issn = {0167-8191},
 pages = {1093--1109},
 numpages = {17},
 url = {http://dx.doi.org/10.1016/S0167-8191(05)80028-6},
 doi = {10.1016/S0167-8191(05)80028-6},
 acmid = {1746189},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Parallel processing, parallel speedup, performance measurement, scaled speedup, sizeup}
} 

@techreport{DongarraPerformance:1992,
author = {Jack Dongarra},
title = {{Performance of Various Computers Using Standard Linear Equations Software}},
institution = {Computer Science Dept., Univ. of Tennessee, Knoxville},
year = {1992},
number = {TN 37996-1301},
}


@inproceedings{Mahlke:1992:LimitedRegisters,
author = {Mahlke, S.A. and Chen, W.Y. and Chang, P.P. and Hwu, W.-M.W.},
title = {Scalar program performance on multiple-instruction-issue processors with a limited number of registers},
booktitle = {Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences},
date = {7-10 Jan 1992},
year = {1992},
volume = {1},
pages = {34 - 44},
doi = {10.1109/HICSS.1992.183141},
}


@article{FateofEDVAC1993,
 author = {Williams, Michael R.},
 title = {{The Origins, Uses, and Fate of the EDVAC}},
 journal = {IEEE Ann. Hist. Comput.},
 issue_date = {January 1993},
 volume = {15},
 number = {1},
 month = jan,
 year = {1993},
 issn = {1058-6180},
 pages = {22--38},
 numpages = {17},
 url = {http://dx.doi.org/10.1109/85.194089},
 doi = {10.1109/85.194089},
 acmid = {612518},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
} 

@book{ModernRelativity:1993,
	author = { Anadijiban Das},
	title = {{The special theory of relativity: a mathematical exposition}},
	publisher = {Springer},
	isbn = {978–0–387–94042–7},
	edition = {1},
	year = {1993}
}

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{GodfreyIEEE1993,
	author = {M. D. Godfrey and D. F. Hendry},
	title = {{The Computer as von Neumann Planned It}},
	journal = {IEEE Annals of the History of Computing},
	volume = {15},
	year = {1993},
	pages ={11-21},
	number = {1},	
	howpublished = {\url{https://archive.org/details/vonneumann_computer/}}
}

@article{PriorityInversion:1993,
	abstract = "A priority inversion occurs when a low-priority task causes the execution of a higher-priority task to be delayed. The possibility of priority inversions complicates the analysis of systems that use priority-based schedulers because priority inversions invalidate the assumption that a task can be delayed by only higher-priority tasks. This paper formalizes priority inversion and gives sufficient conditions as well as some new protocols for preventing priority inversions.",
	author = "Babaoglu, Ozalp and Marzullo, Keith and Schneider, Fred B.",
	day = "01",
	doi = "10.1007/BF01088832",
	issn = "1573-1383",
	journal = "Real-Time Systems",
	month = "Oct",
	number = "4",
	pages = "285–303",
	title = "A formalization of priority inversion",
	url = "https://doi.org/10.1007/BF01088832",
	volume = "5",
	year = "1993"
}

@MISC{WallLimitsOfILP:1993,
 author = {Wall, David W.},
 title = {Limits of Instruction-level Parallelism},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {Apr. 1991},
 volume = {25},
 number = {Special Issue},
 month = apr,
 year = {1991},
 issn = {0163-5980},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106991},
 doi = {10.1145/106974.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
%howpublished = {\url{http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-93-6.pdf}}

% issn = {0018-9162},
%url = {http://dx.doi.org/10.1109/MC.1993.274941},
@article{ScalingParallel:1993,
 author = {Singh, Jaswinder Pal and Hennessy, John L. and Gupta, Anoop},
 title = {Scaling Parallel Programs for Multiprocessors: Methodology and Examples},
 journal = {Computer},
 issue_date = {July 1993},
 volume = {26},
 number = {7},
 month = jul,
 year = {1993},
 pages = {42--50},
 numpages = {9},
 doi = {10.1109/MC.1993.274941},
 acmid = {619634},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 


@MISC{CharmIntroduction93,
	author = {L. V. Kale and S. Krisnan},
	title = {{CHARM++: A Portable Concurrent Object Oriented System Based on C++}},
	year = {1993},
	howpublished = {\url{http://http://charm.cs.illinois.edu/newPapers/93-02/paper.pdf}}
}


@INPROCEEDINGS{CellularNeuralNetworks93,
	author = {Valerio Cimagalli and Marco Balsi},
	title = {Cellular Neural Networks: A Review},
	booktitle = {Proc. 6th Italian Workshop on Parallel Architectures and Neural Networks, Vietri sul Mare, Italy},
	year = {1993},
	pages = {12--14},
	note = {ISBN: 9789814534604},
	publisher = {World Scientific}
}

@CONFERENCE{ThekkathException94,
author = {C. A. Thekkath and H. M. Levy},
title = {Hardware and software support for efficient exception handling},
booktitle = {ASPLOS-VI Proceedings},
year = {1994},
note = {DOI: 10.1145/195470.195515},
url = {http://www.thekkath.org/Documents/traps.pdf}
}


@article{Wulf:MemoryWall:1995,
	author = {Wm. A. Wulf and 	Sally A. McKee},
	title = {{Hitting the memory wall: implications of the obvious}},
	journal = {ACM SIGARCH Computer Architecture News},
	volume = {23},
	year = {1995},
	pages ={20-24},
	number = {1},
	doi = {10.1145/216585.216588}
}



@MISC{AmdalVsGustafson96,
	author = {Shi,Yuan},
	title = {{Reevaluating Amdahl's Law and Gustafson's Law}},
	year = {1996},
	howpublished = {\url{https://www.researchgate.net/publication/ 228367369\_Reevaluating\_Amdahl's\_law\_and \_Gustafson's\_law}}
}



@book{CPPBoook:1997,
	author = {Stroustrup, Bjarne},
	title = {{The C++ Programming Language}},
	year = {1997},
	publisher = {Addison-Wesley},
}



@MISC{SpawnJoinArchitectureVishkin:1998,
	author = {Uzi Y. Vishkin},
	title = {{Spawn-join instruction set architecture for providing explicit multithreading }},
	year = {1998},
	howpublished = {\url{https://patents.google.com/patent/US6463527B1/en}}
}


@inproceedings{Pollack:ThermalWall:1999,
 author = {Pollack, Fred J.},
 title = {New Microarchitecture Challenges in the Coming Generations of CMOS Process Technologies (Keynote Address)(Abstract Only)},
 booktitle = {Proceedings of the 32Nd Annual ACM/IEEE International Symposium on Microarchitecture},
 series = {MICRO 32},
 year = {1999},
 isbn = {0-7695-0437-X},
 location = {Haifa, Israel},
 pages = {2--},
 url = {http://cs.nyu.edu/courses/spring12/CSCI-GA.3033-012/lecture12.pdf},
 acmid = {320082},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

 
@inproceedings{ClockVsIPC2000,
	author = {Agarwal, Vikas and Hrishikesh, M. S. and Keckler, Stephen W. and Burger, Doug},
	title = {{Clock Rate Versus IPC: The End of the Road for Conventional Microarchitectures}},
	booktitle = {Proceedings of the 27th Annual International Symposium on Computer Architecture},
	series = {ISCA '00},
	year = {2000},
	isbn = {1-58113-232-8},
	location = {Vancouver, British Columbia, Canada},
	pages = {248--259},
	numpages = {12},
	url = {http://doi.acm.org/10.1145/339647.339691},
howpublished = {\url{www.cs.utexas.edu/~skeckler/pubs/isca00.pdf}},
	doi = {10.1145/339647.339691},
	acmid = {339691},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@phdthesis{borph2000,
	author = {Hayden Kwok-Hay So},
	title = {{BORPH: An Operating System for FPGA-Based Reconfigurable Computers}},
	school = {University of California, Berkeley},
	year = {2000},
}

@article{EPIC:2000,
	author = {Schlansker, M.S. and Rau, B.R.},
	title = {{EPIC: Explicitly Parallel Instruction Computing}},
	journal = {Computer},
	issue_date = {February 2000},
	volume = {33},
	number = {2},
	month = {Feb},
	year = {2000},
	pages = {37--45},
	publisher = {IEEE},
} 
%	doi = {10.1109/2.820037},

@article{UsesAbusesAmdahl:2001,
 author = {Krishnaprasad, S.},
 title = {{Uses and Abuses of Amdahl's Law}},
 journal = {J. Comput. Sci. Coll.},
 issue_date = {December 2001},
 volume = {17},
 number = {2},
 month = dec,
 year = {2001},
 issn = {1937-4771},
 pages = {288--293},
 numpages = {6},
 publisher = {Consortium for Computing Sciences in Colleges},
 address = {USA},
} 
% url = {http://dl.acm.org/citation.cfm?id=775339.775386},
% acmid = {775386},

%     PHDTHESIS{citation_key,
%                required_fields [, optional_fields] }
%Required fields: author, title, school, year
%
%Optional fields: address, month, note, key
@PHDTHESIS{Akesson01,
author = {Johan F. Akesson},
title = {{Interprocess Communication Utilising Special Purpose Hardware}},
school = {Uppsala University},
year = {2001},
url = {http://www.it.uu.se/research/publications/lic/2001-016/2001-016.pdf}
}

@article{ComptonReconfigurableComputing:2002,
 author = {Compton, Katherine and Hauck, Scott},
 title = {Reconfigurable Computing: A Survey of Systems and Software},
 journal = {ACM Comput. Surv.},
 issue_date = {June 2002},
 volume = {34},
 number = {2},
 month = jun,
 year = {2002},
 issn = {0360-0300},
 pages = {171--210},
 numpages = {40},
 url = {http://doi.acm.org/10.1145/508352.508353},
 doi = {10.1145/508352.508353},
 acmid = {508353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Automatic design, FPGA, field-programmable, manual design, reconfigurable architectures, reconfigurable computing, reconfigurable systems},
} 

@inproceedings{ScratchpadMemory:2002,
 author = {Banakar, Rajeshwari and Steinke, Stefan and Lee, Bo-Sik and Balakrishnan, M. and Marwedel, Peter},
 title = {Scratchpad Memory: Design Alternative for Cache On-chip Memory in Embedded Systems},
 booktitle = {Proceedings of the Tenth International Symposium on Hardware/Software Codesign},
 series = {CODES '02},
 year = {2002},
 isbn = {1-58113-542-4},
 location = {Estes Park, Colorado},
 pages = {73--78},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/774789.774805},
 doi = {10.1145/774789.774805},
 acmid = {774805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@ARTICLE{Vishkin:FineGrainedProgramming,
	AUTHOR = "Dorit Naishlos and Joseph Nuzman and Chau-Wen Tseng and Uzi Vishkin",
	TITLE = "{Towards a First Vertical Prototyping of an Extremely Fine-Grained Parallel Programming Approach}",
	JOURNAL = "Theory of Computing Systems",
	VOLUME = {36},
	NUMBER = {5},
	PAGES = {521-552},
	MONTH = "September",
	DOI = {10.1007/s00224-003-1086-6},
	YEAR = {2003}	}


@online{IntelHyperTechnol:2003,
author = {Intel},
title = {{Intel®	Hyper-Threading Technology}},
year = {2003},
howpublished = {\url{https://www.utdallas.edu/~edsha/parallel/2010S/Intel-HyperThreads.pdf}}
}


@INPROCEEDINGS{Zhao2003,
	author = {Jianjun Zhao},
	title = {Data-flow-based unit testing of aspect-oriented programs},
	booktitle = {Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC 2003)},
	year = {2003},
	pages = {188--197},
	publisher = {IEEE Computer Society},
	doi = {http://ieeexplore.ieee.org/iel5/8813/27898/01245340.pdf?isnumber=27898\&prod=STD\&arnumber=1245340\&arnumber=1245340\&arSt=+188\&ared=+197\&arAuthor=Jianjun+Zhao},
	location = {Dallas, USA},
}


@ARTICLE{DynamicSleep:2003,
	AUTHOR = {J. W. Tschanz and S.G. Narendra and Y. Ye and B.A. Bloechel and S. Borkar and V. De},
	TITLE = "{Dynamic sleep transistor and body bias
	for active leakage power control of microprocessors}",
	JOURNAL = "IEEE Journal of Solid State Circuits",
	VOLUME = {38},
	NUMBER = {11},
	PAGES = {1838 - 1845},
	MONTH = "November",
	YEAR = {2003}	}


@book{LiljaComputerPerformance:2004,
	author = {David J. Lilja},
	title = {{Measuring Computer Performance: A practitioner's guide}},
	year = {2004},
	publisher = {Cambridge University Press},
	ISBN = {0-521-64105-5}
}


@online{ARMdualport2004,
author = {ARM},
title = {{Dual port DMA-capable RAM example}},
year = {2004},
howpublished = {\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0213e/Cihbchdd.html}}
}


@ARTICLE{ThreeStateUnidirectional:2004,
	AUTHOR = {B. Linder and J. Garcia-Ojalvo and A. Neiman and L. Schimansky-Geier},
	TITLE = "{Effects of noise in excitable systems}",
	JOURNAL = "Physics reports",
	VOLUME = {392},
	NUMBER = {6},
	PAGES = {321-424},
	YEAR = {2004}
}

@article{MarkovianIonChannel:2005,
	title = {Non-Markovian stochastic resonance: Three-state model of ion channel gating},
	author = {Goychuk, Igor and H\"anggi, Peter and Vega, Jose L. and Miret-Art\'es, Salvador},
	journal = {Phys. Rev. E},
	volume = {71},
	issue = {6},
	pages = {061906},
	numpages = {11},
	year = {2005},
	month = {Jun},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevE.71.061906},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.71.061906}
}
 
@online{DeBenedictis_zettaflops:2005,
author =  {{ERIK P. DeBenedictis}},
title = {{Petaflops, Exaflops, and Zettaflops for Science and Defense}},
year = {2005},
howpublished = {\url{http://debenedictis.org/erik/SAND-2005/SAND2005-2690-CUG2005-B.pdf}}
}



@ONLINE{Curriculum2005,
	author = {{ACM}},
	title = {{Computing Curricula 2005 - The Overview Report}},
	year = {2006},
	url ={http://www.acm.org/education/education/curric_vols/CC2005-March06Final.pdf}
}

@ARTICLE{SynergisticCell:2006,
	AUTHOR = "{M. Kistler and M. Perrone and F. Petrini}",
	TITLE = "{Cell multiprocessor communication network: Built for speed}",
	JOURNAL = "IEEE Micro",
	VOLUME = {26},
	NUMBER = {3},
	PAGES = {10-23},
	MONTH = "May",
	YEAR = {2006}	}


@book{PerformanceEvaluation2006,
	editor = {Lizy Kurian John and Lieven Eeckhout},
	title = {{Performance Evaluation and Benchmarking}},
	year = {2006},
	isbn = {0849336228, 9780849336225},
	publisher = {CRC, Taylor and Francis},
}


@book{QTProgramming,
author={Blanchette, Jasmin and Summerfield, Mark},
year = {2006},
title = {{A Brief History of Qt. C++ GUI Programming with Qt 4}},
publisher = {Prentice-Hall}
}


@book{HennessyArchitecture2007,
	author = {John L. Hennessy and  David A. Patterson},
	title = {{Computer Architecture: A Quantitative Approach}}, 
	publisher = {Morgan Kaufmann Publishers},
	isbn = {978-0-12-370490-0},
	year = {2007}
}


@article{ParallelPanic:2006,
	author = {O'Hanlon, C},
	title = {{A conversation with John Hennessy and David Patterson.}},
	journal = {Queue},
	volume = {10},
	number = {4},
	month = jan,
	year = {2006},
	pages = {14--22},
} 

@article{HWcontrolledthreadsMahesri:2007,
	author = {Mahesri, Aqeel and Wang, Nicholas J. and Patel, Sanjay J.},
	title = {{Hardware Support for Software Controlled Multithreading}},
	journal = {SIGARCH Comput. Archit. News},
	issue_date = {March 2007},
	volume = {35},
	number = {1},
	month = mar,
	year = {2007},
	issn = {0163-5964},
	pages = {3--12},
	numpages = {10},
	doi = {10.1145/1241601.1241606},
	acmid = {1241606},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@CONFERENCE{ReinventingComputing2007,
	author = {B. Smith},
	title = {Reinventing computing},
	booktitle = {International Supercomputing Conference},
	date = { June 20–21, 2007},
	year = {2007},
	url = {http://www.cct.lsu.edu/~estrabd/LACSI2006/Smith.pdf}
}

@article{TooManyCores2007,
author = {Avi Mendelson},
title= {{How many cores are too many cores? }},
publisher = {IBM Research},
	year = {2007},
howpublished = {Last accessed July 7, 2017 [Online].
	\url{https://www.research.ibm.com/haifa/Workshops/compiler2007/present/ avi\_mendelson.pdf}
	}
}

@MISC{VishkinHome2007,
	author = {{Uzi Vishkin}},
	title = {{Explicit Multi-Threading (XMT): A PRAM-On-Chip Vision --
	A Desktop Supercomputer}},
	year = {2007},
	howpublished = {Last accessed Dec. 12, 2015 [Online].
	\url{http://www.umiacs.umd.edu/users/vishkin/XMT/index.shtml}}
}


@inbook{Hartenstein07,
author = {Reiner Hartenstein},
editor = {K.L.M. Bertels and S.D. Cotofana and G.N. Gaydadjiev and K.G.W. Goossens and S. Hamdioui and B.H.H. Juurlink and A.J. van Genderen and S. Wong},
title = {The Future of Computing, essays in memory of Stamatis Vassiliadis}, 
publisher = {Computer Engineering Laboratory, TU Delft},
year = {2007}, 
url = {http://hartenstein.de/4SlashdotDec07.pdf}
}

@inproceedings{Borkar:ThousandCore:2007,
author = {Shekhar Borkar},
title = {{Thousand Core Chips—A Technology Perspective}},
 booktitle = {{Proceedings of the ACM/IEEE 44th Design Automation Conf.(DAC)}},
series = {{DAC'07}},
year = {2007},
publisher = {ACM Press},
pages = {746-749},
}


@Article{AmdalsLaw-Paul2007,
author="Paul, JoAnn M.
and Meyer, Brett H.",
title="{Amdahl's Law Revisited for Single Chip Systems}",
journal="International Journal of Parallel Programming",
year="2007",
month="Apr",
day="01",
volume="35",
number="2",
pages="101--123",
abstract="Amdahl's Law is based upon two assumptions -- that of boundlessness and homogeneity -- and so it can fail when applied to single chip heterogeneous multiprocessor designs, and even microarchitecture. We show that a performance increase in one part of the system can negatively impact the overall performance of the system, in direct contradiction to the way Amdahl's Law is instructed. Fundamental assumptions that are consistent with Amdahl's Law are a heavily ingrained part of our computing design culture, for research as well as design. This paper points in a new direction. We motivate that emphasis should be made on holistic, system level views instead of divide and conquer approaches. This, in turn, has relevance to the potential impacts of custom processors, system-level scheduling strategies and the way systems are partitioned. We realize that Amdahl's Law is one of the few, fundamental laws of computing. However, its very power is in its simplicity, and if that simplicity is carried over to future systems, we believe that it will impede the potential of future computing systems.",
}
% issn="1573-7640",
% doi="10.1007/s10766-006-0028-8",
% url="https://doi.org/10.1007/s10766-006-0028-8"


@inproceedings{Tsafrir:2007,
 author = {Tsafrir, Dan},
 title = {The Context-switch Overhead Inflicted by Hardware Interrupts (and the Enigma of Do-nothing Loops)},
 booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science},
 series = {ExpCS '07},
 year = {2007},
 isbn = {978-1-59593-751-3},
 location = {San Diego, California},
 articleno = {4},
 pages = {3--3},
 acmid = {1281704},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock interrupts, operating system noise, ticks},
} 
% url = {http://doi.acm.org/10.1145/1281700.1281704},
% doi = {10.1145/1281700.1281704},

@inproceedings{armContextSwitching:2007,
	author = {David, Francis M. and Carlyle, Jeffrey C. and Campbell, Roy H.},
	title = {{Context Switch Overheads for Linux on ARM Platforms}},
	booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science},
	series = {ExpCS '07},
	year = {2007},
	isbn = {978-1-59593-751-3},
	location = {San Diego, California},
	articleno = {3},
	url = {http://doi.acm.org/10.1145/1281700.1281703},
	doi = {10.1145/1281700.1281703},
	acmid = {1281703},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {context switch overhead, operating system},
} 



@article{EnergyProportional2007,
author = {Luiz André Barroso and Urs Hölzle},
title = {{The Case for Energy-Proportional Computing}},
journal = {Computer},
volume = {40},
year = {2007},
pages = {33--37}
}


@article{Congy:CoreSpilling:2007,
	author = {J. Congy and et al},
	title = {{Accelerating Sequential Applications on CMPs Using Core Spilling}},
	journal = {Parallel and Distributed Systems},
	volume = {18},
	year = {2007},
	pages = {1094--1107},
} 


@CONFERENCE{ComputingDensity2008,
author = {J. Williams and A. D. George and J. Richardson and K. Gosrani and S. Suresh},
title = {Computational density of fixed and reconfigurable multi-core devices for application acceleration},
booktitle = {Proceedings of Reconfigurable Systems Summer Institute, Urbana, IL, Jul. 2008.},
year = {2008}
}

@INPROCEEDINGS{VishkinFPGA2008,
	author = {	Xingzhi Wen	and	Uzi Vishkin},
	title = "{FPGA-based prototype of a pram-on-chip processor}",
	booktitle = {Proceedings of the 5th conference on Computing frontiers, CF'08},
	year = {2008},
	address = {New York, NY, USA},
	pages = {55-66},
	publisher = {ACM},
	DOI = {10.1145/1366230.1366240},
	ISBN={978-1-60558-077-7}
}


@online{IntelHyper:2008,
author = {Intel},
title = {{Hyper-Threading Technology Architecture and Microarchitecture}},
year = {2008},
howpublished = {\url{http://noggin.intel.com/technology-journal/2002/61/hyper-threading-technology}}
}


@article{HillMulticoreAmdahl2008,
	author = {Hill, M. D.  and Marty, M. R. },
	title = {{Amdahl's Law in the Multicore Era}},
	journal = {IEEE Computer },
	volume = {41},
	year = {2008},
	pages ={33-38},
	number = {7}
}



@inproceedings{Qthreads:2008,
	author = {{Wheeler, K.B.  and Murphy, R.C. and Thain, D.}},
	title = {{Qthreads: An API for programming with millions of lightweight threads}},
	booktitle = {Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on},
	date = {14-18 April 2008},
	year = {2008},
	pages = {1--8},
	doi = {10.1109/IPDPS.2008.4536359},
}


@inproceedings{ChandyParallelism:2009,
author = {Chandy, John A. and Singaraju, Janardhan},
title = {Hardware Parallelism vs. Software Parallelism},
booktitle = {Proceedings of the First USENIX Conference on Hot Topics in Parallelism},
series = {HotPar'09},
year = {2009},
location = {Berkeley, California},
pages = {2--2},
numpages = {1},
acmid = {1855593},
publisher = {USENIX Association},
address = {Berkeley, CA, USA},
} 

@article{Minkowski100:2008,
	author = {Scott Walter},
	title = {Hermann Minkowski and the scandal of spacetime},
	journal = {ESI News},
	volume = {1},
	number = {3},
	year = {2008},
	pages = {6--8},
	howpublished = {https://halshs.archives-ouvertes.fr/halshs-00319209/document},
} 

@article{WilliamsRoofline:2009,
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
	journal = {Commun. ACM},
	issue_date = {April 2009},
	volume = {52},
	number = {4},
	month = apr,
	year = {2009},
	issn = {0001-0782},
	pages = {65--76},
	publisher = {ACM},
	address = {New York, NY, USA},
} 
%	url = {http://doi.acm.org/10.1145/1498765.1498785},
%doi = {10.1145/1498765.1498785},
%acmid = {1498785},

@inbook{Ferreira09,
title={RTOS Hardware Coprocessor Implementation in VHDL},
url={http://aveiro.academia.edu/documents/0054/9267/OReK\_CoProcessor.pdf},
booktitle={RTOS Hardware Coprocessor Implementation in VHDL},
publisher={Intellectual Property / Embedded Systems Conference (IP/ESC)},
author={Ferreira, Carlos Miguel and Oliveira, Arnaldo S R}, year={2009}, pages={6}
}

@ARTICLE{AsanovicParallelCACM:2009,
	AUTHOR = "Krste Asanovic and Rastislav Bodik and James Demmel and Tony
	Keaveny and Kurt Keutzer and John Kubiatowicz and Nelson Morgan
	and David Patterson and Koushik Sen and John Wawrzynek and
	David Wessel and Katherine Yelick",
	lab = {University of California, Berkeley},
	TITLE = "{A View of the Parallel Computing Landscape}",
	JOURNAL = "Comm. ACM",
	VOLUME = {52},
	NUMBER = {10},
	PAGES = {56-67},
	YEAR = {2009}	}

% isbn = {978-1-60558-406-5},
%url = {http://doi.acm.org/10.1145/1508244.1508246},
%doi = {10.1145/1508244.1508246},
@inproceedings{Gebhart:TRIPS:2009,
 author = {Gebhart, Mark and Maher, Bertrand A. and Coons, Katherine E. and Diamond, Jeff and Gratz, Paul and Marino, Mario and Ranganathan, Nitya and Robatmili, Behnam and Smith, Aaron and Burrill, James and Keckler, Stephen W. and Burger, Doug and McKinley, Kathryn S.},
 title = {An Evaluation of the TRIPS Computer System},
 booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XIV},
 year = {2009},
 location = {Washington, DC, USA},
 pages = {1--12},
 numpages = {12},
 acmid = {1508246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {trips},
} 


@BOOK{ArtOfConcurrencyBook:2009,
	author = {Clay Breshears},
	title = {The Art of Concurrency},
	year = {2009},
	publisher = {O'Reilly Media, Inc.},
	isbn = {9780596802424}
}

@BOOK{EnglanderBook,
	author = {Irv Englander},
	title = {The Architecture of COMPUTER HARDWARE, SYSTEMS SOFTWARE AND NETWORKING
	An Information Technology Approach},
	year = {2010},
	publisher = {John Wiley \& Sons, Inc. },
	edition = {Fourth},
	place = {Lawrence, Kansas, USA},
	isbn = {978-0-470-40028-9}
}

@BOOK{SystemCBook:2010,
	author = {David C. Black and Jack Donovan and Bill Bunton and Anna Keist},
	title = {SystemC: From the Ground Up},
	year = {2010},
	publisher = {Springer},
	edition = {second},
	place = {New York Dordrecht Heidelberg London},
	isbn = {978-0-387-69957-8}
}

@inbook{Hartenstein2010,
author = {Hartenstein, R.},
title = {{The Grand Challenge To Reinvent Computing}}, 
publisher = {XXX Congress of the SBC},
year = {20-23 July, 2010},
place = {Belo Horizonte, MG, Brazil}, 
url = {http://hartenstein.de/4SlashdotDec07.pdf}
}


%% http://www.cslab.pepperdine.edu/warford/cosc330/

@article{ParallelEfficiency:Orii:2010,
title = "Metrics for evaluation of parallel efficiency toward highly parallel processing ",
journal = "Parallel Computing ",
volume = "36",
number = "1",
pages = "16 - 25",
year = "2010",
note = "",
issn = "0167-8191",
doi = "http://dx.doi.org/10.1016/j.parco.2009.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167819109001227",
author = "Shigeo Orii",
keywords = "Parallel performance evaluation",
abstract = "I studied novel performance metrics of parallel processing efficiency, determined simply by measuring timing data in a parallel process. The metrics, which consider load-imbalance, are called the parallel efficiency, load-balancing, and parallel impediment metrics. The relationships between these three can be unified into one expression that makes it possible to conduct detailed performance evaluations of parallel processing. The parallel efficiency metric also makes it possible to estimate the acceleration potential of the parallel process. "
}


@BOOK{Warford2010,
	author = {Stanley J. Warford},
	title = {Computer Systems},
	year = {2010},
	publisher = {Jones and Bartlett},
	place = {Sudbury, Massachusetts, USA},
	isbn = { 0-7637-7144-9}
}

%http://www.soos-project.eu/
@MISC{SoOS:2010,
author = {{S(o)OS~project}},
lab={{S(o)OS~project}},
title = {Resource-independent execution support on exa-scale systems},
year = {2010},
howpublished = {\url{http://www.soos-project.eu/index.php/related-initiatives}}
}

@inproceedings{InefficiencyHameed2010,
 author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
 title = {Understanding Sources of Inefficiency in General-purpose Chips},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 series = {ISCA '10},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 location = {Saint-Malo, France},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1815961.1815968},
 doi = {10.1145/1815961.1815968},
 acmid = {1815968},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASIC, chip multiprocessor, customization, energy efficiency, h.264, high performance, tensilica},
} 

@inproceedings{ResourceAwareCompilerVishkin:2010,
 author = {Caragea, George C. and Tzannes, Alexandros and Keceli, Fuat and Barua, Rajeev and Vishkin, Uzi},
 title = {Resource-Aware Compiler Prefetching for Many-Cores},
 booktitle = {Proceedings of the 2010 Ninth International Symposium on Parallel and Distributed Computing},
 series = {ISPDC '10},
 year = {2010},
 isbn = {978-0-7695-4120-4},
 pages = {133--140},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/ISPDC.2010.16},
 doi = {10.1109/ISPDC.2010.16},
 acmid = {1848308},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {parallel architectures, optimizing compilers},
} 

@inproceedings{Lee:GPUvsCPU2010,
 author = {Lee, Victor W. and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
 title = {{Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput Computing on CPU and GPU}},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 series = {ISCA '10},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 location = {Saint-Malo, France},
 pages = {451--460},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1815961.1816021},
 doi = {10.1145/1815961.1816021},
 acmid = {1816021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cpu architecture, gpu architecture, performance analysis, performance measurement, software optimization, throughput computing},
} 


@article{ReconfigurableMulticoresWilliams:2010,
 author = {Williams, Jason and Massie, Chris and George, Alan D. and Richardson, Justin and Gosrani, Kunal and Lam, Herman},
 title = {Characterization of Fixed and Reconfigurable Multi-Core Devices for Application Acceleration},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {November 2010},
 volume = {3},
 number = {4},
 month = {nov},
 year = {2010},
 issn = {1936-7406},
 pages = {19:1--19:29},
 articleno = {19},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/1862648.1862649},
 doi = {10.1145/1862648.1862649},
 acmid = {1862649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Computational density per watt, internal memory bandwidth},
} 

@ARTICLE{Ungerer_MERASA:2010,
	AUTHOR = "T. Ungerer",
	TITLE = "{Multi-core execution of hard real-time applications supporting analyzability}",
	JOURNAL = "IEEE Micro",
	VOLUME = {99},
	PAGES = {66--75},
	YEAR = {2010}
}

@misc{ScienceExascaleRace:2010,
author = {{US DOE}},
title = {{The Opportunities and Challenges of Exascale Computing}},
year = {2010},
howpublished = {\url{https://science.energy.gov/~/media/ascr/ascac/pdf/reports/Exascale\_subcommittee\_report.pdf}}
}


@article{CriticalSectionAmdahlEyerman:2010,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {{Modeling Critical Sections in Amdahl's Law and Its Implications for Multicore Design}},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 2010},
 volume = {38},
 number = {3},
 month = jun,
 year = {2010},
 pages = {362--370},
 numpages = {9},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Amdahl's law, analytical performance modeling, critical sections, synchronization},
} 

% issn = {0163-5964},
% url = {http://doi.acm.org/10.1145/1816038.1816011},
% doi = {10.1145/1816038.1816011},
% acmid = {1816011},

@CONFERENCE{InefficiencyCPUs:2010,
	author= { R. Hameed et al},
	year = {2010},
	title = {{ Understanding Sources of Inefficiency in General-purpose Chips}},
	booktitle = { Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA '10)},
	pages = {37–47},
}


	
@inproceedings{SpiNNakerNeuromimetic:2010,
	author = {Rast, Alexander D. and Jin, Xin and Galluppi, Francesco and Plana, Luis A. and Patterson, Cameron and Furber, Steve},
	title = {Scalable Event-driven Native Parallel Processing: The SpiNNaker Neuromimetic System},
	booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers},
	series = {CF '10},
	year = {2010},
	isbn = {978-1-4503-0044-5},
	location = {Bertinoro, Italy},
	pages = {21--30},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/1787275.1787279},
	doi = {10.1145/1787275.1787279},
	acmid = {1787279},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {asynchronous, event-driven, universal neural processor},
} 


@article{GaluzziInstructionSetExtension:2011,
 author = {Galuzzi, Carlo and Bertels, Koen},
 title = {The Instruction-Set Extension Problem: A Survey},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {May 2011},
 volume = {4},
 number = {2},
 month = may,
 year = {2011},
 issn = {1936-7406},
 pages = {18:1--18:28},
 articleno = {18},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/1968502.1968509},
 doi = {10.1145/1968502.1968509},
 acmid = {1968509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {HW/SW codesign, Instruction-set, customization, instruction generation, instruction selection, instruction-set extension, reconfigurable architecture},
} 


@BOOK{ReconfigurableComputing2011,
	author = {J. M. P. Cardoso and M. H\"ubner},
	title = {Reconfigurable Computing},
	year = {2011},
	publisher = {Springer, Inc. },
	place = {Lawrence, Kansas, USA},
	isbn = {978-1-4614-0060-8},
	doi= {10.1007/978-1-4614-0061-5}
}


@article{YanPingXian11,
author = {Yan Li and Ping-Ping Gu and Xian-Xhan Wang},
title = {{The implementation of Semaphore Management in Hardware Real Time Operating System}},
journal = {Information Technology Journal},
volume = {10},
year = {2011},
pages = {158-163},
number = {1}
}

@article{Borkar:FutureOfUp:2011,
author = {Shekhar Borkar and Andrew A. Chien},
title = {{The Future of Microprocessors}},
journal = {{Communications of the ACM}},
volume = {54},
year = {2011},
pages = {67-77},
number = {5}
}

@CONFERENCE{NASAhyper2011,
	author= {{Saini, S. and Haoqiang Jin and Hood, R. and Barker, D. and	more authors}},
	year = {2011},
	title = {{The impact of hyper-threading on processor resource utilization in production applications}},
	date = {18-21 Dec. 2011},
	booktitle = {High Performance Computing (HiPC), 2011 18th International Conference on},
	pages = {1 -- 10},
	doi = {10.1109/HiPC.2011.6152743}
}



@ONLINE{ARM:big.LITTLE:2011,
	author = {{ARM}},
	title = {{big.LITTLE technology}},
	year = {2011},
	url = {https://developer.arm.com/technologies/big-little}
}


@ARTICLE{Vishkin:AbstractionCACM,
	AUTHOR = "Uzi Vishkin",
	TITLE = "{Using Simple Abstraction to Reinvent Computing for Parallelism}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {54},
	NUMBER = {1},
	PAGES = {75-85},
	MONTH = "January",
	YEAR = {2011}	}

@INPROCEEDINGS{XMTtoolchain2011,
	author = {Keceli, F. and Tzannes, A. and Caragea G.C. and Barua, R. and et al},
	title = "{Toolchain for Programming, Simulating and Studying the XMT Many-Core Architecture}",
	booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on},
	year = {2011},
	publisher = {IEEE},
	DOI = {10.1109/IPDPS.2011.270},
	ISBN={978-1-61284-425-1}
}


@incollection{InvasiveComputing:2011,
	author = {J. Teich and J. Henkel and A. Herkersdorf and D. Schmitt-Landsiedel and W. Schr\"oder-Preikschat and G. Snelting},
	booktitle   = {Multiprocessor System-on-Chip},
	title = {{Invasive Computing: An Overview}},
	editor      = {M. H\"ubner and		J. Becker},
	year = {2011},
	isbn = {978-1-4419-6459-5},
	pages = {241-268},
	publisher   = {Springer},
}
%%howpublished = {\url{http://invasic.informatik.uni-erlangen.de/en/index.php}}

@ARTICLE{ComputingPerformance:2011,
	author = {S.~H.~Fuller and  L.~I.~Millett},
	title = {{Computing Performance: Game Over or Next Level?}},
	journal = {Computer},
	volume = {44},
	Issue = {1},
	pages = {31-38},
	year = {2011}
}

@INPROCEEDINGS{EfficiacyAPU:2011,
author={M. Daga and A. M. Aji and W. c. Feng},
booktitle={2011 Symposium on Application Accelerators in High-Performance Computing},
title={{On the Efficacy of a Fused CPU+GPU Processor (or APU) for Parallel Computing}},
year={2011},
volume={},
number={},
pages={141-149},
abstract={The graphics processing unit (GPU) has made significant strides as an accelerator in parallel computing. However, because the GPU has resided out on PCIe as a discrete device, the performance of GPU applications can be bottlenecked by data transfers between the CPU and GPU over PCIe. Emerging heterogeneous computing architectures that "fuse" the functionality of the CPU and GPU, e.g., AMD Fusion and Intel Knights Ferry, hold the promise of addressing the PCIe bottleneck. In this paper, we empirically characterize and analyze the efficacy of AMD Fusion, an architecture that combines general-purpose x86 cores and programmable accelerator cores on the same silicon die. We characterize its performance via a set of micro-benchmarks (e.g., PCIe data transfer), kernel benchmarks(e.g., reduction), and actual applications (e.g., molecular dynamics). Depending on the benchmark, our results show that Fusion produces a 1.7 to 6.0-fold improvement in the data-transfer time, when compared to a discrete GPU. In turn, this improvement in data-transfer performance can significantly enhance application performance. For example, running a reduction benchmark on AMD Fusion with its mere 80 GPU cores improves performance by 3.5-fold over the discrete AMD Radeon HD 5870 GPU with its 1600 more powerful GPU cores.},
keywords={computer graphic equipment;coprocessors;parallel processing;AMD Fusion;AMD Radeon HD 5870;APU;Intel Knights Ferry;data transfer;fused CPU+GPU Processor;graphics processing unit;parallel computing;programmable accelerator;x86 core acclerator;Acceleration;Benchmark testing;Engines;Graphics processing unit;Instruction sets;Multicore processing;AMD Fusion;APU;GPGPU;GPU;OpenCL;accelerated processing unit;benchmarking;graphics processing unit;heterogeneous computing;performance evaluation},
doi={10.1109/SAAHPC.2011.29},
ISSN={2166-5133},
month={July},}

@book{ComputingPerformanceBook:2011,
	editor = {S.~H.~Fuller and  L.~I.~Millett},
	title = {{The Future of Computing Performance: Game Over or Next Level?}},
	lab = {National Research Council},
	publisher = {National Academies Press, Washington},
	isbn = {978-0-309-15951-7},
	doi= {https://doi.org/10.17226/12980},
	year = {2011}
}

@article{Pingali:2011:TaoOfParallelism,
	author = { Keshav Pingali and  Donald Nguyen and  Milind Kulkarni and Martin Burtscher and M. Amber Hassaan  and  Rashid Kaleem and  Tsung-Hsien Lee and  Andrew Lenharth and  Roman Manevich and Mario M{\'e}ndez-Lojo and Dimitrios Prountzos and  Xin Sui},
	title = {{The Tao of Parallelism in Algorithms}},
	journal = {SIGPLAN Not.},
	issue_date = {June 2011},
	volume = {46},
	number = {6},
	month = jun,
	year = {2011},
	issn = {0362-1340},
	pages = {12--25},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/1993316.1993501},
	doi = {10.1145/1993316.1993501},
	acmid = {1993501},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {amorphous data-parallelism, galois system, irregular programs, operator formulation, tao-analysis},
} 

%	howpublished = {\url{http://invasic.informatik.uni-erlangen.de/en/index.php}}


@Inbook{MemoryWallMcKee:2011,
author="McKee, Sally A.
and Wisniewski, Robert W.",
editor="Padua, David",
title="Memory Wall",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="1110--1116",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_234",
url="http://dx.doi.org/10.1007/978-0-387-09766-4_234"
}


@MISC{GameOverYelick:2011,
	author = {{US National Research Council}},
	lab = {{US National Research Council}},
	title = {{The Future of Computing Performance: Game Over or Next Level?}},
	year = {2011},
	url ={http://science.energy.gov/~/media/ascr/ascac/pdf/meetings/mar11/Yelick.pdf}
}


@phdthesis{HSWscalable2012,
	author = {Daniel Sanchez Martin},
	title = {{HARDWARE AND SOFTWARE TECHNIQUES FOR SCALABLE
	THOUSAND-CORE SYSTEMS}},
	school = {Stanford University, Berkeley},
	year = {2012},	 
	howpublished = {\url{http://purl.stanford.edu/mz572jk7876}}
}



@INPROCEEDINGS{EfficiencyMetric:2012,
	author = { Chung-Hsing Hsu and Jeffery Alan Kuehn  and	Stephen W Poole},
	title = {{Towards efficient supercomputing: searching for the right efficiency metric}},
booktitle={ Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
year={2012},
volume={},
number={},
pages={1157–162},
url = {https://doi.org/10.1145/2188286.2188309}
} 

@ONLINE{NSA_Secret_Report:2012,
	author = {{US National Security Agency}},
	title = {{NSA Outs Top-Secret Report That Missed the Future of Supercomputing}},
	year = {2012},
	url = {https://www.wired.com/2012/11/top-secret-nsa-report/}
}




@article{DarkSilicon2012,
	author = {Esmaeilzadeh, H. and  Blem, E. and St. Amant, R. and Sankaralingam, K. and et al.},
	title = {{Dark Silicon and the End of Multicore Scaling}},
	journal = {IEEE Micro},
	volume = {32},
	year = {2012},
	pages = {122-134},
	number = {3}
}


@ARTICLE{InherentSequentiality:2012,
   author = {F. Ellen and D. Hendler and N. Shavit},
   title = {{On the Inherent Sequentiality of Concurrent Objects}}, 
   doi = {10.1137/08072646X},
   journal = {SIAM J. Comput.},
     year = {2012},
     volume = {43},
     number = {3},
     pages = {519–536}
}


@MISC{XeonHyperthread2013,
	author = {{Intel}},
	title = {{Software-Hardware Interface for Multi-Many-Core}},
	year = {2013},
	howpublished = {\url{https://software.intel.com/en-us/forums/topic/515522}}
}

@CONFERENCE{EmaniRuntimeInfo2013,
author = {Emani, M.K. and Wang, Zheng  and O'Boyle, M.F.P.},
title = {{Smart, adaptive mapping of parallelism in the presence of external workload}},
booktitle = {Code Generation and Optimization (CGO), 2013 IEEE/ACM International Symposium on},
pages = {1--10},
year = {2013},
}

%howpublished = {\url{www.cs.utexas.edu/~skeckler/pubs/isca00.pdf}}


@CONFERENCE{SynchronizationEverything2013,
	author = {David,	T.  and	Guerraoui, R. and	Trigonakis, V.},
	title = {{Everything you always wanted to know about synchronization but were afraid to ask}},
	booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13)},
	year = {2013},
	pages = {33-48},
	doi = {10.1145/2517349.2522714}
}

@ONLINE{RebootingComputing2013,
	author = {{IEEE}},
	title = {{IEEE Rebooting Computing}},
	year = {2013},
	howpublished = {\url{http://rebootingcomputing.ieee.org/}}
}
//http://www.acm.org/education/CS2013-final-report.pdf

@MISC{SHIMM13:Multicore,
	author = {{The Multicore Association.}},
	title = {{Software-Hardware Interface for Multi-Many-Core}},
	year = {2013},
	howpublished = {\url{http://www.multicore-association.org/workgroup/shim.php}}
}

@article{FutureIoT2013,
	author = {Jayavardhana Gubbi and Rajkumar Buyya and Slaven Marusic and Marimuthu Palaniswami },
	title = {{Internet of Things (IoT): A vision, architectural elements, and future directions}},
	journal = {Future Generation Computer Systems},
	volume = {29},
	year = {2013},
	pages = {1645-1660},
	doi = {doi:10.1016/j.future.2013.01.010}
}


@ONLINE{Curriculum2013,
	author = {{ACM}},
	title = {{Computer Science	Curricula 2013 - Curriculum Guidelines for
	Undergraduate Degree Programs	in Computer Science}},
	year = {2014},
	url = {Last accessed Dec. 12, 2015[Online].
	http://www.acm.org/education/CS2013-final-report.pdf}
}

@techreport{Adaptiveresourcecontrolinmulticoresystems2013,
author = {F. Xia and A. Mokhov and A. Yakovlev and A. Iliasov and A. Rafiev and A. Romanovsky},
title = {{Adaptive resource control in multi-core systems}},
institution = {Newcastle University, School of Electrical and Electronic Engineering, Philadephia},
year = {2013},
number = {NCL-EEE-MICRO-TR-2013-183},
howpublished = {\url{http://async.org.uk/tech-reports/NCL-EEE-MICRO-TR-2013-183.pdf}}
}



@INPROCEEDINGS{PerformanceCounter2013,
author={Weaver, V.M. and Terpstra, D. and Moore, S.},
booktitle={Performance Analysis of Systems and Software (ISPASS), 2013 IEEE International Symposium on},
title={Non-determinism and overcount on modern hardware performance counter implementations},
year={2013},
month={April},
pages={215-224},
keywords={computer architecture;multiprocessing systems;performance evaluation;ARM;IA64 system;PMU implementations;POWER system;SPARC system;architectural redesign;counter event;deterministic replay library implementation;deterministic threading library implementation;hardware performance counter implementation;ideal hardware performance counters;nondeterminism;overcount;performance monitoring unit;run-to-run variation;x86_64 CPU implementations;Assembly;Benchmark testing;Hardware;Kernel;Phasor measurement units;Radiation detectors},
doi={10.1109/ISPASS.2013.6557172},}

@inproceedings {NeumannRetire:2013,
	author = {Aleksander Budzynowski and Gernot Heiser},
	title = {The von Neumann Architecture Is Due for Retirement},
	booktitle = {Presented as part of the 14th Workshop on Hot Topics in Operating Systems},
	year = {2013},
	address = {Santa Ana Pueblo, NM},
	url = {https://www.usenix.org/conference/hotos13/von-neumann-architecture-due-retirement},
	publisher = {USENIX},
}


@MISC{InterCoreShared:2013,
	author = {GlobalFoundries Inc},
	title = {{Administering inter-core communication via shared memory}},
	year = {2013},
	howpublished = {\url{https://patents.google.com/patent/US9223505
	}}
}




@ARTICLE{SpiNNaker:2013,
	author = {S. B. Furber and D. R. Lester and L. A. Plana and J. D. Garside and E. Painkras and S. Temple and A. D. Brown},
	journal = {{IEEE Transactions on Computers}},
	title = {{Overview of the SpiNNaker System Architecture}},
	year = {2013},
	volume = {62},
	number = {12},
	pages = {2454-2467},
	keywords={Network architecture;Program processors;Biological system modeling;Computer architecture;Neural networks},
}
%	doi = {10.1109/TC.2012.142},
% url = {doi.ieeecomputersociety.org/10.1109/TC.2012.142},
% ISSN = {0018-9340},
% month={Dec.}


@inproceedings {AsynchronParadigm:2013,
	author = { S. Kumar and et al},
	title = {Acceleration of an Asynchronous Message Driven Programming Paradigm on IBM Blue Gene/Q},
	booktitle = {2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
	year = {2013},
	address = {Boston},
	url = {https://https://ieeexplore.ieee.org/abstract/document/ 6569854},
	publisher = {IEEE},
}


@ARTICLE{CompilerInfrastructure:2014,
author = {W. Sheng and S. Sch\"urmans and M. Odendahl and M. Bertsch and V. Volevach and R. Leupers and G. Ascheid},
title = {{A compiler infrastructure for embedded heterogeneous MPSoCs}},
journal = {Parallel Computing},
volume = {40},
Issue = {2},
pages = {51-68},
year = {2014},
}

@misc{DeBenedictis_supercomputing:2014,
author = {{Machine Intelligence Research Institute}},
title = {{Erik DeBenedictis on supercomputing}},
lab ={SAND Number 2014-2679P},
year = {2014},
url = {https://intelligence.org/2014/04/03/erik-debenedictis/}
}





@misc{Intel10GHz:2014,
author = {Intel},
title = {{Why has CPU frequency ceased to grow?}},
year = {2014},
url = {https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow}
}



%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{YavitsMulticoreAmdahl2014,
	author = { Yavits, L. and  Morad, A. and Ginosar, R.  },
	title = {{The effect of communication and synchronization on Amdahl's law in multicore systems}},
	journal = {Parallel Computing},
	volume = {40},
	year = {2014},
	pages ={1-16},
	number = {1}
}


@MISC{Epiphany2014,
author = {Adapteva},
title = {{Epiphany Architecture Reference}},
year = {2014},
howpublished = {\url{http://adapteva.com/docs/epiphany_arch_ref.pdf}}
}

@article{LimitsOfLimits2014,
	author = {Markov, I.L.},
	title = {{Limits on fundamental limits to computation}},
	journal = {Nature},
	volume = {512(7513)},
	year = {2014},
	pages = {147-154},
}

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{ReliableParallel2014,
	author = {Junfeng Yang and Heming Cui and Jingyue Wu and Yang Tang and Gang Hu},
	title = {{Making Parallel Programs Reliable with Stable Multithreading}},
	journal = {Communications of the ACM},
	volume = {57},
	year = {2014},
	pages = {58-69},
	number = {3},
	doi = {10.1145/2500875}
}

@book{hallaron,
	author = {{Randal E. Bryant and  David R. O'Hallaron}},
	title = {{Computer Systems: A Programmer's Perspective}},
	year = {2014},
	publisher = {Pearson},
	place = {Edinburgh Gate, Harlow, Essex CM20 2JE},
	isbn = {978-1-292-02584-1}
}

@ONLINE{TorwaldsParallel2014,
	author = {Linus Torwalds},
title = {{Linus: The whole "parallel computing is the future" is a bunch of crock}},
	year = {2014},
	url = {http://highscalability.com/blog/2014/12/31/linus-the-whole-parallel-computing-is-the-future-is-a-bunch.html}
}




@MISC{MIPSUserManual:2014,
	author = {{Imagination Technologies Ltd.}},
	title = {{MIPS32\raisebox{4pt}{\textregistered} \ 
	microAptiv\textsuperscript{TM} UP Processor	Core Family Software User's Manual}},
	year = {2014},
	howpublished = {\url{http://community.imgtec.com/university/}}
}


@ARTICLE{Vishkin:ViewpointProgrammingMulticore,
	AUTHOR = "Uzi Vishkin",
	TITLE = "{Is Multicore Hardware for	General-Purpose Parallel Processing Broken?}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {57},
	NUMBER = {4},
	PAGES = {35},
	MONTH = "May",
	DOI = {10.1145/2580945},
	YEAR = {2014}	}

@misc{FastAwake:lachwani2014,
  title={Fast awake from low power mode},
  author={Lachwani, M. and Puckett, J.M. and Berbessou, D. and Bowen, J.S. and Isbister, D.J.},
  url={https://www.google.com/patents/US8766919},
  year={2014},
  month=jul # "~1",
  publisher={Google Patents},
  note={US Patent 8,766,919}
}

@ARTICLE{MarkovLimitsOfLimits:2014,
   author = {I.L.~Markov},
   title = {{Limits on fundamental limits to computation}},
   journal = {Nature},
   volume = {512},
   Issue = {7513},
   pages = {147-154},
   note = {\url{http://download.nap.edu/cart/download.cgi?\&record_id=12980}},
   year = {2014}
}


 
@ARTICLE{HybridDataflowNeumann:2014,
   author = {F. Yazdanpanah and C. Alvarez-Martinez and D. Jimenez-Gonzalez and Y. Etsion},
   title = {{Hybrid Dataflow/von-Neumann Architectures}},
   journal = {{IEEE Transactions on Parallel and Distributed Systems}},
   volume = {25},
   Issue = {6},
   pages = {1489 - 1509},
   note = {\url{https://pdfs.semanticscholar.org/f1af/0791836175668598d8112df42ca091d9600b.pdf}},
   year = {2014}
}
 
 
@book{CommunicatingSerialProcesses:2015,
	editor = {Abdallah, Ali E. and Jones, Cliff and Sanders, Jeff W.},
	title = {{Communicating Sequential Processes. The First 25 Years}},
	year = {2005},
	isbn = {978-3-540-32265-87},
	publisher = {Springer},
	doi ={10.1007/b136154}
}

 
@book{HPCFPGA:2014,
	editor = {Wim Vanderbauwhede and Khaled Benkrid},
	title = {{High-Performance Computing Using FPGAs}},
	year = {2014},
	publisher = {Springer},
}



@Book{ArpaciDusseau14-Book,
  author =       {Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau},
  title =        "{Operating Systems: Three Easy Pieces}",
  publisher =    {Arpaci-Dusseau Books},
  month =        "May",
  year =         {2015},
  edition =      {0.91},
}

@ARTICLE{Larus:ProgrammingMulticoreCACM,
	AUTHOR = "James Larus",
	TITLE = "{Programming Multicore Computers}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {58},
	NUMBER = {5},
	PAGES = {76},
	MONTH = "May",
	DOI = {10.1145/2580945},
	YEAR = {2015}	}
	
	@MISC{DeepLearning2015,
	author = {Tim Dettmers},
	title = {{The Brain vs Deep Learning Part I: Computational Complexity -- Or Why the Singularity Is Nowhere Near}},
	year = {2015},
	howpublished = {\url{http://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/}}
	}
	
	
	

@inproceedings{Esmaeilzadeh:2015,
	author = {H. Esmaeilzadeh},
	title = {Approximate Acceleration: A Path Through the Era of Dark Silicon and Big Data},
	booktitle = {Proceedings of the 2015 International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
	series = {CASES '15},
	year = {2015},
	isbn = {978-1-4673-8320-2},
	location = {Amsterdam, The Netherlands},
	pages = {31--32},
	numpages = {2},
} 
%	url = {http://dl.acm.org/citation.cfm?id=2830689.2830693},
% acmid = {2830693},
% publisher = {IEEE Press},
% address = {Piscataway, NJ, USA},

@article{NinjaPerformanceGap:2015:CACM,
	author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
	title = "{Can Traditional Programming Bridge the Ninja Performance Gap for Parallel Computing Applications?}",
	journal = {Commun. ACM},
	issue_date = {May 2015},
	volume = {58},
	number = {5},
	month = apr,
	year = {2015},
	issn = {0001-0782},
	pages = {77--86},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2742910},
	doi = {10.1145/2742910},
	acmid = {2742910},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


% issn = {1544-3566},
%url = {http://doi.acm.org/10.1145/2686874},
%doi = {10.1145/2686874},

@article{Matheou:2015:ASD:2695583.2686874,
 author = {Matheou, George and Evripidou, Paraskevas},
 title = {Architectural Support for Data-Driven Execution},
 journal = {ACM Trans. Archit. Code Optim.},
 issue_date = {January 2015},
 volume = {11},
 number = {4},
 month = jan,
 year = {2015},
 pages = {52:1--52:25},
 articleno = {52},
 numpages = {25},
 acmid = {2686874},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data-Driven Multithreading, FPGA},
} 

@MISC{Cypress15,
	author = {Cypress},
	title = {{CY7C026A: 16K x 16 Dual-Port Static RAM}},
	year = {2015},
	howpublished = {\url{http://www.cypress.com/documentation/datasheets/cy7c026a-16k-x-16-dual-port-static-ram}}
}

@book{Dataflow:2015,
	editor = {Ai Hurson and Veljko Milutinovic},
	series = {Advances in Computers, Volume 96},
	title = {{Dataflow processing}},
	year = {2015},
	isbn = { 9780128021347},
	publisher = {Elsevier},
}



@book{ClockDistribution:2012,
	editor = {Rainer Waser },
	lab = {Aachen University},
	series = {Nanoelectronics and Information Technology},
	title = {{Advanced Electronics Materials and Novel Devices}},
	year = {2012},
	isbn = { 9783-527-40927-3},
	publisher = {Wiley},
}

@MISC{IntelMemorywall2015,
author = {Holger Pirk},
publisher = {Intel},
title = {{Memory Wall? What Memory Wall?}},
year = {2015},
url = {http://istc-bigdata.org/index.php/memory-wall-what-memory-wall/}
}

@Article{HPCGB:2015,
	author = {Jack Dongarra and Michael A Heroux and Piotr Luszczek},
title = {{High-performance conjugate-gradient benchmark: A new metric for ranking high-performance computing systems}},
journal = {{The International Journal of High Performance Computing Applications}},
year={2015},
url = {\url{https://doi.org/10.1177/1094342015593158}}
}


@misc{NeuromorphicComputing:2015,
author = {{US DOE Office of Science}},
title = {{Report of a Roundtable Convened to 
Consider Neuromorphic Computing 
Basic Research Needs}},
year = {2015},
howpublished = {\url{https://science.osti.gov/-/media/ascr/pdf/programdocuments/docs/Neuromorphic-Computing-Report\_FNLBLP.pdf}}
}

@Article{CooperativeComputing2015,
author="Zheng, Fang
and Li, Hong-Liang
and Lv, Hui
and Guo, Feng
and Xu, Xiao-Hong
and Xie, Xiang-Hui",
title="Cooperative Computing Techniques for a Deeply Fused and Heterogeneous Many-Core Processor Architecture",
journal="Journal of Computer Science and Technology",
year="2015",
month="Jan",
day="01",
volume="30",
number="1",
pages="145--162",
abstract="Due to advances in semiconductor techniques, many-core processors have been widely used in high performance computing. However, many applications still cannot be carried out efficiently due to the memory wall, which has become a bottleneck in many-core processors. In this paper, we present a novel heterogeneous many-core processor architecture named deeply fused many-core (DFMC) for high performance computing systems. DFMC integrates management processing elements (MPEs) and computing processing elements (CPEs), which are heterogeneous processor cores for different application features with a unified ISA (instruction set architecture), a unified execution model, and share-memory that supports cache coherence. The DFMC processor can alleviate the memory wall problem by combining a series of cooperative computing techniques of CPEs, such as multi-pattern data stream transfer, efficient register-level communication mechanism, and fast hardware synchronization technique. These techniques are able to improve on-chip data reuse and optimize memory access performance. This paper illustrates an implementation of a full system prototype based on FPGA with four MPEs and 256 CPEs. Our experimental results show that the effect of the cooperative computing techniques of CPEs is significant, with DGEMM (double-precision matrix multiplication) achieving an efficiency of 94{\%}, FFT (fast Fourier transform) obtaining a performance of 207 GFLOPS and FDTD (finite-difference time-domain) obtaining a performance of 27 GFLOPS.",
}
%issn="1860-4749",
%doi="10.1007/s11390-015-1510-9",
%url="https://doi.org/10.1007/s11390-015-1510-9"


@MISC{MentorMooreFuture2016,
author = {Mentor Graphics},
title = {{Moore's Law and the Future of Solid-State Electronics}},
year = {2016},
howpublished = {\url{http://blogs.scientificamerican.com/guest-blog/moore-s-law-and-the-future-of-solid-state-electronics/}}
}

% issn = {1544-3566},
%url = {http://doi.acm.org/10.1145/2890506},
%doi = {10.1145/2890506},
%acmid = {2890506},
@article{Braak:2016:RRG:2899032.2890506,
 author = {Braak, Gert-Jan Van Den and Corporaal, Henk},
 title = {{R-GPU: A Reconfigurable GPU Architecture}},
 journal = {ACM Trans. Archit. Code Optim.},
 issue_date = {April 2016},
 volume = {13},
 number = {1},
 month = mar,
 year = {2016},
 pages = {12:1--12:24},
 articleno = {12},
 numpages = {24},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, reconfigurable architecture},
} 


@techreport{DongarraSunwaySystem:2016,
author = {Jack Dongarra},
title = {{Report on the Sunway TaihuLight System}},
institution = {University of Tennessee
Department of Electrical Engineering and Computer Science},
year = {2016},
month = {June},
   url = {\url{http://www.netlib.org/utk/people/JackDongarra/PAPERS/sunway-report-2016.pdf}},
number = {Tech Report UT-EECS-16-742},
}

@Article{FuSunwaySystem2016,
author="Fu, Haohuan
and Liao, Junfeng
and Yang, Jinzhe
and Wang, Lanning
and Song, Zhenya
and Huang, Xiaomeng
and Yang, Chao
and Xue, Wei
and Liu, Fangfang
and Qiao, Fangli
and Zhao, Wei
and Yin, Xunqiang
and Hou, Chaofeng
and Zhang, Chenglong
and Ge, Wei
and Zhang, Jian
and Wang, Yangang
and Zhou, Chunbo
and Yang, Guangwen",
title={{The Sunway TaihuLight supercomputer: system and applications}},
journal="{Science China Information Sciences}",
year="2016",
volume="59",
number="7",
pages="1--16",
abstract="The Sunway TaihuLight supercomputer is the world's first system with a peak performance greater than 100 PFlops. In this paper, we provide a detailed introduction to the TaihuLight system. In contrast with other existing heterogeneous supercomputers, which include both CPU processors and PCIe-connected many-core accelerators (NVIDIA GPU or Intel Xeon Phi), the computing power of TaihuLight is provided by a homegrown many-core SW26010 CPU that includes both the management processing elements (MPEs) and computing processing elements (CPEs) in one chip. With 260 processing elements in one CPU, a single SW26010 provides a peak performance of over three TFlops. To alleviate the memory bandwidth bottleneck in most applications, each CPE comes with a scratch pad memory, which serves as a user-controlled cache. To support the parallelization of programs on the new many-core architecture, in addition to the basic C/C++ and Fortran compilers, the system provides a customized Sunway OpenACC tool that supports the OpenACC 2.0 syntax. This paper also reports our preliminary efforts on developing and optimizing applications on the TaihuLight system, focusing on key application domains, such as earth system modeling, ocean surface wave modeling, atomistic simulation, and phase-field simulation.",
}
%issn="1869-1919",
%doi="10.1007/s11432-016-5588-7",
%url="http://dx.doi.org/10.1007/s11432-016-5588-7"

@MISC{FileSystemHierarchy:2016,
author = {The Linux fundations},
editor = {Yeoh, Christopher; Russell, Rusty; Quinlan, Daniel, },
title = {{Filesystem Hierarchy Standard}},
year = {2016},
howpublished = {\url{https://refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf}}
}


   
@inbook{ReconfigurableAdaptive2016,
author = {Luiza de Macedo Mourelle and Nadia Nedjah and
Fábio Goncalves Pessanha},
title = {{Reconfigurable and Adaptive Computing: Theory and Applications}}, 
	editor = {Nadia Nedjah and Chao Wang},
	chapter = {5: Interprocess Communication via Crossbar for Shared Memory Systems-on-chip},
publisher = {CRC press},
year = {2016},
isbn = {978-1-4987-3176-8}
}


@book{HwangParallelism:2016,
	author = {Kai Hwang and Naresh Jotwani},
	title = {{Advanced Computer Architecture: Parallelism, Scalability, Programmability}}, 
	publisher = {Mc Graw Hill},
	isbn = {978-93-392-2093-8},
	edition = {3},
	year = {2016}
}

%	abstract=
%	{
%	https://books.google.hu/books?hl=en&lr=&id=grz-CwAAQBAJ&oi=fnd&pg=PT13&ots=fCadzG5Psv&sig=sKh_UGATcrAVaHNDewH6VMMjyG0&redir_esc=y#v=onepage&q&f=false	
%	}


@MISC{IntelFPGAcloud:2016,
author = {{cloudcomputing-news.net}},
title = {How Intel’s acquisition of Altera could transform IoT and the data center},
year = {2016},
howpublished = {\url{http://www.cloudcomputing-news.net/news/2016/apr/07/intel-acquires-altera-what-their-fpga-capabilities-could-mean-for-data-centers-and-iot//}}
}


@MISC{IntelDumpsXeonPhi:2017,
	author = {{www.top500.org}},
	title = {Intel Dumps Knights Hill, Future of Xeon Phi Product Line Uncertain},
	year = {2017},
	howpublished = {\url{https://www.top500.org/news/intel-dumps-knights-hill-future-of-xeon-phi-product-line-uncertain///}}
}


@MISC{Arpaci-Dusseau_OS:2016,
	author = {Remzi\&Andrea Arpaci-Dusseau},
	title = {{Operating Systems: Three Easy Pieces}},
	year = {2016},
	month = {July},
	howpublished = {\url{http://www.lulu.com/shop/remzi-arpaci-dusseau-and-andrea-arpaci-dusseau/operating-systems-three-easy-pieces-softcover-version-091/paperback/product-22796750.html}}
}


@MISC{NSA_DOE_HPC_Report_2016,
author = {{US Government NSA and DOE}},
title = {{A Report from the NSA-DOE Technical Meeting on High Performance Computing}},
year = {2016},
month = {December},
howpublished = {\url{https://www.nitrd.gov/ \\nitrdgroups/images/b/b4/NSA\_DOE\_HPC\_TechMeetingReport.pdf}}
}


@MISC{HPCG_List:2016,
author = {{HPCG Benchmark}},
title = {{HPCG Benchmark}},
year = {2016},
howpublished = {\url{http://www.hpcg-benchmark.org/}}
}


@misc{EUActionPlan:2016,
author = {{European Commission}},
title = {{Implementation of the Action Plan for the European High-Performance Computing strategy}},
year = {2016},
howpublished = {http://ec.europa.eu/newsroom/dae/document.cfm? doc\_id=15269}
}


@MISC{Top500:2016,
author = {{TOP500.org}},
lab = {{TOP500.org}},
title = {The Top 500 supercomputers},
year = {2019},
howpublished = {\url{https://www.top500.org/}}
}

   
%    doi = {DOI:10.1145/2976758},
 @ARTICLE{ExponentialLawsComputing:2017,
    author = {P. J. Denning and T.G. Lewis},
     title = {{Exponential Laws of Computing Growth}},
   journal = {Communications of the ACM},
      year = 2017,
      pages = {54-65},
     month = Jan,
 }

 @article{Post-Moore_2017,
author = {J. S. Vetter and E. P. DeBenedictis and T. M. Conte },
title = {{Architectures for the Post-Moore Era}},
journal = "IEEE Micro",
year = {2017},
volume = {37},
issue = {4},
pages = {6--8}
}
%10.1109/MM.2017.3211127
 
 

@article{Computing_Dark_Silicon_2017,
title = "{Computing in the dark silicon era: Current trends and research challenges}",
abstract = "Power density has become the major constraint for many on-chip designs. As an introduction to the Special Issue on Dark Silicon, the authors provide the newest trends and a survey on the topic that has valuable information for novices and experts alike.",
author = "Muhammad Shafique and Siddharth Garg",
year = "2017",
month = "4",
doi = "10.1109/MDAT.2016.2633408",
volume = "34",
pages = "8--23",
journal = "IEEE Design and Test",
issn = "2168-2356",
publisher = "IEEE Computer Society",
number = "2",
}
%{\url{http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862952}},

@article{Lifetime_Dark_Silicon_2017,
title = "
{Can Dark Silicon Be Exploited to Prolong System Lifetime?} ",
abstract = "Besides stringent power and thermal constraints, a dark silicon chip is also subjected to various reliability threats. This article illustrates how the dark silicon can be exploited to improve the chip's lifetime through efficient utilization of computational resources and power budget, while still performing in a similar way.",
author = "Mohammad-Hashem Haghbayan ; Amir M. Rahmani ; Pasi Liljeberg ; Axel Jantsch ; Antonio Miele ; Cristiana Bolchini ; Hannu Tenhunen",
year = "2017",
month = "4",
doi = "10.1109/MDAT.2016.2633408",
volume = "34",
pages = "51-59",
journal = "IEEE Design and Test",
issn = "2168-2356",
publisher = "IEEE Computer Society",
number = "2",
}


%doi={10.1109/JSSC.2016.2638459},
%ISSN={0018-9200},
@ARTICLE{KilocoreChip:2017,
author={B. Bohnenstiehl and A. Stillmaker and J. J. Pimentel and T. Andreas and B. Liu and A. T. Tran and E. Adeagbo and B. M. Baas},
journal={IEEE Journal of Solid-State Circuits},
title={{KiloCore: A 32-nm 1000-Processor Computational Array}},
year={2017},
volume={52},
number={4},
pages={891-902},
abstract={A processor array containing 1000 independent processors and 12 memory modules was fabricated in 32-nm partially depleted silicon on insulator CMOS. The programmable processors occupy 0.055 mm2 each, contain no algorithmspecific hardware, and operate up to an average maximum clock frequency of 1.78 GHz at 1.1 V. At 0.9 V, processors operating at an average of 1.24 GHz dissipate 17 mW while issuing one instruction per cycle. At 0.56 V, processors operating at an average of 115 MHz dissipate 0.61 mW while issuing one instruction per cycle, resulting in an energy consumption of 5.3 pJ/instruction. On-die communication is performed by complementary circuit and packet-based networks that yield a total array bisection bandwidth of 4.2 Tb/s. Independent memory modules handle data and instructions and operate up to an average maximum clock frequency of 1.77 GHz at 1.1 V. All processors, their packet routers, and the memory modules contain unconstrained clock oscillators within independent clock domains that adapt to large supply voltage noise. Compared with a variety of Intel i7s and Nvidia GPUs, the KiloCore at 1.1 V has geometric mean improvements of 4.3× higher throughput per area and 9.4× higher energy efficiency for AES encryption, 4095-b low-density parity-check decoding, 4096-point complex fast Fourier transform, and 100-B record sorting applications.},
keywords={CMOS digital integrated circuits;UHF integrated circuits;UHF oscillators;clocks;energy conservation;integrated circuit noise;memory architecture;multiprocessing systems;network routing;network-on-chip;parallel processing;power aware computing;power consumption;silicon-on-insulator;100-B record sorting applications;1000-processor computational array;4095-b low-density parity-check decoding;4096-point complex fast Fourier transform;AES encryption;KiloCore;NoC;complementary circuit;data handling;energy consumption;energy efficiency;geometric mean improvements;memory modules;on-die communication;packet routers;packet-based networks;partially depleted silicon on insulator CMOS;programmable processors;size 32 nm;supply voltage noise;unconstrained clock oscillators;voltage 0.56 V to 1.1 V;Clocks;Memory management;Oscillators;Parallel processing;Pipelines;Ports (Computers);Throughput;Globally asynchronous locally synchronous (GALS);NoC;many core;multicore;parallel processor},
month={April},}
 

@online{IntelArchitectureOptimization2016,
author = {Intel},
title = {{Intel® 64 and IA-32 Architectures
Optimization Reference Manual}},
year = {2016},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf}}
}


@inproceedings{IBMasynchronous:2016,
	author = {Akopyan, Filipp},
	title = {{Design and Tool Flow of IBM's TrueNorth: An Ultra-Low Power Programmable Neurosynaptic Chip with 1 Million Neurons}},
	booktitle = {Proceedings of the 2016 on International Symposium on Physical Design},
	series = {ISPD '16},
	year = {2016},
	isbn = {978-1-4503-4039-7},
	location = {Santa Rosa, California, USA},
	pages = {59--60},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/2872334.2878629},
	doi = {10.1145/2872334.2878629},
	acmid = {2878629},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {asynchronous circuits, asynchronous communication, custom tool flow, design automation, design methodology, image recognition, logic design, low-power electronics, neural network hardware, neural networks, neuromorphics, parallel architectures, real-time systems, synchronous circuits, very large-scale integration},
} 



@inproceedings{WhyNotExascale:2014,
	author = {Simon, Horst},
	title = {{Why we need Exascale and why we won't get there by 2020}},
	booktitle = {Exascale Radioastronomy Meeting},
	series = {AASCTS2},
	year = {2014},
	location = {Monterey, California, USA},
	numpages = {60},
	url = {https://www.researchgate.net/publication/261879110\\ \_Why\_we\_need\_Exascale\_and\_why\_we\_won't\_get \_there\_by\_2020},
} 

@online{PEZY2048cores:2017,
author = {PEZY},
title = {{2048 core chip}},
year = {2017},
howpublished = {\url{https://www.top500.org/green500/lists/ 2017/11/}}
}

 
@online{HP_MemoryDriven:2017,
author = {Hewlett Packard Enterprise (HPE)},
title = {{HP unleashes "The Machine" memory-centric supercomputer prototype}},
year = {2017},
howpublished = {\url{https://newatlas.com/hewlett-packard-enterprise-the-machine-big-data/49561/}}
}



@misc{HPCGPerformanceList:2017,
author = {{HPCG Benchmark}},
title = {{7th HPCG Performance List}},
year = {2017},
howpublished = {\url{http://www.hpcg-benchmark.org/custom/index.html?lid=154\&slid=292}}
}


@misc{DongarraExascaleRace:2017,
author = {J. Dongarra},
title = {{The Global Race for Exascale High Performance Computing}},
year = {2017},
howpublished = {\url{http://ec.europa.eu/newsroom/document.cfm?doc_id=45647}}
}

 
 @misc{Gyoukou:2017,
 author = {TOP500},
 title = {{November 2017 list of supercomputers}},
 year = {2017},
 howpublished = {\url{https://www.top500.org/lists/2017/11/}}
 }
 
@book{RISCVarchitecture:2017,
	editor = {D.A. Patterson and J.L. Hennessy},
	title = {{Computer Organization and design. RISC-V Edition}},
	year = {2017},
	publisher = {Morgan Kaufmann},
}


@inproceedings{BenchmarkingClouds:2017,
	author = {Mohammadi, Mohammad and Bazhirov, Timur},
	title = {{Comparative Benchmarking of Cloud Computing Vendors with High Performance Linpack}},
	booktitle = {Proceedings of the 2Nd International Conference on High Performance Compilation, Computing and Communications},
	series = {HP3C},
	year = {2018},
	isbn = {978-1-4503-6337-2},
	location = {Hong Kong, Hong Kong},
	pages = {1--5},
	numpages = {5},
	acmid = {3195613},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {benchmarking, cloud computing, high performance computing, linpack},
} 

%	url = {http://doi.acm.org/10.1145/3195612.3195613},
% doi = {10.1145/3195612.3195613},

@misc{PricePerformanceClouds:2017,
author = {E. Wustenhoff and T. S. E. Ng},
lab = {""},
title = {{Cloud Computing Benchmark}},
year = {2017},
howpublished = {\url{https://www.burstorm.com/price-performance-benchmark/1st-Continuous-Cloud-Price-Performance-Benchmarking.pdf}}
}


@misc{DifferentBenchmarks:2017,
author = {{IEEE Spectrum}},
title = {{Two Different Top500 Supercomputing Benchmarks Show Two Different Top Supercomputers}},
year = {2017},
howpublished = {\url{https://spectrum.ieee.org/tech-talk/computing/hardware/two-different-top500-supercomputing- benchmarks-show\ -two -different-top-supercomputers}}
}

@misc{TOP500:2017,
author = {TOP500},
title = {{November 2017 list of supercomputers}},
year = {2017},
howpublished = {\url{https://www.top500.org/lists/2017/11/}}
}

@misc{DOEAurora:2017,
author = {{Top500.org}},
title = {{Retooled Aurora Supercomputer Will Be America’s First Exascale System}},
year = {2017},
howpublished = {\url{https://www.top500.org/news/retooled-aurora-supercomputer-will-be -americas- first-exascale-system/}}
}

@misc{DOEAuroraMistery:2017,
author = {{Inside HPC}},
title = {{Is Aurora Morphing into an Exascale AI Supercomputer?}},
year = {2017},
 howpublished = {\url{https://insidehpc.com/2017/06/told-aurora-morphing-novel-architecture-ai-supercomputer/}}
}

@misc{SupercomputersCosmicRay:2018,
	author = {\url{wired.com}},
	title = {{Cosmic Ray Showers Crash Supercomputers. Here's What to Do About It}},
	year = {2018},
	howpublished = {\url{https://www.wired.com/story/cosmic-ray-showers-crash-supercomputers-heres-what-to-do-about-it/}}
}



@misc{PEZY-SC2:2017,
author = {{fuse.wikichip.org}},
title = {{The 2,048-core PEZY-SC2 sets a Green500 record}},
year = {2017},
 howpublished = {\url{https://fuse.wikichip.org/news/191/the-2048-core-pezy-sc2-sets-a-green500-record/}}
 }

@InProceedings{AmdahlsRefutation_Devai2017,
author="D{\'e}vai, Ferenc",
editor="Gervasi, Osvaldo
and Murgante, Beniamino
and Misra, Sanjay
and Borruso, Giuseppe
and Torre, Carmelo M.
and Rocha, Ana Maria A.C.
and Taniar, David
and Apduhan, Bernady O.
and Stankova, Elena
and Cuzzocrea, Alfredo",
title="{The Refutation of Amdahl's Law and Its Variants}",
booktitle="Computational Science and Its Applications -- ICCSA 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="480--493",
abstract="Amdahl's law, imposing a restriction on the speedup achievable by a multiple number of processors, based on the concept of sequential and parallelizable fractions of computations, has been used to justify, among others, asymmetric chip multiprocessor architectures and concerns of ``dark silicon''. This paper demonstrates flaws in Amdahl's law that (i) in theory no inherently sequential fractions of computations exists (ii) sequential fractions appearing in practice are inherently different from parallelizable fractions and therefore usually have different growth rates and that (iii) the time requirement of sequential fractions can be proportional to the number of processors. However, mathematical analyses are also provided to demonstrate that sequential fractions have negligible effect on speedup if the growth rate of the parallelizable fraction is higher than that of the sequential fraction. Examples from computational geometry are given that Amdahl's law and its variants fail to represent limits to parallel computation. In particular, Gustafson's law, claimed to be a refutation of Amdahl's law by some authors, is shown to contradict established theoretical results. We can conclude that no simple formula or law governing concurrency exists.",
isbn="978-3-319-62395-5"
}

@misc{GyoukouFraud:2017,
author = {{The Japan Times}},
title = {{Chief of firm behind world’s fourth-fastest supercomputer arrested in Tokyo for alleged fraud}},
year = {2017},
howpublished = {\url{https://www.japantimes.co.jp/news/2017/12/05/national/crime-legal/chief-firm-behind-worlds-fourth-fastest-supercomputer-arrested-tokyo-alleged-fraud/\#.WmQ-KXRG3CI}}
}

@article{StrechingSupercomputers:2017,
	author = {K. Bourzac},
	title = {{Streching supercomputers to the limit}},
	journal = {Nature},
	volume = {551},
	year = {2017},
	pages = {554-556},
}


@misc{AuroraEarlyScience:2017,
author = {{Top500.org}},
title = {{DOE Witholds Details of First Exascale Supercomputer, Even as it Solicits Researchers to Apply for Early Access}},
year = {2018},
howpublished = {\url{https://www.top500.org/news/doe-witholds-details-of-first-exascale -supercomputer- even-as-it-solicits-researchers -to-apply-for-early-access/}}
}

@misc{ExaScaleRace:2018,
	author = {{Top500.org}},
	title = {{The race to exascale}},
	year = {2018},
	howpublished = {\url{https://sciencenode.org/feature/the-race-to-exascale.php}}
}

@article{GordonBellPrize:2017,
	abstract = { The Gordon Bell Prize is awarded each year by the Association for Computing Machinery to recognize outstanding achievement in high-performance computing (HPC). The purpose of the award is to track the progress of parallel computing with particular emphasis on rewarding innovation in applying HPC to applications in science, engineering, and large-scale data analytics. Prizes may be awarded for peak performance or special achievements in scalability and time-to-solution on important science and engineering problems. Financial support for the US\$10,000 award is provided through an endowment by Gordon Bell, a pioneer in high-performance and parallel computing. This article examines the evolution of the Gordon Bell Prize and the impact it has had on the field. },
	author = {Gordon Bell and David H Bailey and Jack Dongarra and Alan H Karp and Kevin Walsh},
	journal = {The International Journal of High Performance Computing Applications},
	number = {6},
	pages = {469–484},
	title = {{A look back on 30 years of the Gordon Bell Prize}},
	url = { https://doi.org/10.1177/1094342017738610},
	volume = {31},
	year = {2017}
}
%	doi = {10.1177/1094342017738610},
% eprint = {https://doi.org/10.1177/1094342017738610 },


@MISC{SystemChome:2017,
author = {IEEE/Accellera},
title = {{Systems initiative}},
year = {2017},
howpublished = {\url{http://www.accellera.org/downloads/standards/systemc}}
}



@article{Scienceexascale:2018,
	author = {Robert F. Service},
	title = {{Design for U.S. exascale computer takes shape}},
	journal = {Science},
	volume = {359},
	issue={6376},
	year = {2018},
	pages = {617--618},
}

%  volume    = {abs/1708.01462},
%  url       = {http://arxiv.org/abs/1708.01462},
%  archivePrefix = {arXiv},
%  eprint    = {1708.01462},
%  timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
%  biburl    = %{http://dblp.org/rec/bib/journals/corr/abs-1708-01462},
%  bibsource = {dblp computer science bibliography, http://dblp.org}

@article{FogCloudComputing:2018,
	author = {J. Du and L. Zhao and J. Feng and X. Chu},
	title = {{Computation Offloading and Resource Allocation in Mixed Fog/Cloud Computing Systems With Min-Max Fairness Guarantee}},
	journal = {IEEE Transactions on Communications},
	volume = {66},
	issue={4},
	year = {2018},
	pages = {1594--1608},
}



%issn = "0167-8191",
%doi = "https://doi.org/10.1016/j.parco.2018.03.001",

@ARTICLE{SpikingPetascale2014,	
	AUTHOR={Kunkel, Susanne and Schmidt, Maximilian and Eppler, Jochen M. and Plesser, Hans E. and Masumoto, Gen and Igarashi, Jun and Ishii, Shin and Fukai, Tomoki and Morrison, Abigail and Diesmann, Markus and Helias, Moritz},   	
	TITLE={Spiking network simulation code for petascale computers},      	
	JOURNAL={Frontiers in Neuroinformatics},      	
	VOLUME={8},      	
	PAGES={78},     	
	YEAR={2014},      	
	DOI={10.3389/fninf.2014.00078},      	
	ISSN={1662-5196},   	
	ABSTRACT={Brain-scale networks exhibit a breathtaking heterogeneity in the dynamical properties and parameters of their constituents. At cellular resolution, the entities of theory are neurons and synapses and over the past decade researchers have learned to manage the heterogeneity of neurons and synapses with efficient data structures. Already early parallel simulation codes stored synapses in a distributed fashion such that a synapse solely consumes memory on the compute node harboring the target neuron. As petaflop computers with some 100,000 nodes become increasingly available for neuroscience, new challenges arise for neuronal network simulation software: Each neuron contacts on the order of 10,000 other neurons and thus has targets only on a fraction of all compute nodes; furthermore, for any given source neuron, at most a single synapse is typically created on any compute node. From the viewpoint of an individual compute node, the heterogeneity in the synaptic target lists thus collapses along two dimensions: the dimension of the types of synapses and the dimension of the number of synapses of a given type. Here we present a data structure taking advantage of this double collapse using metaprogramming techniques. After introducing the relevant scaling scenario for brain-scale simulations, we quantitatively discuss the performance on two supercomputers. We show that the novel architecture scales to the largest petascale supercomputers available today.}
}    

%URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00291},       
%DOI={10.3389/fnins.2018.00291},      
%ISSN={1662-453X},   

@inproceedings{DeepNeuralNetworkTraining:2016,
	author = { Janis Keuper and Franz-Josef Preundt},
	title = {{Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability}},
	booktitle = {2nd Workshop on Machine Learning in HPC Environments (MLHPC)},
	year = {2016},
	pages = {1469--1476},
	numpages = {11},
	publisher = {IEEE},
	doi = {10.1109/MLHPC.2016.006},
	url = {https://www.researchgate.net/publication/308457837}
} 




@MISC{Saturating:2017,
	author = {Stefan Hamminga},
	title = {{C++ saturating arithmetic functions and types}},
	year = {2017},
	howpublished = {\url{https://github.com/StefanHamminga/saturating}}
}


@inproceedings{HalfPrecisionArithmetic:2017,
	author = {Haidar, Azzam and Wu, Panruo and Tomov, Stanimire and Dongarra, Jack},
	title = {{Investigating Half Precision Arithmetic to Accelerate Dense Linear System Solvers}},
	booktitle = {{Proceedings of the 8th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems}},
	series = {ScalA '17},
	year = {2017},
	location = {Denver, Colorado},
	pages = {10:1--10:8},
	articleno = {10},
	numpages = {8},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {GPGPU, HPC, half precision, mixed-precision iterative refinement},
} 
%	isbn = {978-1-4503-5125-6},
%	url = {http://doi.acm.org/10.1145/3148226.3148237},
% doi = {10.1145/3148226.3148237},
% acmid = {3148237},


@ARTICLE{NeuralNetworkPerformance:2018,
AUTHOR={van Albada, Sacha J. and Rowley, Andrew G. and Senk, Johanna and Hopkins, Michael and Schmidt, Maximilian and Stokes, Alan B. and Lester, David R. and Diesmann, Markus and Furber, Steve B.},   
TITLE={{Performance Comparison of the Digital Neuromorphic Hardware SpiNNaker and the Neural Network Simulation Software NEST for a Full-Scale Cortical Microcircuit Model}},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={12},      
PAGES={291},     
YEAR={2018},      
ABSTRACT={The digital neuromorphic hardware SpiNNaker has been developed with the aim of enabling
large-scale neural network simulations in real time and with low power consumption. Real-time
performance is achieved with 1 ms integration time steps, and thus applies to neural networks
for which faster time scales of the dynamics can be neglected. By slowing down the simulation,
shorter integration time steps and hence faster time scales, which are often biologically relevant,
can be incorporated. We here describe the first full-scale simulations of a cortical microcircuit
with biological time scales on SpiNNaker. Since about half the synapses onto the neurons arise
within the microcircuit, larger cortical circuits have only moderately more synapses per neuron.
Therefore, the full-scale microcircuit paves the way for simulating cortical circuits of arbitrary size.
With approximately 80,000 neurons and 0.3 billion synapses, this model is the largest simulated on
SpiNNaker to date. The scale-up is enabled by recent developments in the SpiNNaker software
stack that allow simulations to be spread across multiple boards. Comparison with simulations
using the NEST software on a high-performance cluster shows that both simulators can reach
a similar accuracy, despite the fixed-point arithmetic of SpiNNaker, demonstrating the usability
of SpiNNaker for computational neuroscience applications with biological time scales and large
network size. The runtime and power consumption are also assessed for both simulators on the
example of the cortical microcircuit model. To obtain an accuracy similar to that of NEST with
0.1 ms time steps, SpiNNaker requires a slowdown factor of around 20 compared to real time.
The runtime for NEST saturates around 3 times real time using hybrid parallelization with MPI
and multi-threading. However, achieving this runtime comes at the cost of increased power and
energy consumption. The lowest total energy consumption for NEST is reached at around 144
parallel threads and 4.6 times slowdown. At this setting, NEST and SpiNNaker have a comparable
energy consumption per synaptic event. Our results widen the application domain of SpiNNaker
and help guide its development, showing that further optimizations such as synapse-centric
network representation are necessary to enable real-time simulation of large biological neural
networks.}

}

@ARTICLE{NeuralScaling2017,
	AUTHOR={Ippen, Tammo and Eppler, Jochen M. and Plesser, Hans E. and Diesmann, Markus},   
	TITLE={{Constructing Neuronal Network Models in Massively Parallel Environments}},      
	JOURNAL={Frontiers in Neuroinformatics},      
	VOLUME={11},      
	PAGES={30},     
	YEAR={2017},      
	ABSTRACT={Recent advances in the development of data structures to represent spiking neuron network models enable us to exploit the complete memory of petascale computers for a single brain-scale network simulation. In this work, we investigate how well we can exploit the computing power of such supercomputers for the creation of neuronal networks. Using an established benchmark, we divide the runtime of simulation code into the phase of network construction and the phase during which the dynamical state is advanced in time. We find that on multi-core compute nodes network creation scales well with process-parallel code but exhibits a prohibitively large memory consumption. Thread-parallel network creation, in contrast, exhibits speedup only up to a small number of threads but has little overhead in terms of memory. We further observe that the algorithms creating instances of model neurons and their connections scale well for networks of ten thousand neurons, but do not show the same speedup for networks of millions of neurons. Our work uncovers that the lack of scaling of thread-parallel network creation is due to inadequate memory allocation strategies and demonstrates that thread-optimized memory allocators recover excellent scaling. An analysis of the loop order used for network construction reveals that more complex tests on the locality of operations significantly improve scaling and reduce runtime by allowing construction algorithms to step through large networks more efficiently than in existing code. The combination of these techniques increases performance by an order of magnitude and harnesses the increasingly parallel compute power of the compute nodes in high-performance clusters and supercomputers.}
}

%	URL={https://www.frontiersin.org/article/10.3389/fninf.2017.00030},       
% DOI={10.3389/fninf.2017.00030},      
% ISSN={1662-5196},   

@MISC{IBMAsynchronousAPI2017,
	author = {Gohil, P and  Horn, J and He, J and  Papageorgiou, A and Poole,C},
	title = {{IBM CICS Asynchronous API: Concurrent Processing Made Simple}},
	year = {2017},
	howpublished = {
	\url{http://www.redbooks.ibm.com/redbooks/pdfs/sg248411.pdf
	}
	}
}


@article{patent:IBMNeural2018,
	title     = " GRAPH PARTITIONING AND PLACEMENT FOR MULTI - CHIP NEUROSYNAPTIC NETWORKS",
	number    = "0260682",
	author    = "Arnon Amir and Pallab Datta  and Myron D . Flickner  and Dharmendra S . Modha and  Tapan K . Nayak",
	year      = "2017",
	month     = "Mar",
	url       = "https://patentimages.storage.googleapis.com/8e/51/4a/204809be525db9/US20180260682A1.pdf"
}

@article{patent:FacebookNeural2018,
	title     = "  SEQUENCE - TO - SEQUENCE CONVOLUTIONAL ARCHITECTURE",
	number    = "0261214",
	author    = "Jonas Gehring and Michael Auli and Yann Nicolas Dauphin and David G . Grangier and Dzianis Yarats)",
	year      = "2017",
	month     = "Dec",
	url       = "https://https://patentimages.storage.googleapis.com/6a/67/30/1d5b7f99f4b1f8/US20180261214A1.pdf"
}

@article{patent:GoogleNeural2018,
	title     = "  SEQUENCE - TO - SEQUENCE CONVOLUTIONAL ARCHITECTURE",
	number    = "0260220",
	author    = "William  Lacy and Gregory Michael Thorson and Christopher Aaron Clark and Norman Paul Jouppi and Thomas Norrie and Andrew Everett Phelps",
	year      = "2017",
	month     = "Mar",
	url       = "https://patentimages.storage.googleapis.com/ae/ec/c7/b6c9311ebee07b/US20180260220A1.pdf"
}

@article{RejectingMemristor:2018,
	author = {Abraham, I},
	title = {{The case for rejecting the memristor as a fundamental circuit element.}},
	journal = {Scientific Reports},
	volume = {8},
	year = {2018},
	pages = {10972},
	publisher = {ACM},
	doi = {10.1038/s41598-018-29394-7},
} 

https://doi.org/10.1038/s41598-018-29394-7

@article{TaihulightHPCG:2018,
	author = {Ao, Yulong and Yang, Chao and Liu, Fangfang and Yin, Wanwang and Jiang, Lijuan and Sun, Qiao},
	title = {{Performance Optimization of the HPCG Benchmark on the Sunway TaihuLight Supercomputer}},
	journal = {ACM Trans. Archit. Code Optim.},
	issue_date = {April 2018},
	volume = {15},
	number = {1},
	month = mar,
	year = {2018},
	pages = {11:1--11:20},
	articleno = {11},
	numpages = {20},
	acmid = {3182177},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {HPCG, Sunway TaihuLight, heterogeneous many-core processor, performance optimization},
} 


@article{patent:20180189231,
	title     = "PROCESSORS, METHODS, AND SYSTEMS WITH A CONFIGURABLE SPATIAL ACCELERATOR",
	number    = "20180189231",
	author    = "Fleming Jr. and Kermin E.  and Glossop, Kent D.  and Steely Jr., Simon C.  and Tang, Jinjie  and Gara, Alan G.",
	year      = "2018",
	month     = "July",
	url       = "http://www.freepatentsonline.com/y2018/0189231.html"
}


@MISC{CosmicRaysSupercomputers:2018,
	author = {wrired,com},
	title = {{Cosmic Ray Showers Crash Supercomputers. Here's What to Do About It}},
	year = {2018},
	howpublished = {\url{https://www.wired.com/story/cosmic-ray-showers-crash-supercomputers-heres-what-to-do-about-it/
	}}
}

@MISC{IntelSectretExascale:2017,
	author = {Intel},
	title = {{Looking Ahead to Intel’s Secret Exascale Architecture}},
	year = {2017},
	howpublished = {\url{https://www.nextplatform.com/2017/11/14/looking-ahead-intels-secret-exascale-architecture/
	}}
}

@MISC{KeepingComputerIndustryInUS:2017,
	author = {T. M. Conte and E. P. Debenedictis and R. S. William sand  M. D. Hill},
	title = {{Challenges to Keeping the Computer Industry Centered in the US}},
	year = {2017},
	howpublished = {\url{https://arxiv.org/abs/1706.10267
	}}
}



@MISC{IntelSwitchableTopology:2018,
	author = {Intel},
	title = {{Switchable topology machine}},
	year = {2016},
	howpublished = {\url{https://patents.google.com/patent/US20180113838A1
	}}
}


@MISC{NokiaParallelMulticore:2013,
	author = {Nokia},
	title = {{Method, apparatus, and computer program product for parallel functional units in multicore processors}},
	year = {2013},
	howpublished = {\url{https://patents.google.com/patent/US20130151817A1/
	}}
}

@MISC{HumanBrainProject:2018,
	author = {EU Human Brain Project},
	title = {{Human Brain Project}},
	year = {2018},
	howpublished = {\url{https://www.humanbrainproject.eu/en/
	}}
}


@MISC{QuantumComputing:2018,
	author = {M. Dyakonov},
	title = {{The Case Against Quantum Computing}},
	year = {2018},
	howpublished = {\url{https://spectrum.ieee.org/computing/hardware/the-case-against-quantum-computing
	}}
}

@MISC{IntelDropsX86:2018,
	author = {\textit{Intel}},
	title = {{Intel’s Exascale Dataflow Engine Drops X86 And Von Neumann}},
	year = {2018},
	howpublished = {\url{https://www.nextplatform.com/2018/08/30/intels-exascale-dataflow-engine-drops-x86-and-von-neuman/
	}}
}

@MISC{IntelDataflowPatent:2018,
	author = {Intel},
	title = {{Processors, methods and systems with a configurable spatial accelerator}},
	year = {2018},
	howpublished = {\url{http://www.freepatentsonline.com/y2018/0189231.html}}
}

@MISC{JapanExascale:2018,
author = {Extremtech},
	year = "2018",
	title = {{ Japan Tests Silicon for Exascale Computing in 2021.}},
howpublished="https://www.extremetech.com/computing/
	272558-japan-tests-silicon-for-exascale-computing -in-2021"
}

@ARTICLE{VeghMendedCores:2018,
	author = {J. V\'egh and J. V\'as\'arhelyi},
	title =  {{Can Broken Multicore Hardware be Mended?}},
	journal = {	Global Journal of Researches in Engineering },
	year = {2018},
	volume = {18},
	issue = {1},
	pages = {15--21}
}
%ISSN: 2249-4596


@article{ChinaExascale:2018,
	abstract = "High-performance computing (HPC) is essential for both traditional and emerging scientific fields, enabling scientific activities to make progress. With the development of high-pThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	erformance computing, it is foreseeable that exascale computing will be put into practice around 2020. As Moore's law approaches its limit, high-performance computing will face severe challenges when moving from exascale to zettascale, making the next 10 years after 2020 a vital period to develop key HPC techniques. In this study, we discuss the challenges of enabling zettascale computing with respect to both hardware and software. We then present a perspective of future HPC technology evolution and revolution, leading to our main recommendations in support of zettascale computing in the coming future.",
	author = "{Liao, Xiang-ke et al}",
	doi = "10.1631/FITEE.1800494",
	issn = "2095-9230",
	journal = "Frontiers of Information Technology {\&} Electronic Engineering",
	month = "Oct",
	number = "10",
	pages = "1236–1244",
	title = "Moving from exascale to zettascale computing: challenges and techniques",
	url = "https://doi.org/10.1631/FITEE.1800494",
	volume = "19",
	year = "2018"
}

%	author = "{Liao, Xiang-ke and Lu, Kai and Yang, Can-qun and Li, Jin-wen and Yuan, Yuan and Lai, Ming-che and Huang, Li-bo and Lu, Ping-jing and Fang, Jian-bin and Ren, Jing and Shen, Jie}",

   

%https://www.zdnet.com/article/neuromorphic-computing-and-the-brain-that-wouldnt-die/
%https://www.zdnet.com/article/the-rise-fall-and-rise-of-the-supercomputer-in-the-cloud-era
%https://www.zdnet.com/article/supercomputing-and-ai-meets-the-cloud/
%https://features.slashdot.org/story/99/05/27/0126232/the-power-of-deep-computing
%

@article{CommunicationCollapse:2018,
	year = 2018,
	month = {oct},
	publisher = {{IOP} Publishing},
	volume = {52},
	number = {1},
	pages = {014003},
	author = {Saber Moradi and Rajit Manohar},
	title = {{The impact of on-chip communication on memory technologies for neuromorphic systems}},
	journal = {Journal of Physics D: Applied Physics},
	abstract = {Emergent nanoscale non-volatile memory technologies with high integration density offer a promising solution to overcome the scalability limitations of CMOS-based neural networks architectures, by efficiently exhibiting the key principle of neural computation. Despite the potential improvements in computational costs, designing high-performance on-chip communication networks that support flexible, large-fanout connectivity remains as daunting task. In this paper, we elaborate on the communication requirements of large-scale neuromorphic designs, and point out the differences with the conventional network-on-chip architectures. We present existing approaches for on-chip neuromorphic routing networks, and discuss how new memory and integration technologies may help to alleviate the communication issues in constructing next-generation intelligent computing machines.}
}
% https://arxiv.org/pdf/1809.06016.pdf
%	doi = {10.1088/1361-6463/aae641},
%url = {https://doi.org/10.1088\%2F1361-6463\%2Faae641},


@article{LiaoZettaScale:2018,
	abstract = "High-performance computing (HPC) is essential for both traditional and emerging scientific fields, enabling scientific activities to make progress. With the development of high-pThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	erformance computing, it is foreseeable that exascale computing will be put into practice around 2020. As Moore's law approaches its limit, high-performance computing will face severe challenges when moving from exascale to zettascale, making the next 10 years after 2020 a vital period to develop key HPC techniques. In this study, we discuss the challenges of enabling zettascale computing with respect to both hardware and software. We then present a perspective of future HPC technology evolution and revolution, leading to our main recommendations in support of zettascale computing in the coming future.",
	author = "Liao, Xiang-ke and Lu, Kai and Yang, Can-qun and Li, Jin-wen and Yuan, Yuan and Lai, Ming-che and Huang, Li-bo and Lu, Ping-jing and Fang, Jian-bin and Ren, Jing and Shen, Jie",
	day = "01",
	doi = "10.1631/FITEE.1800494",
	issn = "2095-9230",
	journal = "Frontiers of Information Technology {\&} Electronic Engineering",
	month = "Oct",
	number = "10",
	pages = "1236–1244",
	title = "Moving from exascale to zettascale computing: challenges and techniques",
	url = "https://doi.org/10.1631/FITEE.1800494",
	volume = "19",
	year = "2018"
}

@inproceedings{MixedPrecisionHPL:2018,
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	title = {{Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed Up Mixed-precision Iterative Refinement Solvers}},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
	series = {SC '18},
	year = {2018},
	location = {Dallas, Texas},
	pages = {47:1--47:11},
	articleno = {47},
	numpages = {11},
	publisher = {IEEE Press},
	keywords = {FP16 arithmetic, GPU computing, half precision, iterative refinement computation, linear algebra, mixed precision solvers},
} 
%	url = {https://doi.org/10.1109/SC.2018.00050},
%doi = {10.1109/SC.2018.00050},
%acmid = {3291719},
%	address = {Piscataway, NJ, USA},

@MISC{ExascaleGrandfatherHPC:2019,
	author = {M. Feldman},
	title = {{Exascale Is Not Your Grandfather’s HPC}},
	year = {2019},
	howpublished = {\url{https://www.nextplatform.com/2019/10/22/exascale-is-not-your-grandfathers-hpc/}}
}
@MISC{GustafsonFloating:2018,
	author = {John L. Gustafson and Isaac Yonemoto},
	title = {{Beating Floating Point at its Own Game: Posit Arithmetic}},
	doi = {10.14529/jsﬁ170206},
	year = {2018},
	howpublished = {\url{http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf}}
}

@MISC{WiringDominance:2019,
	author = {Waine Luk},
	title = {{Imperial College London, textbook}},
	lab =  {{Imperial College London}},
	year = {2019},
	howpublished = {\url{http://www.imperial.ac.uk/~wl/teachlocal/cuscomp/notes/chapter2.pdf}}
}

http://www.imperial.ac.uk/~wl/teachlocal/cuscomp/notes/chapter2.pdf

@ARTICLE{SpiNNaker2:2018,
	
	AUTHOR={Liu, Chen and Bellec, Guillaume and Vogginger, Bernhard and Kappel, David and Partzsch, Johannes and Neumärker, Felix and Höppner, Sebastian and Maass, Wolfgang and Furber, Steve B. and Legenstein, Robert and Mayr, Christian G.},   
	
	TITLE={{Memory-Efficient Deep Learning on a SpiNNaker 2 Prototype}},      
	
	JOURNAL={Frontiers in Neuroscience},      
	
	VOLUME={12},      
	
	PAGES={840},     
	
	YEAR={2018},      
	
	URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00840},       
	
	DOI={10.3389/fnins.2018.00840},      
	
	ISSN={1662-453X},   
	
	ABSTRACT={The memory requirement of deep learning algorithms is considered incompatible with the memory restriction of energy-efficient hardware. A low memory footprint can be achieved by pruning obsolete connections or reducing the precision of connection strengths after the network has been trained. Yet, these techniques are not applicable to the case when neural networks have to be trained directly on hardware due to the hard memory constraints. Deep Rewiring (DEEP R) is a training algorithm which continuously rewires the network while preserving very sparse connectivity all along the training procedure. We apply DEEP R to a deep neural network implementation on a prototype chip of the 2nd generation SpiNNaker system. The local memory of a single core on this chip is limited to 64 KB and a deep network architecture is trained entirely within this constraint without the use of external memory. Throughout training, the proportion of active connections is limited to 1.3%. On the handwritten digits dataset MNIST, this extremely sparse network achieves 96.6% classification accuracy at convergence. Utilizing the multi-processor feature of the SpiNNaker system, we found very good scaling in terms of computation time, per-core memory consumption, and energy constraints. When compared to a X86 CPU implementation, neural network training on the SpiNNaker 2 prototype improves power and energy consumption by two orders of magnitude.}
}

@MISC{OpenCAPI:2019,
	author = {IBM},
	title = {{Why IBM sees OpenCAPI and OMI as the future for accelerator-driven computing}},
	year = {2019},
	howpublished = {\url{https://www.techrepublic.com/article/why-ibm-sees-opencapi-and-omi-as-the-future-for-accelerator-driven-computing/}}
}

@MISC{ToolingUpForExascale:2019,
	author = {www.nextplatform.com},
	title = {{Tooling Up For Exascale}},
	year = {2019},
	howpublished = {\url{https://www.nextplatform.com/2019/11/12/tooling-up-for-exascale/}}
}

%https://sciencenode.org/feature/the-race-to-exascale.php


@inproceedings{RebootingComputingModels:2019,
	author = {P. Cadareanu et al.},
	title = {{Rebooting Our Computing Models}},
	booktitle = {Proceedings of the  2019 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
	year = {2019},
	pages = {1469--1476},
	numpages = {11},
	publisher = {IEEE Press},
	doi = {10.23919/DATE.2019.8715167},
} 


@techreport{DongarraFugakuSystem:2020,
	author = {Jack Dongarra},
	title = {{Report on the Fujitsu Fugaku System}},
	institution = {University of Tennessee
	Department of Electrical Engineering and Computer Science},
	year = {2016},
	month = {June},
	number = {Tech Report ICL-UT-20-06},
}
%	url = {\url{http://bit.ly/fugaku-report}},

%https://www.bnl.gov/modsim2019/files/talks/SatoshiMatsuoka.pdf
%https://www.cnet.com/news/fujitsu-supercomputer-simulates-1-second-of-brain-activity/