
%% AI- related citations


@article{TuringTest:1950,
	title={{Computing Machinery and Intelligence}},
	author={A. M. Turing},
	journal={Mind},
	year={1950},
	volume={LIX},
	issue={236},
	pages={433-460},
	doi={10.1093/mind/LIX.236.433}
}


@article{ML_Checkers_Game:1959,
	 author = {Samuel, A.R.},
	 title = {Some studies in machine learning using the game of checkers},
	 journal = {IBM J Res Dev},
	 year = {1959},
	 volume = {44},
	 pages = {1210–1229},
	}


@misc{AILighthillReport:1972,
	  author = { Sir Lighthill, James},
	  title = {{Artificial Intelligence: A General Survey}},
howpublished={\url{https://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm}},
}


@ARTICLE {ComputerBrainSejnowski:1989,
	author = {T. J. Sejnowski},
	journal = {IEEE Annals of the History of Computing},
	title = {The Computer and the Brain Revisited},
	year = {1989},
	volume = {11},
	number = {03},
	issn = {1934-1547},
	pages = {197-201},
	keywords = {},
	doi = {10.1109/MAHC.1989.10028},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jul}
}



@article{NeuromorphicSystems:1990,
	author = {Carver Mead},
	title = {{Neuromorphic electronic systems}},
	journal = {{Proc. IEEE}},
	volume = {78},
	year = {1990},
	issn = {1936-7406},
	pages = {1629--1636},
} 
%	doi = {10.1146/annurev-neuro-062111-150444},


@misc{AIDangerousThoughts:1991,
	author = {Turkle, Sherry},
	title = {{Dangerous Thoughts . . . And Machines With Big Ideas}},
	year = {1991},
howpublished = {\url{https://www.nytimes.com/1991/03/17/books/dangerous-thoughts-and-machines-with-big-ideas.html}},
				note = {Accessed: 2023-08-30}

}


@article{FurberNeuralEngineering:2007,
	author = {{S. Furber and S. Temple}},
	title = {{Neural systems engineering}},
	journal = {{J. R. Soc. Interface}},
	volume = {4},
	year = {2007},
	pages = {193--206},
	numpages = {14},
	doi = {10.1098/rsif.2006.0177},
} 

@article{IBMTrueNorthMillionSpikingNeuron:2014,
	title={A million spiking-neuron integrated circuit with a scalable communication network and interface},
	author={Paul Merolla and John V. Arthur and Rodrigo Alvarez-Icaza and Andrew S. Cassidy and Jun Sawada and Filipp Akopyan and Bryan L. Jackson and Nabil Imam and Chen Guo and Yutaka Nakamura and Bernard Brezzo and Ivan Vo and Steven K. Esser and Rathinakumar Appuswamy and Brian Taba and Arnon Amir and Myron Flickner and William P. Risk and Rajit Manohar and Dharmendra S. Modha},
	journal={Science},
	year={2014},
	volume={345},
	pages={668 - 673}
}



@misc{ConditionalComputationBengio:2016,
	author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precu},
	booktitle={ICLR'16: },
	title={{Conditional Computation in Neural Networks for faster models}}, year={2016},
	eprint={1511.06297},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	howpublished={\url{https://arxiv.org/pdf/1511.06297}},
	note = {Accessed: 2023-08-30}
	
} 



@article{NatureBuildingBrain:2020,
	author ={Dmitri Strukov and Giacomo Indiveri and Julie Grollier and Stefano Fusi},
	title = {{Building brain-inspired computing}},
	journal = {Nature Communications},
	lab = {Nature Communications},
	volume = {10},
	number = {12},
	pages = {4838},
	year = {2019},
	doi = {10.1038/s41467-019-12521-x}
}
%doi = {10.1063/1.5142089},

@article{BrainInspiredBlocks:2020,
	author = {Jack D. Kendall and Suhas Kumar},
	title = {{The building blocks of a brain-inspired computer}},
	journal = {Appl. Phys. Rev.},
	year = {2020},
	volume = {7},
	pages ={011305},
	doi = {10.1063/1.5129306}
}




@misc{SuspicousBackPropagation:2017,
	author = {axios.com},
	title = {Artificial intelligence pioneer says we need to start over},
	howpublished={\url{https://www.axios.com/2017/12/15/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524}}
}
% geoffrey Hinton

@article{LimitationsNeuromorphicComputing:2017,
	author = {Geoffrey W. Burr and Robert M. Shelby and Abu Sebastian and Sangbum Kim and Seyoung Kim and Severin Sidler and Kumar Virwani and Masatoshi Ishii and Pritish Narayanan and Alessandro Fumarola and Lucas L. Sanches and Irem Boybat and Manuel Le Gallo and Kibong Moon and Jiyoo Woo and Hyunsang Hwang and Yusuf Leblebici},
	title = {Neuromorphic computing using non-volatile memory},
	journal = {Advances in Physics: X},
	volume = {2},
	number = {1},
	pages = {89-124},
	year  = {2017},
	publisher = {Taylor & Francis},
	doi = {10.1080/23746149.2016.1259585},	
	URL = { 
	https://doi.org/10.1080/23746149.2016.1259585
	},
	eprint = { 
	https://doi.org/10.1080/23746149.2016.1259585	
	},
	abstract = {
Dense crossbar arrays of non-volatile memory (NVM) devices represent one possible path for implementing massively-parallel and highly energy-efficient neuromorphic computing systems. We first review recent advances in the application of NVM devices to three computing paradigms: spiking neural networks (SNNs), deep neural networks (DNNs), and ‘Memcomputing’. In SNNs, NVM synaptic connections are updated by a local learning rule such as spike-timing-dependent-plasticity, a computational approach directly inspired by biology. For DNNs, NVM arrays can represent matrices of synaptic weights, implementing the matrix–vector multiplication needed for algorithms such as backpropagation in an analog yet massively-parallel fashion. This approach could provide significant improvements in power and speed compared to GPU-based DNN training, for applications of commercial significance. We then survey recent research in which different types of NVM devices – including phase change memory, conductive-bridging RAM, filamentary and non-filamentary RRAM, and other NVMs – have been proposed, either as a synapse or as a neuron, for use within a neuromorphic computing application. The relevant virtues and limitations of these devices are assessed, in terms of properties such as conductance dynamic range, (non)linearity and (a)symmetry of conductance response, retention, endurance, required switching power, and device variability.
	}	
}

%% Very good discussion of neuromorphic limitations

@InProceedings{AIWinter:2019,
	author="Yasnitsky, Leonid N.",
	editor="Antipova, Tatiana",
	title="Whether Be New ``Winter'' of Artificial Intelligence?",
	booktitle="Integrated Science in Digital Age",
	year="2020",
	publisher="Springer International Publishing",
	address="Cham",
	pages="13--17",
	abstract="The article analyzes the formation and development of artificial intelligence as scientific industry, identifies cycles of leaps and drops of its popularity. It's concluded that the decline in the popularity of artificial intelligence in the near future is inevitable.",
	isbn="978-3-030-22493-6"
}


@article{AIcoreProgressStalled:2020,
	author = {Science},
	title = {{Core progress in AI has stalled in some fields}},
	journal = {Science},
	year = {2020},
	volume = {368},
	pages ={6494/927},
	doi = {10.1126/science.368.6494.927}
}


@BOOK{ComputationalTheoryCognition:2016,
	author = {Gualtiero Piccinini},
	title = {The Computational Theory of Cognition. Chapter 13 in Fundamental Issues of Artificial Intelligence},
	year = {2016},
	publisher = {Springer, Inc. },
	doi= {10.1007/978-3-319-26485-1_13}
}

@article{NeuralSelfReconfiguration:2017,
	title = "Shifting attention to dynamics: Self-reconfiguration of neural networks",
	journal = "Current Opinion in Systems Biology",
	volume = "3",
	pages = "132 - 140",
	year = "2017",
	issn = "2452-3100",
	doi = "https://doi.org/10.1016/j.coisb.2017.04.006",
	url = "http://www.sciencedirect.com/science/article/pii/ S2452310017300732",
	author = "Christoph Kirst and Carl D. Modes and Marcelo O. Magnasco",
}

@article{AIGoogleFlu:2014,
	author = {LAZER, DAVID  and KENNEDY, RYAN  and KING,  GARY  and  VESPIGNANI, ALESSANDRO},
	journal = {Science},
	year = {2014},
	volume = {343},
	issue = {617},
	pages = {1203--1205},
	doi = {10.1126/science.1248506},
	howpublished = {\url{https://gking.harvard.edu/files/gking/files/0314policyforumff.pdf}}
}



@INPROCEEDINGS{ReviewAhmed:2015,
	author={M. R. {Ahmed} and B. K. {Sujatha}},
	booktitle={2015 International Conference on Communications and Signal Processing (ICCSP)}, 
	title={A review on methods, issues and challenges in neuromorphic engineering}, 
	year={2015},
	volume={},
	number={},	
	pages={0899-0903},
}

@misc{AILearningPhysicalIntuition:2016,
	title={Learning Physical Intuition of Block Towers by Example}, 
	author={Adam Lerer and Sam Gross and Rob Fergus},
	year={2016},
	eprint={1603.01312},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@article{NeuroscienceInspiredAI:2017,
author = {Hassabis, D. and Kumaran, D. and Summerfield, C.
and Botvinick, M.},
 title = {Neuroscience-Inspired Artificial Intelligence},
 journal = {Neuron},
 year = {2017},
 volume = {95},
 issue = {2},
 pages = {245-258},
 doi = {10.1016/j.neuron.2017.06.011},
}

@misc{PornographyDetect:2017,
	title = {{Why Robots (Not Human Moderators) Should Be Filtering Out Child Pornography}},
howpublished = {\url{https://www.huffingtonpost.co.uk/richard-pursey/robots-child-porn-detection_b_17578818.html}},
}

@misc{COVID_TuringReport:2020,
	author = {{The Alan Turing Institute}},
	title = {{Reflections on the response of the UK’s
	data science and AI community to the
	COVID-19 pandemic}},
	year = {2020},
howpublished =
{\url{https://www.turing.ac.uk/sites/default/files/2021-06/data-science-and-ai-in-the-age-of-covid_full-report_2.pdf}},
}



@misc{PornographyParlament:2023,
title = {{Offences against Children: Artificial Intelligence}},
howpublished =
{\url{https://www.theyworkforyou.com/wrans/?id=2023-06-28.191568.h}},
}

@article{AI_COVID_NIH:2022,
author = {Comito, C and Pizzuti, C.},
title = {{Artificial intelligence for forecasting and diagnosing COVID-19 pandemic: A focused review}},
journal = {Artif Intell Med},
year = {2022},
volume = {128},
page = {102286},
doi= {10.1016/j.artmed.2022.102286}
}

@article{AI_Security_Pena:2023,
	title={INTELIGENCIA ARTIFICIAL APLICADA A LA SEGURIDAD}, howpublished ={\url{https://revistacugc.es/article/view/5911}},  number={1},
	journal={Logos Guardia Civil, Revista Científica del Centro Universitario de la Guardia Civil}, author={Castro Peña, Juan Luis}, year={2023}, pages={35–60} }

@misc{AIPoliticalCampaign:2023,
	title = {{Here’s What Happened When ChatGPT Wrote to Elected Politicians
	}},
howpublished =
{\url{https://newrepublic.com/article/171459/chatgpt-ai-cornell-experiment-politicians-constituents-emails}},
}

@misc{AIThatCanLearn:2017,
	year = {2017},
	title = {{DARPA Seeking AI That Learns All the Time}},
	howpublished = {\url{https://spectrum.ieee.org/darpa-seeking-ai-that-can-learn-all-the-time}},
	note = {Accessed: 2023-08-30}
}

@misc{DeepLearningCriticalAppraisal:2018,
	author = {Marcus, Gary},
	title = {Deep Learning:
	A Critical Appraisal},
howpublished = {\url{https://arxiv.org/ftp/arxiv/papers/1801/1801.00631.pdf}},
}

@misc{AIDivorceAndMargarin:2017,
title = {Divorce And Margarine},
howpublished = {\url{https://blogs.ams.org/blogonmathblogs/2017/04/10/divorce-and-margarine/}},
}

@misc{AIHowToStopDataCenters:2018,
	author = {Nature},
	title = {{How to stop data centres from gobbling up the world's electricity
	}},
	year = {2018},
	howpublished={\url{https://www.nature.com/articles/d41586-018-06610-y}}
}


@MISC{RevolutionHasnotHappenedYet:2019,
	author = { Jordan, M. I.},
	title = {{Artificial Intelligence—The Revolution Hasn’t Happened Yet}},
	year = {2019},
	issues = {1},
	JOURNAL={Harvard Data Science Review},     
	howpublished = {\url{https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/10}}, 
}
%	howpublished = {\url{https://doi.org/10.1162/99608f92.f06c6e61}}


@article{NeuromorphicSpikesAI:2019,
	author = {Roy, K. and Jaiswal, A. and Panda, P.},
	title = {{Towards spike-based machine intelligence with neuromorphic computing.}},
	journal = {Nature},
	volume = {575},
	year = {2019},
	pages = {607–617},
	doi = "https://doi.org/10.1038/s41586-019-1677-2",
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.}
} 



@MISC{stop-calling-everything-ai:2020,
	author = {Kathy Pretz},
	title = {{Stop Calling Everything AI, Machine-Learning Pioneer Says}},
	year = {2021},
	JOURNAL={IEEE Spectrum},      
	howpublished = {\url{https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says}},
		note = {Accessed: 2023-08-30}
}


@misc{LackOfSleepAI:2020,
	title = {{Lack of Sleep Could Be a Problem for AIs}},
	howpublished = {\url{https://www.scientificamerican.com/article/lack-of-sleep-could-be-a-problem-for-ais/}, \url{https://scitechdaily.com/lack-of-sleep-could-be-a-problem-for-\\ artificial-intelligence}},
		note = {Accessed: 2023-08-30}
}

@article{MemristorArrayEfficiency:2020,
	author = {Yao P. and Wu H. and Gao B. and Tang J and Zhang Q and Zhang W and Yang JJ and Qian H.},
	title = {Fully hardware-implemented memristor convolutional neural network},
	journal = {Nature},
	year = {2020},
	volume = {577},
	issue = {7792},
	pages = {641-646},
	doi = {10.1038/s41586-020-1942-4},
}



@misc{MagyarMIStrategia:2019,
	year = {2019},
	title = {{Magyarország Mesterséges Intelligencia Stratégiája}},
howpublished = {\url{https://digitalisjoletprogram.hu/files/2f/32/2f32f239878a4559b6541e46277d6e88.pdf}},
	note = {Accessed: 2023-08-30}
}


@misc{DeepLimitations:2021,
author = { Cremer, Carla Zoe},
year =  {2021},
title = {Deep limitations? Examining expert disagreement over deep learning},
journal = {Progress in Artificial Intelligence},
abstract = { We investigate expert disagreement over the potential and limitations of deep learning. We conducted 25 expert interviews to reveal the reasons and arguments that underlie the disagreement about the limitations of deep learning, here evaluated in respect to high-level machine intelligence. Experts in our sample named 40 limitations of deep learning. Using interview data, we identify and explore five crucial, unresolved research subjects that underpin this scholarly disagreement: abstraction, generalisation, explanatory models, emergence of planning and intervention. We suggest that such origins of disagreement can be used to form a research road map to guide efforts towards overcoming the limitations of deep learning.},
howpublished = {\url{https://doi.org/10.1007/s13748-021-00239-1}},
doi = {10.1007/s13748-021-00239-1},
}

@misc{AITyrannyOfDistance:2021,
	author = {nextplatform.com},
	title = {{DOE AI EXPERT SAYS NEW HPC ARCHITECTURE IS NEEDED}},
	howpublished = {\url{https://www.nextplatform.com/2021/08/26/doe-ai-expert-says-new-hpc-architecture-is-needed/}},
	year = {2021}
}

% https://www.ncbi.nlm.nih.gov/books/NBK92848/

@article{InterplayAINeuroscience:2021,
title = {{Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research}},
author = {Tom Macpherson and Anne Churchland and Terry Sejnowski and James DiCarlo and Yukiyasu Kamitani and Hidehiko Takahashi and Takatoshi Hikida},
journal = {Neural Networks},
volume = {144},
pages = {603-613},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003683},
keywords = {Artificial intelligence, Neuroscience, Neural imaging, Visual processing, Working memory, Computational psychiatry},
abstract = {Neuroscience and artificial intelligence (AI) share a long history of collaboration. Advances in neuroscience, alongside huge leaps in computer processing power over the last few decades, have given rise to a new generation of in silico neural networks inspired by the architecture of the brain. These AI systems are now capable of many of the advanced perceptual and cognitive abilities of biological systems, including object recognition and decision making. Moreover, AI is now increasingly being employed as a tool for neuroscience research and is transforming our understanding of brain functions. In particular, deep learning has been used to model how convolutional layers and recurrent connections in the brain’s cerebral cortex control important functions, including visual processing, memory, and motor control. Excitingly, the use of neuroscience-inspired AI also holds great promise for understanding how changes in brain networks result in psychopathologies, and could even be utilized in treatment regimes. Here we discuss recent advancements in four areas in which the relationship between neuroscience and AI has led to major advancements in the field; (1) AI models of working memory, (2) AI visual processing, (3) AI analysis of big neuroscience datasets, and (4) computational psychiatry.}
}

@ARTICLE{BrainComputerMetaphore:2022,	
	AUTHOR={Richards, Blake A. and Lillicrap, Timothy P.},   
	TITLE={The Brain-Computer Metaphor Debate Is Useless: A Matter of Semantics},      	
	JOURNAL={Frontiers in Computer Science},      	
	VOLUME={4},           	
	YEAR={2022},      	
	URL={https://www.frontiersin.org/articles/10.3389/fcomp.2022.810358},       	
	DOI={10.3389/fcomp.2022.810358},      	
	ISSN={2624-9898},   	
	ABSTRACT={It is commonly assumed that usage of the word “computer” in the brain sciences reflects a metaphor. However, there is no single definition of the word “computer” in use. In fact, based on the usage of the word “computer” in computer science, a computer is merely some physical machinery that can in theory compute any computable function. According to this definition the brain is literally a computer; there is no metaphor. But, this deviates from how the word “computer” is used in other academic disciplines. According to the definition used outside of computer science, “computers” are human-made devices that engage in sequential processing of inputs to produce outputs. According to this definition, brains are not computers, and arguably, computers serve as a weak metaphor for brains. Thus, we argue that the recurring brain-computer metaphor debate is actually just a semantic disagreement, because brains are either literally computers or clearly not very much like computers at all, depending on one's definitions. We propose that the best path forward is simply to put the debate to rest, and instead, have researchers be clear about which definition they are using in their work. In some circumstances, one can use the definition from computer science and simply ask, what type of computer is the brain? In other circumstances, it is important to use the other definition, and to clarify the ways in which our brains are radically different from the laptops, smartphones, and servers that surround us in modern life.}
} 

@ARTICLE{BrainsAsComputers:2022,	
	AUTHOR={Brette, Romain},   
	TITLE={Brains as Computers: Metaphor, Analogy, Theory or Fact?},      	
	JOURNAL={Frontiers in Ecology and Evolution},      	
	VOLUME={10},           	
	YEAR={2022},      	
	URL={https://www.frontiersin.org/articles/10.3389/fevo.2022.878729},       	
	DOI={10.3389/fevo.2022.878729},      	
	ISSN={2296-701X},   	
	ABSTRACT={Whether electronic, analog or quantum, a computer is a programmable machine. Wilder Penfield held that the brain is literally a computer, because he was a dualist: the mind programs the brain. If this type of dualism is rejected, then identifying the brain to a computer requires defining what a brain “program” might mean and who gets to “program” the brain. If the brain “programs” itself when it learns, then this is a metaphor. If evolution “programs” the brain, then this is a metaphor. Indeed, in the neuroscience literature, the brain-computer is typically not used as an analogy, i.e., as an explicit comparison, but metaphorically, by importing terms from the field of computers into neuroscientific discourse: we assert that brains compute the location of sounds, we wonder how perceptual algorithms are implemented in the brain. Considerable difficulties arise when attempting to give a precise biological description of these terms, which is the sign that we are indeed dealing with a metaphor. Metaphors can be both useful and misleading. The appeal of the brain-computer metaphor is that it promises to bridge physiological and mental domains. But it is misleading because the basis of this promise is that computer terms are themselves imported from the mental domain (calculation, memory, information). In other words, the brain-computer metaphor offers a reductionist view of cognition (all cognition is calculation) rather than a naturalistic theory of cognition, hidden behind a metaphoric blanket.}
}  

@ARTICLE{InterpretabilityOfANNs:2022,
	author = {Kar, K. and Kornblith, S. and  Fedorenko, E.},
	title = {{Interpretability of artificial neural network models in artificial intelligence versus neuroscience}},
	journal = {Nature Machine Intelligence},
	volume= {4}, 
	pages = {1065--1067},
	year =  {2022},
	doi = {10.1038/s42256-022-00592-3},
}


@ARTICLE{BarriersOfDeepLearning:2022,
	author = {Colbrook, Matthew J.  and Antun, Vegard   and Hansen, Anders C.  },
	title = {{The difficulty of computing stable and accurate neural networks: On the barriers of deep learning and Smale’s 18th problem}},
	journal = {PNAS Logo},
	volume= {119},
	number = {12}, 
	pages = {e2107151119},
	year =  {2022},
	doi = {10.1073/pnas.2107151119},
}

@misc{AIjogEU:2022,
	year = {2022},
	title = {{Jogszabály a mesterséges intelligenciáról: a Tanács szorgalmazza az alapvető jogokat tiszteletben tartó, biztonságos mesterséges intelligencia előmozdítását}},
	howpublished = {\url{https://www.consilium.europa.eu/hu/press/press-releases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting \ -safe-ai-that-respects-fundamental-rights/
	}},
			note = {Accessed: 2023-08-30}
	
}

@misc{AItiltasEU:2023,
	year = {2023},
	title = {{JELENTÉS a mesterséges intelligenciára vonatkozó harmonizált szabályok (a mesterséges intelligenciáról szóló jogszabály) megállapításáról és egyes uniós jogalkotási aktusok módosításáról szóló európai parlamenti és tanácsi rendeletre irányuló javaslatról}},
	howpublished = {\url{https://www.europarl.europa.eu/doceo/document/A-9-2023-0188_HU.html}},
			note = {Accessed: 2023-08-30}
	
}

@misc{AIregulatoryFrameworkEU:2023,
	year = {2023},
	title = {{A szabályozási keretre vonatkozó javaslat a mesterséges intelligenciáról}},
	howpublished = {\url{https://digital-strategy.ec.europa.eu/hu/policies/regulatory-framework-ai}},
		note = {Accessed: 2023-08-30}
}



@ARTICLE{SleepPreventsForgettingAI:2022,
	author = {Golden, R and Delanois, JE and Sanda, P and Bazhenov, M},
	title = {Sleep prevents catastrophic forgetting in spiking neural networks by forming a joint synaptic weight representation},
	journal = {PLoS Comput Biol},
	volume= {18},
	number = {12}, 
	pages = {e1010628},
	year =  {2022},
	doi = {10.1371/journal.pcbi.1010628},
}

@misc{AIFailures:2021,
	year = {2021},
	title = {{AI failures}},
	howpublished = {\url{https://spectrum.ieee.org/ai-failures}},
		note = {Accessed: 2023-08-30}
}

@misc{hoffmann2022training,
	title={Training Compute-Optimal Large Language Models}, 
	author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
	year={2022},
	eprint={2203.15556},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{OverhypedChatGPT:2023,
	year = {2023},
	title = {{How Overhyped is ChatGPT in 2023? The Truth about its Hype Explained}},
	howpublished = {\url{https://shape-labs.com/articles/how-overhyped-is-chatGPT}},
			note = {Accessed: 2023-09-30}
	
}

@misc{bowman2023eightthings,
	title={Eight Things to Know about Large Language Models}, 
	author={Samuel R. Bowman},
	year={2023},
	eprint={2304.00612},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	howpublished = {\url{https://shape-labs.com/articles/how-overhyped-is-chatGPT}},
note = {Accessed: 2023-12-30}
}

@misc{IEEEslepAI:2023,
	author = {IEEE},
	year = {2023},
	title = {{Sleep Can Keep AI From Catastrophic Forgetting}},
	howpublished = {\url{https://spectrum.ieee.org/catastrophic-forgetting-deep-learning}},
			note = {Accessed: 2023-08-30}
	
}

@misc{joshi2023flame,
	title={FLAME: A small language model for spreadsheet formulas}, 
	author={Harshit Joshi and Abishai Ebenezer and José Cambronero and Sumit Gulwani and Aditya Kanade and Vu Le and Ivan Radiček and Gust Verbruggen},
	year={2023},
	eprint={2301.13779},
	archivePrefix={arXiv},
	primaryClass={cs.PL}
}

@misc{EUGeneralPurposeAI:2023,
	year = {2023},
	title = {{General-purpose artificial intelligence}},
	howpublished = {\url{https://epthinktank.eu/2023/03/31/general-purpose-artificial-intelligence/}},
			note = {Accessed: 2023-08-30}
	
}

@misc{OrganoidIntelligence:2023,
	author = {Smirnova, L.  et al},
	title = {{Organoid intelligence (OI): the new frontier in biocomputing and intelligence-in-a-dish}},
	journal = {Frontiers in Science},
	year =  {2023},
	doi = {10.3389/fsci.2023.1017235},
	howpublished = {\url{https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2023.1017235/full}},
}

@misc{BrainOrganoidsReal:2023,
	author = {Quirion, R},
	title = {{Brain organoids: are they for real?}},
	journal = {Frontiers in Science},
	year =  {2023},
	doi = {10.3389/fsci.2023.1148127},
	howpublished = {\url{https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2023.1148127/full}},
}

@misc{AIisOver:2023,
	year = {2023},
	title = {{AI is over. Now it’s OI. Get your head in the game. Literally.}},
	howpublished = {\url{https://finance.yahoo.com/news/ai-over-now-oi-head-124512588.html}},
		note = {Accessed: 2023-08-30}
}

@misc{LargeLanguageModels:2021,
	year = {2021},
	title = {{How Large Language Models Will Transform Science, Society, and AI}},
	howpublished = {\url{https://hai.stanford.edu/news/how-large-\ language-models-will-transform-science-society-and-ai}},
		note = {Accessed: 2023-08-30}
}

@misc{AIRisksNIST:2022,
	year = {2022},
	author = {National Bureau of Standards, US},
	title = {AI Risk Management Framework: Initial Draft},
howpublished = {\url{https://www.nist.gov/system/files/documents/2022/03/17/AI-RMF-1stdraft.pdf}},
		note = {Accessed: 2023-08-30}
}

@misc{AIWeather:2023,
	title = {{The outlook for AI weather prediction}},
	doi = {10.1038/d41586-023-02084-9},
	authors = {Ebert-Uphoff, Imme and Hilburn, Kyle},
	journal = {Nature},
	howpublished = {\url{https://www.nature.com/articles/d41586-023-02084-9}},
	abstract = {Two models demonstrate the enormous potential that artificial intelligence holds for weather prediction. But the risks involved demand that meteorologists learn to design, evaluate and interpret such systems.},
		note = {Accessed: 2023-08-30},
}

@misc{AIAtomicBomb:2023,
year = {2023},
title = {{Oppenheimer Offers Us a Fresh Warning of AI’s Danger}},
howpublished = {\url{https://www.scientificamerican.com/article/oppenheimer-offers-us-a-fresh-warning-of-ais-danger/}},
		note = {Accessed: 2023-08-30}

}

@ARTICLE{DoubleEdgedSwordAI:2023,
	author = { Shen, Yiqiu and Heacock, Laura  and Elias, Jonathan and   Hentel, Keith D. and  Reig,  Beatriu and  Shih,George and Moy,  Linda},
	title = {{ChatGPT and Other Large Language Models Are Double-edged Swords}},
	journal = {Radiology},
	year =  {2023},
	doi = {10.1148/radiol.230163},
	howpublished = {\url{https://doi.org/10.1148/radiol.230163}},
}

@misc{SurveyLLMs:2023,
	title={A Survey on Evaluation of Large Language Models}, 
	author={Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Linyi Yang and Kaijie Zhu and Hao Chen and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qiang Yang and Xing Xie},
	year={2023},
	eprint={2307.03109},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	howpublished = {\url{https://arxiv.org/abs/2307.03109}},
				note = {Accessed: 2023-08-30}
	
}

@ARTICLE{EU_HBP_WrapUp:2023,
	title = {Europe spent €600 million to recreate the human brain in a computer. How did it go?},
	journal = {Nature},
	volume = {620},
	pages = {718-720},
	year = {2023},
	doi = {10.1038/d41586-023-02600-x},
}

@misc{AI_Sparks_GPT4:2023,
	title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
	author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
	year={2023},
	howpublished={\url{https://arxiv.org/pdf/2303.12712.pdf}}
}

@misc{GPTMedicalExamination:2023,
	title = {{Evaluation of the performance of GPT-3.5 and GPT-4 on the Medical Final
	Examination}},
	author = {Rosol, M. and JGąsior, J. S. and
	Laba J. and Korzeniewski, K. and Mlynczak, M.},
	howpublished={\url{https://www.medrxiv.org/content/10.1101/2023.06.04.23290939v1.full.pdf}}
}

@misc{fedus2022review,
	title={A Review of Sparse Expert Models in Deep Learning}, 
	author={William Fedus and Jeff Dean and Barret Zoph},
	year={2022},
	eprint={2209.01667},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{openai2023gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{SambanovaGPUTackles:2023,
	author = {nextplatform.com},
	title = {{SAMBANOVA TACKLES GENERATIVE AI WITH NEW CHIP AND NEW APPROACH}},
howpublished={\url{https://www.nextplatform.com/2023/09/20/sambanova-tackles-generative-ai-with-new-chip-and-new-approach/}}
}

@misc{AIdesignsAccelerators:2023,
	author = {nextplatform.com},
	title = {{WHAT HAPPENS WHEN LLMS DESIGN AI ACCELERATORS?}},
	howpublished={\url{https://www.nextplatform.com/2023/09/25/what-happens-when-llms-design-ai-accelerators/}}
}

@misc{WhoIsWorking:2023,
	title = {{Students or Bots:
	Who's doing the work in your course?
	}},
	howpublished={\url{https://players.brightcove.net/624142947001/default_default/index.html?videoId=6322300588112}}
}

@misc{AIChatGPTLesson:2023,
	title = {{Will ChatGPT give us a lesson in education?
	}},
	year={2023},
	howpublished={\url{https://www.nature.com/articles/d42473-023-00083-y}}
}

@misc{ChatGPTDetector:2023,
	author = {nature.com},
	title = {{‘ChatGPT detector’ catches AI-generated papers with unprecedented accuracy}},
	year = {2023},
	howpublished={\url{https://www.nature.com/articles/d41586-023-03479-4}},
}


@misc{AIharmadikTel:2023,
	title = {Közeleg az újabb AI tél (AI winter)?},
	year={2023},
	howpublished={\url{https://elemzeskozpont.hu/kozeleg-az-ujabb-ai-tel-ai-winter}}
}

@misc{AIThirdWinter:2022,
	author = {cortical.io},
	title = {{Third AI Winter ahead? Why OpenAI, Google et Co are heading towards a dead-end}},
	year={2022},
	howpublished={\url{https://www.cortical.io/blog/third-ai-winter-ahead-why-openai-google-co-are-heading-towards-a-dead-end/}},
}


@misc{ChatGPTScience:2023,
	title = {{ChatGPT: five priorities for research.}},
	subtitle = {{Conversational AI is a game-changer for science.}},
	year = {2023},
	howpublished = {\url{https://www.nature.com/articles/d41586-023-00288-7}},
	note = {Accessed: 2023-09-10}
}

@misc{ChatGPTTuring:2023,
	title = {{ChatGPT broke the Turing test }},
	subtitle = {{THE EASY
	INTELLIGENCE
	TESTS THAT AI
	CHATBOTS FAIL}},
	year = {2023},
	howpublished = {\url{https://www.nature.com/articles/d41586-023-02361-7.pdf}},
	note = {Accessed: 2023-09-10}}



@article{AIBiggerBetter:2023,
	author = {Nature},
	title = {{In AI, is bigger always better?}},
	journal = {Nature},
	volume = {615},
	year = {2023},
	page = {202},
	howpublished = {\url{https://www.nature.com/articles/d41586-023-00641-w}},
	note = {Accessed: 2023-09-10},
}

@misc{ComputationalIntelligence:2023,
	title = {{What is Computational Intelligence?}},
	year = {2023},
	howpublished = {\url{https://cis.ieee.org/about/what-is-ci}},
	note = {Accessed: 2023-09-10}}

@article{AlienIntelligence:2023,
	author = {Frank, M.C.},
	title = {{Baby steps in evaluating the capacities of large language models}},
	journal = {Nat Rev Psychol},
	volume = {2},
	pages = {451–452},
	year = {2023},
	howpublished = {\url{https://doi.org/10.1038/s44159-023-00211-x}},
	note = {Accessed: 2023-09-10}}



@misc{AIBlackBox:2023,
	author = {Nature},
	title = {{ChatGPT is a black box: how AI research can break it open}},
	journal = {Nature},
	volume = {619},
	page = {671},
	howpublished = {\url{https://www.nature.com/articles/d41586-023-02366-2}},
	note = {Accessed: 2023-09-10},
}

@misc{WolframChatGPT:2023,
	author = {Wolfram, Stephen},
	title = {{What Is ChatGPT Doing … and Why Does It Work?}},
	howpublished = {\url{https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/}},
	note = {Accessed: 2023-09-10},
}

