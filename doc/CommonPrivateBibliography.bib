

%	abstract=
%	{
	%	https://books.google.hu/books?hl=en&lr=&id=grz-CwAAQBAJ&oi=fnd&pg=PT13&ots=fCadzG5Psv&sig=sKh_UGATcrAVaHNDewH6VMMjyG0&redir_esc=y#v=onepage&q&f=false	
	%	}

@CONFERENCE{ManyCoreAwareVegh2014,
	author = {V\'egh, J. and Bagoly, Zs. and  Kics\'ak, \'A. and Moln\'ar, P.},
	title = {{A Multicore-aware von Neumann Programming Model}},
	booktitle = { Proceedings of the 9th International Conference on Software Engineering and Applications (ICSOFT-EA-2014)},
	year = {2014},
	pages = {150-155},
}
%   doi = {10.5220/0005097001500155}

@CONFERENCE{Vegh:2014:ICSOFTsemaphore,
	author = {V\'egh, J. and Bagoly, Zs. and  Kics\'ak, \'A. and Moln\'ar, P.},
	title = {{An alternative implementation for accelerating some functions of operating system}},
	booktitle = { Proceedings of the 9th International Conference on Software Engineering and Applications (ICSOFT-EA-2014)},
	year = {2014},
	pages = {494-499},
}
%   doi = {10.5220/0005104704940499}


@ARTICLE{VeghDynamicParallelism:2016,
	author = {{V{\'e}gh}, J.},
	title = {{A new kind of parallelism and its programming in the Explicitly Many-Processor Approach}},
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1608.07155},
	primaryClass = "cs.DC",
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, 68N15, 68N19, C.1.3, D.2.11, F.1.2},
	year = 2016,
	month = aug,
	url = {http://adsabs.harvard.edu/abs/2016arXiv160807155V},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{VeghEMPAthY86:2016,
	author = {{V{\'e}gh}, J.},
	title = {{EMPAthY86:   
	A cycle accurate simulator for Explicitly Many-Processor Approach (EMPA) computer.}}, 
	doi = {10.5281/zenodo.58063)},
	url = {{https://github.com/jvegh/EMPAthY86}},
	year = {2016},
}


@article{VeghEMPA:2016,
	author = {{V{\'e}gh}, J.},
	title = {{A configurable accelerator for manycores: the Explicitly Many-Processor Approach}},
	journal = {ArXiv e-prints},
	eprint = {1607.01643},
	year = 2016,
	howpublished = {\url{http://adsabs.harvard.edu/abs/2016arXiv160701643V}},
}


@article{VeghAlphaEff:2016,
	author    = {J{\'{a}}nos V{\'{e}}gh and
	P{\'{e}}ter Moln{\'{a}}r and
	J{\'{o}}zsef V{\'{a}}s{\'{a}}rhelyi},
	title     = {{A figure of merit for describing the performance of scaling of parallelization}},
	journal   = {CoRR},
	volume    = {abs/1606.02686},
	year      = {2016},
	url       = {http://arxiv.org/abs/1606.02686},
	timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/VeghMV16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}   


@article{Vegh:StatisticalConsiderations:2017,
	author    = {J{\'{a}}nos V{\'{e}}gh},
	title     = {Statistical considerations on limitations of supercomputers},
	journal   = {CoRR},
	volume    = {abs/1710.08951},
	year      = {2017},
	url       = {http://arxiv.org/abs/1710.08951},
	archivePrefix = {arXiv},
	eprint    = {1710.08951},
	timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-08951},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Vegh:2017:AlphaEff,
	author = {J\'anos V\'egh and  P\'eter Moln\'ar},
	title = {{How to measure perfectness of parallelization in hardware/software systems}},
	booktitle = {18th Internat. Carpathian Control Conf.~ICCC 28-31 May 2017; Sinaia, Romania},
	year = {2017},
	pages = {394--399}
}
%	date = {28-31 May, 2017},

@inproceedings{Molnar:2017:Meas,
	author = {P\'eter Moln\'ar and J\'anos V\'egh},
	title = {{Measuring Performance of Processor Instructions
	and Operating System Services in
	Soft Processor Based Systems}},
	booktitle = {18th Internat. Carpathian Control Conf.~ICCC},
	year = {2017},
	pages = {381--387}
}
%	date = {28-31 May, 2017},


@inbook{RenewingComputingVegh:2018,
	author = {{J.~V\'egh}},
	series = { Advances in Parallel Computing},
	title = {{Renewing computing paradigms for more
	efficient parallelization of single-threads}}, 
	publisher = { IOS Press},
	volume = {29},
	chapter = {13},
	pages = {305--330},
	year = {2018},
	url = {https://arxiv.org/abs/1803.04784}
}
%	series = {Data Intensive Computing Applications for Big Data},
%	editor = {Mamta Mittal and Valentina E. Balas and D. Jude %Hemanth and Raghvendra Kumar},


%issn = "0167-8191",
%doi = "https://doi.org/10.1016/j.parco.2018.03.001",

@article{IntroducingEMPA2018,
	title = {{Introducing the Explicitly Many-Processor Approach}},
	journal = "Parallel Computing ",
	volume = "75",
	number = "",
	pages = "28 - 40",
	year = "2018",
	note = "",
	author = "J. V\'egh",
	keywords = "Many-core,
	Single thread,Performance,Neumann abstractions,
	Hybrid architecture,Explicitly many-processor approach,Computing principle",
	abstract = "Abstract The deeper reasons of the present stalling in computing is scrutinized, and to enhance the single-processor performance, a new approach explicitly considering the presence of several computing units is introduced, as opposed to the presently exclusively used, 70-years old single-processor approach. The appearance of many-core processors, having many processing units in close vicinity to each other, requires to re-think some principles of computing. The goal of the approach is to enhance the single-processor performance using cooperating cores, rather than to introduce a new method for parallelization. Technically, it introduces a new control layer above the cores, a new intermediate execution unit called quasi-thread, a modified compiling method and object code for transferring parallelization information from the development system to the processor, and an on-demand self-organizing processor architecture. The resulting processors have more effective and more “green” architecture, considerably increased single-thread performance, allow for more deterministic real-time behaviour, new scheduling principles for multitasking, less operating system overhead, etc. Surprisingly, the resulting computing stack is upward compatible with the presently existing one. "
}
% url = "https://www.sciencedirect.com/science/article/pii/S0167819118300577",


@article{VeghSupercomp:2018,
	author    = {J\'{a}nos V\'{e}gh},
	title     = {{How Amdahl's law restricts supercomputer applications and building
	ever bigger supercomputers}},
	journal   = {CoRR},
	volume    = {abs/1708.01462},
	year      = {2018},
	url       = {http://arxiv.org/abs/1708.01462},
} 
%	archivePrefix = {arXiv},
%eprint    = {1708.01462},
%timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
%biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1708-01462},
%bibsource = {dblp computer science bibliography, http://dblp.org}



@article{Vegh:StatisticalConsiderations:2017,
	author    = {J{\'{a}}nos V{\'{e}}gh},
	title     = {Statistical considerations on limitations of supercomputers},
	journal   = {CoRR},
	volume    = {abs/1710.08951},
	year      = {2017},
	archivePrefix = {arXiv},
	eprint    = {1710.08951},
	timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-08951},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
%  url       = {http://arxiv.org/abs/1710.08951},


@article{Vegh:ExascaleComputing:2018,
	author = {J.~{V{\'e}gh}},
	title = "{Limitations of performance of Exascale Applications and supercomputers they are running on}",
	journal = {ArXiv e-prints; submitted to special issue of IEEE Journal of Parallel and Distributed Computing },
	archivePrefix = "arXiv",
	eprint = {1808.05338},
	primaryClass = "cs.DC",
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	year = 2018,
	month = aug,
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180805338V},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Vegh:MulticoreMended:2019,
	author = {J. V\'egh},
	title = {{Can Broken Multicore Hardware be Mended?}},
	journal = {Global Journal of Research In Engineering},	
	year = {2019},
	volume = {18},
	issue = {I},
	pages ={15--21},
	keywords = {},
	abstract = {The multi- anThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	d many-core (MC) era we have reached was triggered after the beginning of the century by the stalling of single-processor performance. Technology allowed more transistors to be placed on a die, but they could not reasonably be utilized to increase single-processor performance. Predictions about the number of cores has only partly been fulfilled: today’s processors have dozens rather than the predicted hundreds of cores (although the Chinese supercomputer [3] announced in the middle of 2016 comprises 260 cores on a die). Despite this, the big players are optimistic. They expect that Moore-law persists, though based on presently unknown technologies. The effect of the stalled clock frequency is mitigated, and it is even predicted [6] that ”Now that there are multicore processors, there is no reason why computers shouldn’t begin to work faster, whether due to higher frequency or because of parallel task execution. And with parallel task execution it provides even greater functionality and flexibility! ”},
	url = {https://engineeringresearch.org/index.php/GJRE/article/view/1778}
}



@article{VeghParallelizationSave:2019,
	author = {J\'anos V\'egh and J\'ozsef V\'as\'arhelyi and D\'aniel Dr\'otos},
	journal = {Advances in Science, Technology and Engineering Systems Journal},
	number = {1},
	pages = {141--158},
	title = {{Can parallelization save the ( computing ) world ?}},
	volume = {4},
	year = {2019},
	doi = {10.25046/aj040114},
}
%	url = {https://www.researchgate.net/publication/331435142_Can_parallelization_save_the_computing_world},


@inproceedings{VeghPerformanceWall:2019,
	author = {{J. V\'egh, J. V\'as\'arhelyi and D. Dr\'otos}},
	title = {{The performance wall of large parallel computing systems}}, 
	booktitle = {{Lecture Notes in Networks and Systems 68}},
	publisher = {Springer},
	pages = {224--237},
	year = {2019},
	url = {https://link.springer.com/chapter/10.1007\%2F978-3-030-12450-2\_21}
}
% ISBN 978-3-030-12449-6
%     DOI: 10.1007/978-3-030-12450-2_21
% 	editor = {I. Kabashkin and I. Yatskiv and O. Prentkovskis},

@article{VeghBrainAmdahl:2019,
	author = {J. {V\'egh}},
	title = {{How Amdahl's Law limits performance of large artificial neural networks}},
	journal = {Brain Informatics},	
	year = {2019},
	volume = {6},
	issue = {4},
	pages ={1--11},
	doi={10.1186/s40708-019-0097-2},
	keywords = {Amdahl's Law; neural networks;
	performance limitation; clock-driven electronic circuit; supercomputing;
	parallelization},
	abstract = {With both knowing more and more details about how neurons and complex neural networks work
	and having serious demand for making performable huge artificial networks,
	more and more efforts are devoted to build both hardware and/or software simulators 
	and supercomputers targeting artificial intelligence applications,
	demanding an exponentially increasing amount of computing capacity. However, the inherently
	parallel operation of the neural networks is mostly simulated deploying inherently
	sequential (or in the best case: sequential-parallel) computing elements. The paper
	shows that neural network simulators (both software and hardware ones), akin to all
	other sequential-parallel computing systems, have computing performance limitation
	due to deploying clock-driven electronic circuits, the 70-years old computing paradigm
	and Amdahl's Law about parallelized computing systems. The findings explain the
	limitations/saturation experienced in former studies.
	},
}
%	url = {https://braininformatics.springeropen.com/ articles/10.1186/s40708-019-0097-2},

% https://braininformatics.springeropen.com/articles/10.1186/s40708-019-0097-2
%	url = %{https://engineeringresearch.org/index.php/GJRE/article/view/1778}
% doi = {https://doi.org/10.1186/s40708-019-0097-2}
%{\small (Why the functionality of full-scale brain simulation on processor-based simulators is limited)}


@article{VeghRoofline:2019,
	author = {J. V\'egh},
	title = {{The performance wall of parallelized sequential computing:
	the roofline of supercomputer performance gain}},
	journal = {Parallel Computing},
	volume = {in review},
	year = {2019},
	pages = {http://arxiv.org/abs/1908.02280}
}
%	journal = {},
%archivePrefix = "arXiv",


@book{VeghPerformanceBook:2019,
	author = {J\'anos V\'egh},
	title = {{The performance wall of the parallelized sequential computing -- Can parallelization save the (computing) world?}},
	publisher = {Lambert Academic Publishing},
	isbn = {978-620-0-08051-6},
	edition = {1},
	year = {2019}
}



@INPROCEEDINGS{VeghModernParadigm:2019,
	author={J. {V\'egh} and A. {Tisan}},
	booktitle={{%2019 International Conference on Computational Science and Computational Intelligence 
	CSCI
	The 25th Int'l Conf on Parallel and Distributed Processing Techniques and Applications}},
	title = {{The need for modern computing paradigm: Science applied to computing}}, 	
	publisher = {IEEE},
	year={2019}, volume={}, number={}, pages={1523-1532},
	url       = {http://arxiv.org/abs/1908.02651},
	doi = {10.1109/CSCI49370.2019.00283}
}
%https://doi.org/10.1109/CSCI49370.2019.00283


@article{VeghHowMany:2020,
	year = {2020},
        volume = {76},
        number = {12},
        pages = {9430-9455, regularly updated at https://arxiv.org/abs/2001.01266},
	publisher = {Springer Science and Business Media {LLC}},
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {Finally, how many efficiencies the supercomputers have?},
	journal = {The Journal of Supercomputing},
}
%	url = {http://link.springer.com/article/10.1007/s11227-020-03210-4},





@inproceedings{VeghTemporal:2020,
	author="V{\'e}gh, J{\'a}nos",
	editor="Arabnia, Hamid R.
	and Deligiannidis, Leonidas
	and Tinetti, Fernando G.
	and Tran, Quoc-Nam",
	title="Introducing Temporal Behavior to Computing Science",
	booktitle="Advances in Software Engineering, Education, and e-Learning",
	year="2021",
	publisher="Springer International Publishing",
	pages="471--491",
	abstract="The abstraction introduced by von Neumann correctly reflected the state of the art 70 years ago. Although it omitted data transmission time between components of the computer, it served as an excellent base for classic computing for decades. Modern computer components and architectures, however, require to consider their temporal behavior: data transmission time in contemporary systems may be higher than their processing time. Using the classic paradigm leaves a growing number of issues unexplained, from enormously high power consumption to days-long training of artificial neural networks to failures of some cutting-edge supercomputer projects. The paper introduces the up to now missing temporal behavior (a temporal logic) into computing, while keeps the solid computing science base. The careful analysis discovers that with considering the temporal behavior of components and architectural principles, the mystic issues have a trivial explanation. Some classic design principles must be revised, and the temporal logic enables us to design a more powerful and efficient computing.",
	isbn="978-3-030-70873-3",
		howpublished = {\url{https://link.springer.com/chapter/10.1007/978-3-030-70873-3_33} (Accessed on Oct 24, 2021)},
	
}


@inproceedings{VeghSPAEMPA:2020,
	author="V{\'e}gh, J{\'a}nos",
	editor="Arabnia, Hamid R.
	and Deligiannidis, Leonidas
	and Tinetti, Fernando G.
	and Tran, Quoc-Nam",
	title={{How to Extend Single-Processor Approach to Explicitly Many-Processor Approach}},
	booktitle="Advances in Software Engineering, Education, and e-Learning",
	year="2021",
	publisher="Springer International Publishing",
	pages="435--458",
	abstract="The presently used computing paradigm was invented for processing a small amount of data on a single segregated processor, and it cannot meet the challenges set by the present-day computing demands. The recent technical implementations are based on that paradigm and became bottlenecks of computing performance. The paper proposes a new computing paradigm (extending the old one to use several processors explicitly) and discusses some ideas of its possible implementation. Some advantages of the implemented approach, illustrated with the results of a loosely-timed simulator, are presented.",
	isbn="978-3-030-70873-3",
	howpublished      = {\url{https://link.springer.com/chapter/10.1007/978-3-030-70873-3_31}},
}
%	url       = {https://arxiv.org/abs/2006.00532},


@article{VeghDoWeKnow:2020,
		author={V\'egh, J\'anos and Berki, \'Ad\'am J.},
		booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
		title={Do we know the operating principles of our computers better than those of our brain?}, 
		year={2020},
	volume={},
	number={},	
	pages={668-674},	
	doi={10.1109/CSCI51800.2020.00120},
	url = {https://https://american-cse.org/sites/csci2020proc/pdfs/CSCI2020-6SccvdzjqC7bKupZxFmCoA/762400a668/762400a668.pdf,https://arxiv.org/abs/2005.05061}
}



@misc{VeghBiologySpatioTemporal:2020,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am J. Berki},
	title = {{On the spatiotemporal behavior
	in biology-mimicking computing systems}},
	journal = {Researchgate},
	year = {2020},
	howpublished = {\url{https://www.researchgate.net/publication/344325571\_On\_the\_Spatiotemporal\_Behavior\_in\_Biology-Mimicking\_Computing\_Systems} (Accessed on Oct 24, 2021)} 
}
%	url = {https://arxiv.org/abs/2009.08841}
%	pages = {in review},



@INPROCEEDINGS{VeghMissingSecondDraft:2020,
	author={J\'anos V\'egh},
	booktitle={{Proceedings of the 2020 International Conference on Computational
	Science and Computational Intelligence (CSCI'20: December 16-18, 2020,
	Las Vegas, Nevada, USA}},
	title = {{von Neumann's missing "Second Draft": what it should contain}}, 	
	publisher = {IEEE Computer Society},
	year={2020},  
	pages = {1260-1264},
	doi = {10.1109/CSCI51800.2020.00235},
	isbn = {ISBN-13: 978-1-7281-7624-6},
}
%	pages={paper CSCI2019},
	url       = {https://arxiv.org/abs/2011.04727,https://american-cse.org/sites/csci2020proc/pdfs/CSCI2020-6SccvdzjqC7bKupZxFmCoA/762400b260/762400b260.pdf
},


@article{VeghIntroducingTemporalBehaviorScience:2020,
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {{Why  do  we  need  to  Introduce  Temporal  Behavior  in  both  Modern  Science  and  Modern  Computing}},
	subtitle = {{With  an  Outlook  to Researching Modern Effects/Materials and Technologies}},
	journal = {Global Journal of Computer Science and Technology: Hardware \& Computation},
	volume = {20/1},
	year = {2020},
	pages = {13-29},
	url = {https://doi.org/10.34257/GJCSTAVOL20IS1PG13}
}
%
%https://globaljournals.org/GJCST_Volume20/2-Why-do-we-need-to-Introduce-Temporal.pdf

%https://arxiv.org/pdf/2011.08455}


@INPROCEEDINGS{VeghComputingModel:2021,
	author={J\'anos V\'egh},
	booktitle={{2021 international conference on computational science and computational intelligence; foundations of computer science FCS
	}},
	volume={21},
	title = {{A model for storing and processing information in technological and biological computing systems}}, 	
	publisher = {IEEE Las Vegas, USA; July 26-29},
    pages = {FCS4404},
	year={2021},  
}
%The 2022 World Congress in
Computer Science, Computer Engineering and Applied Computing 

@INPROCEEDINGS{VeghInfoProcessing:2021,
	author={J\'anos V\'egh and Ádám József Berki},
	booktitle={{The 2021 international conference on computational science and computational intelligence; foundations of computer science FCS}},
	title = {{Storing and Processing Information in Technological and Biological Computing Systems}}, 	
	volume={21},
	publisher = {IEEE},
	pages = {FCS4378},
	year={2021},  
}
	
@article{WhyMachineLearningDifferent:2021,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am-J\'ozsef Berki},
	title = {{Why learning and machine learning are different}},
	journal = {{Advances in Artificial Intelligence and Machine Learning}},
	volume = {1},
	issue = {2},
	pages = {131-148},
	doi = {10.54364/AAIML.2021.1109},
	year = {2021},
	howpublished= {\url{https://www.oajaiml.com/uploads/archivepdf/77931109.pdf}}
}
%Adv. Artif. Intell. Mach. Learn.˚

@article{VeghScalingANN:2021,
	author = {J. V\'egh},
	title = {{Which scaling rule applies
	to Artificial Neural Networks}},
	journal = {Neural Computing and Applications},
	year = {2021},
	doi = {10.1007/s00521-021-06456-y},
	url = {http://link.springer.com/article/10.1007/s00521-021-06456-y}
}


@Article{VeghRevisingClassicComputing:2021,
	AUTHOR = {V\'egh, J\'anos},
	TITLE = {Revising the Classic Computing Paradigm and Its Technological Implementations},
	JOURNAL = {Informatics},
	VOLUME = {8},
	YEAR = {2021}, 
	NUMBER = {4},
	ARTICLE-NUMBER = {71},
	ISSN = {2227-9709},
	ABSTRACT = {Today’s computing is based on the classic paradigm proposed by John von Neumann, three-quarters of a century ago. That paradigm, however, was justified for (the timing relations of) vacuum tubes only. The technological development invalidated the classic paradigm (but not the model!). It led to catastrophic performance losses in computing systems, from the operating gate level to large networks, including the neuromorphic ones. The model is perfect, but the paradigm is applied outside of its range of validity. The classic paradigm is completed here by providing the “procedure” missing from the “First Draft” that enables computing science to work with cases where the transfer time is not negligible apart from the processing time. The paper reviews whether we can describe the implemented computing processes by using the accurate interpretation of the computing model, and whether we can explain the issues experienced in different fields of today’s computing by omitting the wrong omissions. Furthermore, it discusses some of the consequences of improper technological implementations, from shared media to parallelized operation, suggesting ideas on how computing performance could be improved to meet the growing societal demands.},
	DOI = {10.3390/informatics8040071}
}
%	URL = {https://www.mdpi.com/2227-9709/8/4/71},


@INPROCEEDINGS{VeghGeneralShannonCSCE:2022,
	author={J\'anos V\'egh and \'Ad\'am J\'ozsef Berki},
	booktitle={{ International Conference of Bioinformatics \&
	Computational Biology (BIOCOMP 2022), Held at CSCE'22; July 25-28 2022
	}},
	volume={22},
	title = {{How information theory shall be generalized\\for neural communication}}, 	
	publisher = {CSCREA Press, Las Vegas, USA},
	pages = {BIC3423},
	year={2022},  
}

@article{VeghNeuralShannon:2022,
	author={V{\'e}gh, J{\'a}nos and Berki, {\'A}d{\'a}m J{\'o}zsef},
	JOURNAL = {Entropy},
	VOLUME = {24},
	NUMBER = {8},
	ARTICLE-NUMBER = {1086},
	title = {{Towards generalizing the information theory for neural communication}}, 	
	pages = {1086--},
	year={2022},  
	doi = {10.3390/e24081086},
}

@article{RoleOfInformationTransferSpeed:2022,
	title={{On the Role of Speed in Technological and Biological Information Transfer for Computations}},
author={V{\'e}gh, J{\'a}nos and Berki, {\'A}d{\'a}m J{\'o}zsef},
journal={Acta Biotheoretica},
volume={70},
number={4},
pages={26},
year={2022},
publisher={Springer},
	doi = {10.1007/s10441-022-09450-6},
}


@article{VeghChannelCapacity:2023,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am J\'ozsef Berki},
	title = {{Revisiting neural information, computing and linking capacity}},
	journal = {Mathematical Biology and Engineering},
	volume = {20},
	issue = {7},
	year = {2023},
	pages = {12380-12403},
	doi = {10.3934/mbe.2023551},
}

@INPROCEEDINGS{VeghAI_ASPAI_Tenerife:2023,
	author={{J\'anos V\'egh}},
	booktitle={{5th International Conference on Advances in Signal Processing and Artificial Intelligence (ASPAI' 2023),
	7-9 June 2023, Tenerife (Canary Islands), Spain	}},
	title = {{How Science and Technology Limit the Performance of AI Networks}}, 	
	publisher = {International Frequency Sensor Association (IFSA) Publishing},
	pages = {90-92},
	year={2023},  
}


@book{BuzsakiVeghSpaceTime:2023,
	title = {Space, time and memory},
	author = {Buzs\'aki, Gy. and V\'egh, J.},
	publisher = {Oxford University Press, in print},
	isbn = {Not-yet-known, under edition},
	edition = {1},
	year = {2023}
}	

@INPROCEEDINGS{VeghNeumannDeepLearning:2023,
	title = {{Why does von Neumann obstruct deep learning?}},
	author={{J\'anos V\'egh}},
	booktitle = {{2023 IEEE International Symposium on Computational Intelligence and Informatics (CINTI)}},
	year = {2023},
	pages = {},
}

















%https://www.fz-juelich.de/SharedDocs/Downloads/INM/INM-1/EN/Abstract_INM-1_Seminar_20210913.pdf	
%https://europepmc.org/article/ppr/ppr297778
