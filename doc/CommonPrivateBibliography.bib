

%	abstract=
%	{
	%	https://books.google.hu/books?hl=en&lr=&id=grz-CwAAQBAJ&oi=fnd&pg=PT13&ots=fCadzG5Psv&sig=sKh_UGATcrAVaHNDewH6VMMjyG0&redir_esc=y#v=onepage&q&f=false	
	%	}

@CONFERENCE{ManyCoreAwareVegh2014,
	author = {V\'egh, J. and Bagoly, Zs. and  Kics\'ak, \'A. and Moln\'ar, P.},
	title = {{A Multicore-aware von Neumann Programming Model}},
	booktitle = { Proceedings of the 9th International Conference on Software Engineering and Applications (ICSOFT-EA-2014)},
	year = {2014},
	pages = {150-155},
}
%   doi = {10.5220/0005097001500155}

@CONFERENCE{Vegh:2014:ICSOFTsemaphore,
	author = {V\'egh, J. and Bagoly, Zs. and  Kics\'ak, \'A. and Moln\'ar, P.},
	title = {{An alternative implementation for accelerating some functions of operating system}},
	booktitle = { Proceedings of the 9th International Conference on Software Engineering and Applications (ICSOFT-EA-2014)},
	year = {2014},
	pages = {494-499},
}
%   doi = {10.5220/0005104704940499}


@ARTICLE{VeghDynamicParallelism:2016,
	author = {{V{\'e}gh}, J.},
	title = {{A new kind of parallelism and its programming in the Explicitly Many-Processor Approach}},
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1608.07155},
	primaryClass = "cs.DC",
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, 68N15, 68N19, C.1.3, D.2.11, F.1.2},
	year = 2016,
	month = aug,
	url = {http://adsabs.harvard.edu/abs/2016arXiv160807155V},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{VeghEMPAthY86:2016,
	author = {{V{\'e}gh}, J.},
	title = {{EMPAthY86:   
	A cycle accurate simulator for Explicitly Many-Processor Approach (EMPA) computer.}}, 
	doi = {10.5281/zenodo.58063)},
	url = {{https://github.com/jvegh/EMPAthY86}},
	year = {2016},
	month = {jul},
}


@article{VeghEMPA:2016,
	author = {{V{\'e}gh}, J.},
	title = {{A configurable accelerator for manycores: the Explicitly Many-Processor Approach}},
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1607.01643},
	primaryClass = "cs.DC",
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	year = 2016,
	month = jul,
	url = {http://adsabs.harvard.edu/abs/2016arXiv160701643V},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{VeghAlphaEff:2016,
	author    = {J{\'{a}}nos V{\'{e}}gh and
	P{\'{e}}ter Moln{\'{a}}r and
	J{\'{o}}zsef V{\'{a}}s{\'{a}}rhelyi},
	title     = {{A figure of merit for describing the performance of scaling of parallelization}},
	journal   = {CoRR},
	volume    = {abs/1606.02686},
	year      = {2016},
	url       = {http://arxiv.org/abs/1606.02686},
	timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/VeghMV16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}   


@article{Vegh:StatisticalConsiderations:2017,
	author    = {J{\'{a}}nos V{\'{e}}gh},
	title     = {Statistical considerations on limitations of supercomputers},
	journal   = {CoRR},
	volume    = {abs/1710.08951},
	year      = {2017},
	url       = {http://arxiv.org/abs/1710.08951},
	archivePrefix = {arXiv},
	eprint    = {1710.08951},
	timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-08951},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Vegh:2017:AlphaEff,
	author = {J\'anos V\'egh and  P\'eter Moln\'ar},
	title = {{How to measure perfectness of parallelization in hardware/software systems}},
	booktitle = {18th Internat. Carpathian Control Conf.~ICCC},
	year = {2017},
	pages = {394--399}
}
%	date = {28-31 May, 2017},

@inproceedings{Molnar:2017:Meas,
	author = {P\'eter Moln\'ar and J\'anos V\'egh},
	title = {{Measuring Performance of Processor Instructions
	and Operating System Services in
	Soft Processor Based Systems}},
	booktitle = {18th Internat. Carpathian Control Conf.~ICCC},
	year = {2017},
	pages = {381--387}
}
%	date = {28-31 May, 2017},


@inbook{RenewingComputingVegh:2018,
	author = {{J.~V\'egh}},
	series = { Advances in Parallel Computing},
	title = {{Renewing computing paradigms for more
	efficient parallelization of single-threads}}, 
	publisher = { IOS Press},
	volume = {29},
	chapter = {13},
	pages = {305--330},
	year = {2018},
	url = {https://arxiv.org/abs/1803.04784}
}
%	series = {Data Intensive Computing Applications for Big Data},
%	editor = {Mamta Mittal and Valentina E. Balas and D. Jude %Hemanth and Raghvendra Kumar},


%issn = "0167-8191",
%doi = "https://doi.org/10.1016/j.parco.2018.03.001",

@article{IntroducingEMPA2018,
	title = {{Introducing the Explicitly Many-Processor Approach}},
	journal = "Parallel Computing ",
	volume = "75",
	number = "",
	pages = "28 - 40",
	year = "2018",
	note = "",
	author = "J. V\'egh",
	keywords = "Many-core,
	Single thread,Performance,Neumann abstractions,
	Hybrid architecture,Explicitly many-processor approach,Computing principle",
	abstract = "Abstract The deeper reasons of the present stalling in computing is scrutinized, and to enhance the single-processor performance, a new approach explicitly considering the presence of several computing units is introduced, as opposed to the presently exclusively used, 70-years old single-processor approach. The appearance of many-core processors, having many processing units in close vicinity to each other, requires to re-think some principles of computing. The goal of the approach is to enhance the single-processor performance using cooperating cores, rather than to introduce a new method for parallelization. Technically, it introduces a new control layer above the cores, a new intermediate execution unit called quasi-thread, a modified compiling method and object code for transferring parallelization information from the development system to the processor, and an on-demand self-organizing processor architecture. The resulting processors have more effective and more “green” architecture, considerably increased single-thread performance, allow for more deterministic real-time behaviour, new scheduling principles for multitasking, less operating system overhead, etc. Surprisingly, the resulting computing stack is upward compatible with the presently existing one. "
}
% url = "https://www.sciencedirect.com/science/article/pii/S0167819118300577",


@article{VeghSupercomp:2018,
	author    = {J\'{a}nos V\'{e}gh},
	title     = {{How Amdahl's law restricts supercomputer applications and building
	ever bigger supercomputers}},
	journal   = {CoRR},
	volume    = {abs/1708.01462},
	year      = {2018},
	url       = {http://arxiv.org/abs/1708.01462},
} 
%	archivePrefix = {arXiv},
%eprint    = {1708.01462},
%timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
%biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1708-01462},
%bibsource = {dblp computer science bibliography, http://dblp.org}



@article{Vegh:StatisticalConsiderations:2017,
	author    = {J{\'{a}}nos V{\'{e}}gh},
	title     = {Statistical considerations on limitations of supercomputers},
	journal   = {CoRR},
	volume    = {abs/1710.08951},
	year      = {2017},
	archivePrefix = {arXiv},
	eprint    = {1710.08951},
	timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-08951},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
%  url       = {http://arxiv.org/abs/1710.08951},


@article{Vegh:ExascaleComputing:2018,
	author = {J.~{V{\'e}gh}},
	title = "{Limitations of performance of Exascale Applications and supercomputers they are running on}",
	journal = {ArXiv e-prints; submitted to special issue of IEEE Journal of Parallel and Distributed Computing },
	archivePrefix = "arXiv",
	eprint = {1808.05338},
	primaryClass = "cs.DC",
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	year = 2018,
	month = aug,
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180805338V},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Vegh:MulticoreMended:2019,
	author = {J. V\'egh},
	title = {{Can Broken Multicore Hardware be Mended?}},
	journal = {Global Journal of Research In Engineering},	
	year = {2019},
	volume = {18},
	issue = {I},
	pages ={15--21},
	keywords = {},
	abstract = {The multi- anThe constructional details of the parallelization solutions are infinitely complex and different, so it is not possible to make a universal model to find out where those limitations are. 
	d many-core (MC) era we have reached was triggered after the beginning of the century by the stalling of single-processor performance. Technology allowed more transistors to be placed on a die, but they could not reasonably be utilized to increase single-processor performance. Predictions about the number of cores has only partly been fulfilled: today’s processors have dozens rather than the predicted hundreds of cores (although the Chinese supercomputer [3] announced in the middle of 2016 comprises 260 cores on a die). Despite this, the big players are optimistic. They expect that Moore-law persists, though based on presently unknown technologies. The effect of the stalled clock frequency is mitigated, and it is even predicted [6] that ”Now that there are multicore processors, there is no reason why computers shouldn’t begin to work faster, whether due to higher frequency or because of parallel task execution. And with parallel task execution it provides even greater functionality and flexibility! ”},
	url = {https://engineeringresearch.org/index.php/GJRE/article/view/1778}
}



@article{VeghParallelizationSave:2019,
	author = {J\'anos V\'egh and J\'ozsef V\'as\'arhelyi and D\'aniel Dr\'otos},
	journal = {Advances in Science, Technology and Engineering Systems Journal},
	number = {1},
	pages = {141--158},
	title = {{Can parallelization save the ( computing ) world ?}},
	volume = {4},
	year = {2019},
	url = {https://www.researchgate.net/publication/331435142_Can_parallelization_save_the_computing_world},
	doi = {10.25046/aj040114},
}


@inproceedings{VeghPerformanceWall:2019,
	author = {{J. V\'egh, J. V\'as\'arhelyi and D. Dr\'otos}},
	title = {{The performance wall of large parallel computing systems}}, 
	booktitle = {{Lecture Notes in Networks and Systems 68}},
	publisher = {Springer},
	pages = {224--237},
	year = {2019},
	url = {https://link.springer.com/chapter/10.1007\%2F978-3-030-12450-2\_21}
}
% ISBN 978-3-030-12449-6
%     DOI: 10.1007/978-3-030-12450-2_21
% 	editor = {I. Kabashkin and I. Yatskiv and O. Prentkovskis},

@article{VeghBrainAmdahl:2019,
	author = {J. {V\'egh}},
	title = {{How Amdahl's Law limits performance of large artificial neural networks}},
	journal = {Brain Informatics},	
	year = {2019},
	volume = {6},
	issue = {4},
	pages ={1--11},
	url = {https://braininformatics.springeropen.com/ articles/10.1186/ s40708-019-0097-2/metrics},
	keywords = {Amdahl's Law; neural networks;
	performance limitation; clock-driven electronic circuit; supercomputing;
	parallelization},
	abstract = {With both knowing more and more details about how neurons and complex neural networks work
	and having serious demand for making performable huge artificial networks,
	more and more efforts are devoted to build both hardware and/or software simulators 
	and supercomputers targeting artificial intelligence applications,
	demanding an exponentially increasing amount of computing capacity. However, the inherently
	parallel operation of the neural networks is mostly simulated deploying inherently
	sequential (or in the best case: sequential-parallel) computing elements. The paper
	shows that neural network simulators (both software and hardware ones), akin to all
	other sequential-parallel computing systems, have computing performance limitation
	due to deploying clock-driven electronic circuits, the 70-years old computing paradigm
	and Amdahl's Law about parallelized computing systems. The findings explain the
	limitations/saturation experienced in former studies.
	},
}
% https://braininformatics.springeropen.com/articles/10.1186/s40708-019-0097-2
%	url = %{https://engineeringresearch.org/index.php/GJRE/article/view/1778}
% doi = {https://doi.org/10.1186/s40708-019-0097-2}
%{\small (Why the functionality of full-scale brain simulation on processor-based simulators is limited)}


@article{VeghRoofline:2019,
	author = {J. V\'egh},
	title = {{The performance wall of parallelized sequential computing:
	the roofline of supercomputer performance gain}},
	journal = {Parallel Computing},
	volume = {in review},
	year = {2019},
	pages = {http://arxiv.org/abs/1908.02280}
}
%	journal = {},
%archivePrefix = "arXiv",


@ARTICLE{VeghLayering:2019,
	author = {J. V\'egh},
	title = {{Do we need cross layering activities or reasonable layering
	in computing systems?}},
	journal = {IEEE Design \& Test},
	year = {2019},
	pages = {in review}
}

@book{VeghPerformanceBook:2019,
	author = {J\'anos V\'egh},
	title = {{The performance wall of the parallelized sequential computing -- Can parallelization save the (computing) world?}},
	publisher = {Lambert Academic Publishing},
	isbn = {978-620-0-08051-6},
	edition = {1},
	year = {2019}
}



@INPROCEEDINGS{VeghModernParadigm:2019,
	author={J. {V\'egh} and A. {Tisan}},
	booktitle={{%2019 International Conference on Computational Science and Computational Intelligence 
	CSCI
	The 25th Int'l Conf on Parallel and Distributed Processing Techniques and Applications}},
	title = {{The need for modern computing paradigm: Science applied to computing}}, 	
	publisher = {IEEE},
	year={2019}, volume={}, number={}, pages={1523-1532},
	url       = {http://arxiv.org/abs/1908.02651},
	doi = {10.1109/CSCI49370.2019.00283}
}
%https://doi.org/10.1109/CSCI49370.2019.00283

@inbook{VeghAIperformance:2020,
	author = {J. V\'egh},
	series = {A Closer Look at Convolutional Neural Networks},
	title = {{How deep machine learning can be}}, 
	publisher = {Nova, In press},
	pages = {141--169},
	year = {2020},
	url = {https://arxiv.org/abs/2005.00872}
}
%	series = {Data Intensive Computing Applications for Big Data},
%	editor = {Mamta Mittal and Valentina E. Balas and D. Jude %Hemanth and Raghvendra Kumar},
%	volume = {29},
% chapter = {13},


@article{VeghHowMany:2020,
	year = {2020},
	month = {feb},
        volume = {76},
        number = {12},
        pages = {9430-9455},
	publisher = {Springer Science and Business Media {LLC}},
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {Finally, how many efficiencies the supercomputers have?},
	journal = {The Journal of Supercomputing},
	url = {http://link.springer.com/article/10.1007/ s11227-020-03210-4}
}
%	doi = {http://link.springer.com/article/10.1007/s11227-020-03210-4},



@article{VeghReevaluate:2020,
	author = {J. V\'egh},
	title = {{Re-evaluating scaling methods for distributed parallel systems}},
	journal = {IEEE Transactions on Distributed and Parallel Computing},
	volume = {??},
	year = {2020},
	pages = {Refused},
	url = {https://arxiv.org/abs/2002.08316}
}



@inproceedings{VeghScalingANN:2020,
	author = {J\'anos V\'egh},
	title = {{Which scaling rule applies
	to Artificial Neural Networks}}, 
	booktitle = {{%2020 International Conference on Computational Science and Computational Intelligence 
	(CSCE) The 22nd Int'l Conf on Artificial Intelligence (ICAI'20)}},
	publisher = {IEEE},
	pages = {ICA2246, in print},
	year = {2020},
	eprint={2005.08942},
	url       = {http://arxiv.org/abs/2005.08942},
	archivePrefix={arXiv},
}

@inproceedings{VeghTemporal:2020,
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {{Introducing Temporal Behavior to Computing Science}}, 
	booktitle = {{2020 CSCE, Fundamentals of Computing Science}},
	publisher = {IEEE},
	pages = {FCS2930, in print},
	year = {2020},
	eprint={2006.01128},
	url = {https://arxiv.org/abs/2006.01128},
	archivePrefix={arXiv},
}

@inproceedings{VeghSPAEMPA:2020,
	author = {J\'anos V\'egh},
	title = {{How to extend the Single-Processor Paradigm
	to the Explicitly Many-Processor Approach}}, 
	booktitle = {{2020 CSCE, Fundamentals of Computing Science}},
	publisher = {IEEE},
	pages = {Accepted FCS2243, in print},
	eprint={2006.00532},
	url       = {https://arxiv.org/abs/2006.00532},
	year = {2020},
	archivePrefix={arXiv},
}


@article{VeghDoWeKnow:2020,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am-J\'ozsef Berki},
	title = {{Do we know the operating principles of our computers
	better than those of our brain?}},
	booktitle = {{CSCI'20 CSCI-ISAI: Artificial Intelligence, CSCI2037}},
	year = {2020},
	pages = {in print},
	url = {https://arxiv.org/abs/2005.05061}
}
%	journal = {Neurocomputing},
%        International Conference on Computational Science and
%     Computational Intelligence
%     (CSCI'20 CSCI-ISAI: Artificial Intelligence, CSCI2037, Las Vegas)


@article{VeghBiologySpatioTemporal:2020,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am J. Berki},
	title = {{On the spatiotemporal behavior
	in biology-mimicking computing systems}},
	journal = {Researchgate},
	year = {2020},
	url = {\url{www.researchgate.net/publication/344325571\_On\_the\_Spatiotemporal \_Behavior\_in\_Biology-Mimicking\_Computing\_Systems}}
}
%	url = {https://arxiv.org/abs/2009.08841}
%	pages = {in review},


@INPROCEEDINGS{VeghMissingSecondDraft:2020,
	author={J\'anos V\'egh},
	booktitle={{The 2020 International Conference on Computational Science and
	Computational Intelligence; CSCI'20: December 16-18, 2020, Las Vegas, USA, paper CSCI2019}},
	title = {{von Neumann's missing "Second Draft": what it should contain}}, 	
	publisher = {IEEE},
	year={2020},  
	url       = {https://arxiv.org/abs/2011.04727},
}
%	pages={paper CSCI2019},


@article{VeghRevisingClassicComputing:2020,
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {{Revising the classic computing paradigm and its technological implementations}},
	journal = {Computing},
	year = {2020},
	pages = {in review, revision requested},
	url = {https://arxiv.org/pdf/2011.08455}
}

@article{VeghIntroducingTemporalBehaviorScience:2020,
	author = {J{\'{a}}nos V{\'{e}}gh},
	title = {{Why  do  we  need  to  Introduce  Temporal  Behavior  in  both  Modern  Science  and  Modern  Computing}},
	subtitle = {{With  an  Outlook  to Researching Modern Effects/Materials and Technologies}},
	journal = {Global Journal of Computer Science and Technology: Hardware \& Computation},
	volume = {20/1},
	year = {2020},
	pages = {13-29},
	url = {https://doi.org/10.34257/GJCSTAVOL20IS1PG13}
}
%
%https://globaljournals.org/GJCST_Volume20/2-Why-do-we-need-to-Introduce-Temporal.pdf

%https://arxiv.org/pdf/2011.08455}

@article{RoleOfInformationTransferSpeed:2021,
	author = {J{\'{a}}nos V{\'{e}}gh and \'Ad\'am-J\'ozsef Berki},
	title = {{On the Role of Information Transfer’s Speed in Technological and Biological Computations}},
	journal = {MPDI Brain Science},
	volume = {submitted},
	year = {2021},
	pages = {1},
	doi = {doi:10.20944/preprints202103.0415.v1},
	url = {https://doi: 10.20944/preprints202103.0414.v1}
}

@INPROCEEDINGS{VeghComputingModel:2021,
	author={J\'anos V\'egh},
	booktitle={{The 2021 International Conference on Computational Science and
	Computational Intelligence; Fundamentals of Conputing Science}},
	title = {{A model for storing and processing information in technological and biological computing systems}}, 	
	publisher = {IEEE},
pages = {FCS4404, in print},
	year={2021},  
}


%https://europepmc.org/article/ppr/ppr297778
