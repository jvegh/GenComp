
%% The neuronal- related citations

@Article{LogicalCalculusMcCulloch:1943,
	author =       "Warren S. McCulloch and Walter Pitts",
	title =        "A logical calculus of the ideas immanent in nervous
	activity",
	journal =      "j-BULL-MATH-BIOPHYS",
	volume =       "5",
	number =       "4",
	pages =        "115--133",
	month =        dec,
	year =         "1943",
	CODEN =        "BMBIAO",
	DOI =          "https://doi.org/10.1007/BF02478259",
	ISSN =         "0007-4985 (print), 2376-8398 (electronic)",
	ISSN-L =       "0007-4985",
	bibdate =      "Wed Jun 28 16:12:34 MDT 2017",
	bibsource =    "http://www.math.utah.edu/pub/tex/bib/bullmathbiophys.bib",
	URL =          "http://link.springer.com/article/10.1007/BF02478259",
	acknowledgement = ack-nhfb,
	fjournal =     "Bulletin of Mathematical Biophysics",
	journal-URL =  "http://link.springer.com/journal/11538",
}



@article{NeuralInformationPittsMcCulloch:1947,
  author = {Pitts, Walter and McCulloch, Warren S.},
  year = {1947},
  title = {How we know universals the perception of auditory and visual forms},
  journal = {{The Bulletin of Mathematical Biophysics}},
  pages = {127-147},
  volume = {9},
  issue = {3},
  abstract = {Two neural mechanisms are described which exhibit recognition of forms. Both are independent of small perturbations at synapses of excitation, threshold, and synchrony, and are referred to partiular appropriate regions of the nervous system, thus suggesting experimental verification. The first mechanism averages an apparition over a group, and in the treatment of this mechanism it is suggested that scansion plays a significant part. The second mechanism reduces an apparition to a standard selected from among its many legitimate presentations. The former mechanism is exemplified by the recognition of chords regardless of pitch and shapes regardless of size. The latter is exemplified here only in the reflexive mechanism translating apparitions to the fovea. Both are extensions to contemporaneous functions of the knowing of universals heretofore treated by the authors only with respect to sequence in time.},
  doi = {10.1007/BF02478291}
}


@book{
	HebbBook:1949,
	author = {Hebb, D.O.},
	title = {{The Organization of Behavior}},
	publisher = {New York: Wiley and Sons},
	year = {1949}
}

@article{TuringTest:1950,
	title={{Computing Machinery and Intelligence}},
	author={A. M. Turing},
	journal={Mind},
	year={1950},
	volume={LIX},
	issue={236},
	pages={433-460},
	doi={10.1093/mind/LIX.236.433}
}

@article{mackay1952limiting,
	title={The limiting information capacity of a neuronal link},
	author={MacKay, Donald M and McCulloch, Warren S},
	journal={The bulletin of mathematical biophysics},
	volume={14},
	number={2},
	pages={127--135},
	year={1952},
	publisher={Springer}
}

@article{SteinFrequencyCode:1967,
  author = {Stein, R. B.},
  title = {The information capacity of nerve cells using a frequency code},
  journal =  {Biophys J.},
  year = {1967},
  volume = {6},
  issue = {7},
  pages = {797-826},
  doi = {10.1016/S0006-3495(67)86623-2},
}


@book{SomjenBook:1972,
	author = {Somjen, G. },
	title = {{SENSORY CODING
	in the mammalian nervous system}},
	publisher = {New York, MEREDITH CORPORATION},
	year = {1972},
	howpublished={https://www.springer.com/us/book/9781468417074},
	doi = {10.1007/978-1-4684-1707-4},
}

%[Accessed March 26, 2018.]

@article{KochElectricalPropertiesSpike:1983,
	title={A theoretical analysis of electrical properties of spines},
	author={Christof Koch and Tomaso A. Poggio},
	journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
	year={1983},
	volume={218},
	pages={455 - 477}
}

@article {FireWireTogether:1992,
	author = {Lowel, S and Singer, W},
	title = {Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity},
	volume = {255},
	number = {5041},
	pages = {209--212},
	year = {1992},
	doi = {10.1126/science.1372754},
	publisher = {American Association for the Advancement of Science},
	abstract = {In the visual cortex of the brain, long-ranging tangentially oriented axon collaterals interconnect regularly spaced clusters of cells. These connections develop after birth and attain their specificity by pruning. To test whether there is selective stabilization of connections between those cells that exhibit correlated activity, kittens were raised with artificially induced strabismus (eye deviation) to eliminate the correlation between signals from the two eyes. In area 17, cell clusters were driven almost exclusively from either the right or the left eye and tangential intracortical fibers preferentially connected cell groups activated by the same eye. Thus, circuit selection depends on visual experience, and the selection criterion is the correlation of activity.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/255/5041/209},
	eprint = {https://science.sciencemag.org/content/255/5041/209.full.pdf},
	journal = {Science}
}


@book{
	JohstonWuNeurophysiology:1995,
	author = {Daniel Johnston and Samuel Miao-sin Wu},
	title = {{Foundations of Cellular Neurophysiology}},
	publisher = {Massachusetts Institute of Technology},
	isbn = {978-0-262-10053-3},
	year = {1995}
}


@article{SpatiotemporalPlenz:1996,
	author = {D. Plenz and A. Aertsen},
	title = {{Neural dynamics in cortex-striatum ci-cultures -- II. Spatiotemporal Characteristics of Neural Activity}},
	journal = {{Neuroscience}},
	volume = {70/4},
	year = {1996},
	pages = {893--924},
} 

@article{SpikeReproducibility:1997,
	author = {Rob R. de Ruyter van Steveninck  and Geoffrey D. Lewen  and Steven P. Strong  and Roland Koberle  and William Bialek },
	title = {Reproducibility and Variability in Neural Spike Trains},
	journal = {Science},
	volume = {275},
	number = {5307},
	pages = {1805-1808},
	year = {1997}, 
	doi = {10.1126/science.275.5307.1805},
	abstract = {To provide information about dynamic sensory stimuli, the pattern of action potentials in spiking neurons must be variable. To ensure reliability these variations must be related, reproducibly, to the stimulus. For H1, a motion-sensitive neuron in the fly's visual system, constant-velocity motion produces irregular spike firing patterns, and spike counts typically have a variance comparable to the mean, for cells in the mammalian cortex. But more natural, time-dependent input signals yield patterns of spikes that are much more reproducible, both in terms of timing and of counting precision. Variability and reproducibility are quantified with ideas from information theory, and measured spike sequences in H1 carry more than twice the amount of information they would if they followed the variance-mean relation seen with constant inputs. Thus, models that may accurately account for the neural response to static stimuli can significantly underestimate the reliability of signal transfer under more natural conditions.}
}
%	URL = {https://www.science.org/doi/abs/10.1126/science.275.5307.1805},


@article{ConductanceBySynapticActivity:1998,
	author = {Tim A. Benke and Andreas L\"uthi and John T. R. Isaac and Graham L. Collingridge},
	title = {{Modulation of AMPA receptor
	unitary conductance
	by synaptic activity}},
	journal = {{Nature}},
	volume = {393},
	year = {1998},
	issn = {793-797},
	pages = {1629--1636},
} 

@article{NeuralInformationCost:1998,
  author = {Laughlin, Simon B. and
de Ruyter van Steveninck, Rob R. and
 Anderson, John C.},
 year = {1998},
 title = {The metabolic cost of neural information},
 journal = {Nature Neuroscience},
 pages = {36--41},
 volume ={1},
 issue = {1},
 abstract = {We derive experimentally based estimates of the energy used by neural mechanisms to code known quantities of information. Biophysical measurements from cells in the blowfly retina yield estimates of the ATP required to generate graded (analog) electrical signals that transmit known amounts of information. Energy consumption is several orders of magnitude greater than the thermodynamic minimum. It costs 104 ATP molecules to transmit a bit at a chemical synapse, and 106 - 107 ATP for graded signals in an interneuron or a photoreceptor, or for spike coding. Therefore, in noise-limited signaling systems, a weak pathway of low capacity transmits information more economically, which promotes the distribution of information among multiple pathways.},
 doi = {10.1038/236}
}

@article{EntropyInSpikeTrains:1998,
	title = {Entropy and Information in Neural Spike Trains},
	author = {Strong, S. P. and Koberle, Roland and de Ruyter van Steveninck, Rob R. and Bialek, William},
	journal = {Phys. Rev. Lett.},
	volume = {80},
	issue = {1},
	pages = {197--200},
	numpages = {0},
	year = {1998},
	month = {Jan},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevLett.80.197},
}
%	url = {https://link.aps.org/doi/10.1103/PhysRevLett.80.197}
˚
@article{SpatiotemporalPrut:1998,
	author = {Yifat Prut and Eilon Vaadia and Hagai Bergman and Iris Haalman and Hamutal Slovin and
	Moshe Abeles},
	title = {{Spatiotemporal Structure of Cortical Activity: Properties and
	Behavioral Relevance}},
	journal = {{J. Neurophysiol}},
	volume = {79},
	year = {1998}
}

@book{SejnowskiNeuralComputation:1999,
	authors = {Abbott L., Sejnowski T. J.},
	year = {1999},
	booktitle = {Neural Codes and Distributed Representations: Foundations of Neural Computation},
	publisher = {Cambridge, MA: MIT Press},
}

	@article{ExtraCellularNonOhmic:2016,
	title = {Intracellular Impedance Measurements Reveal Non-ohmic Properties of the Extracellular Medium around Neurons},
	journal = {Biophysical Journal},
	volume = {110},
	number = {1},
	pages = {234-246},
	year = {2016},
	issn = {0006-3495},
	doi = {https://doi.org/10.1016/j.bpj.2015.11.019},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349515011765},
	author = {Jean-Marie Gomes and Claude Bedard and Silvana Valtcheva and Matthew Nelson and Vitalia Khokhlova and Pierre Pouget and Laurent Venance and Thierry Bal and Alain Destexhe},
	abstract = {Determining the electrical properties of the extracellular space around neurons is important for understanding the genesis of extracellular potentials, as well as for localizing neuronal activity from extracellular recordings. However, the exact nature of these extracellular properties is still uncertain. Here, we introduce a method to measure the impedance of the tissue, one that preserves the intact cell-medium interface using whole-cell patch-clamp recordings in vivo and in vitro. We find that neural tissue has marked non-ohmic and frequency-filtering properties, which are not consistent with a resistive (ohmic) medium, as often assumed. The amplitude and phase profiles of the measured impedance are consistent with the contribution of ionic diffusion. We also show that the impact of such frequency-filtering properties is possibly important on the genesis of local field potentials, as well as on the cable properties of neurons. These results show non-ohmic properties of the extracellular medium around neurons, and suggest that source estimation methods, as well as the cable properties of neurons, which all assume ohmic extracellular medium, may need to be reevaluated.},
} 


@article{HybridBrainOperation:1998,
	author = {Sarpeshkar, Rahul},
	title = "{Analog Versus Digital: Extrapolating from Electronics to Neurobiology}",
	journal = {Neural Computation},
	volume = {10},
	number = {7},
	pages = {1601-1638},
	year = {1998},
	month = {10},
	abstract = "{We review the pros and cons of analog and digital computation. We propose that computation that is most efficient in its use of resources is neither analog computation nor digital computation but, rather, a mixture of the two forms. For maximum efficiency, the information and information-processing resources of the hybrid form must be distributed over many wires, with an optimal signal-to-noise ratio per wire. Our results suggest that it is likely that the brain computes in a hybrid fashion and that an underappreciated and important reason for the efficiency of the human brain, which consumes only 12 W, is the hybrid and distributed nature of its architecture.}",
	issn = {0899-7667},
	doi = {10.1162/089976698300017052},
}
%	url = {https://doi.org/10.1162/089976698300017052},
%	eprint = %{https://direct.mit.edu/neco/article-pdf/10/7/1601/813938/089976698300017052.pdf},

@article{NeuralInformationTheory:1999,
	author = {Borst, Alexander and Theunissen, Frederic E.},
	year = {1999},
	title = {Information theory and neural coding},
	journal = {Nature Neuroscience},
	page = {947-957},
	volume = {2},
	issue = {11},
	abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus–response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus–response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
	doi = {10.1038/14731}
}

@book{KochBiophysics:1999,
	author = {Christof Koch},
	title = {{Biophysics of Computation}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-518199-9},
	year = {1999}
}

@CONFERENCE{InformationTheoryNeuralSpikeTrains:1998,
author ={Strong, S. P. and de Ruyter van Steveninck and Bialek,  R. and Koberle, R.},
year = {1998},
title ={On the application of information theory to neural spike trains.},
booktitle = { Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
pages ={621-632},
}

@article{KochVoltageDependentConductance:1999,
	author = {Stemmler, M. and Koch, C.},
	title = {{How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate}},
	journal = {{Nat Neurosci}},
	volume = {2},
	year = {1999},
	pages = {521--527},
	howpublished = {https://doi.org/10.1038/9173},
} 

@article{SynergyNeuralCode:2000,
	author = {Brenner, Naama and Strong, Steven P. and Koberle, Roland and Bialek, William and Steveninck, Rob R. de Ruyter van},
	title = "{Synergy in a Neural Code}",
	journal = {Neural Computation},
	volume = {12},
	number = {7},
	pages = {1531-1552},
	year = {2000},
	month = {07},
	abstract = "{We show that the information carried by compound events in neural spike trains-patterns of spikes across time or across a population of cells-can be measured, independent of assumptions about what these patterns might represent. By comparing the information carried by a compound pattern with the information carried independently by its parts, we directly measure the synergy among these parts. We illustrate the use of these methods by applying them to experiments on the motion-sensitive neuron H1 of the fly's visual system, where we confirm that two spikes close together in time carry far more than twice the information carried by a single spike. We analyze the sources of this synergy and provide evidence that pairs of spikes close together in time may be especially important patterns in the code of H1.}",
	doi = {10.1162/089976600300015259},
}

@article{LAUGHLIN2001475,
	title = {Energy as a constraint on the coding and processing of sensory information},
	journal = {Current Opinion in Neurobiology},
	volume = {11},
	number = {4},
	pages = {475-480},
	year = {2001},
	issn = {0959-4388},
	doi = {https://doi.org/10.1016/S0959-4388(00)00237-3},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438800002373},
	author = {Simon B Laughlin},
	keywords = {sensory system, brain signalling, synapse, neural code, energy, efficiency, information},
	abstract = {Neurons use significant amounts of energy to generate signals. Recent studies of retina and brain connect this energy usage to the ability to transmit information. The identification of energy-efficient neural circuits and codes suggests new ways of understanding the function, design and evolution of nervous systems.}
}

@article{RedundancyReductionBarlow:2001,
	doi = {10.1088/0954-898X/12/3/301},
	year = {2001},
	month = {aug},
	publisher = {},
	volume = {12},
	number = {3},
	pages = {241},
	author = {Horace Barlow},
	title = {Redundancy reduction
	revisited},
	journal = {Network: Computation in Neural Systems},
	abstract = {Soon after Shannon defined the concept of redundancy it was suggested that it gave insight into mechanisms of sensory processing, perception, intelligence and inference. Can we now judge whether there is anything in this idea, and can we see where it should direct our thinking? This paper argues that the original hypothesis was wrong in over-emphasizing the role of compressive coding and economy in neuron numbers, but right in drawing attention to the importance of redundancy. Furthermore there is a clear direction in which it now points, namely to the overwhelming importance of probabilities and statistics in neuroscience. The brain has to decide upon actions in a competitive, chance-driven world, and to do this well it must know about and exploit the non-random probabilities and interdependences of objects and events signalled by sensory messages. These are particularly relevant for Bayesian calculations of the optimum course of action. Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles.}
}
%	url = {https://dx.doi.org/10.1088/0954-898X/12/3/301},

@article{DimitrovMillerDecoding:2001,
	doi = {10.1088/0954-898X/12/4/303},
	year = {2001},
	month = {aug},
	publisher = {},
	volume = {12},
	number = {4},
	pages = {441},
	author = {Alexander G Dimitrov  and John P Miller},
	title = {Neural coding and decoding:
	communication channels and quantization},
	journal = {Network: Computation in Neural Systems},
	abstract = {We present a novel analytical approach for studying neural encoding. As a first step we model a neural sensory system as a communication channel. Using the method of typical sequence in this context, we show that a coding scheme is an almost bijective relation between equivalence classes of stimulus/response pairs. The analysis allows a quantitative determination of the type of information encoded in neural activity patterns and, at the same time, identification of the code with which that information is represented. Due to the high dimensionality of the sets involved, such a relation is extremely difficult to quantify. To circumvent this problem, and to use whatever limited data set is available most efficiently, we use another technique from information theory - quantization. We quantize the neural responses to a reproduction set of small finite size.  Among many possible quantizations, we choose one which preserves as much of the informativeness of the original stimulus/response relation as possible, through the use of an information-based distortion function. This method allows us to study coarse but highly informative approximations of a coding scheme model, and then to refine them automatically when more data become available.}
}
%	url = {https://dx.doi.org/10.1088/0954-898X/12/4/303},

@book{NeuroscienceBook:2001,
	editor = { Dale Purves and George J Augustine and David Fitzpatrick and Lawrence C Katz and Anthony-Samuel LaMantia and James O McNamara and S Mark Williams},
	title = {{Neuroscience}},
	publisher = {Sinauer Associates},
	isbn = {0-87893-742-0},
	edition = {2},
	year = {2001},
	url={https://www.ncbi.nlm.nih.gov/books/NBK10799/}
}
%https://www.ncbi.nlm.nih.gov/books/NBK10799/

@book{GertstnerSpikingNeuronModels:2002,
	author = {Gerstner and Kistler},
	title = {{Spiking Neuron Models. Single Neurons, Populations, Plasticity}},
	publisher = {Cambridge University Press},
	isbn = {ISBN 0 521 89079 9},
	year = {2002}
}

  @book{LyttonGFromComputerToBrain:2002,
	author = {William W. Lytton},
	title = {{From Computer to Brain}},
        subtitle = {{Foundations of Computational Neuroscience}},
	publisher = {Springer},
	isbn = {ISBN 978-0-387-95526-1},
	year = {2002}
}



@article{PerturbationNeuralComputation:2002,
author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
journal = {Neural Computation},
volume = {14},
number = {11},
pages = {2531-2560},
year = {2002},
doi = {10.1162/089976602760407955},
}



@misc{JohnsonDialogue:2003,
	author = {Johnson, D. H.},
	year = {2003},
	title = {Dialogue concerning neural coding and
information theory},
	howpublished = {\\http://www.ece.rice.edu/\~dhj/dialog.pdf},
}


@article{BurstNeuralInformationIzkievich:2003,
	author = {Izhikevich, E. M. and  Desai, N. S. and
	Walcott, E. C. and Hoppensteadt, F. C.},
	title = {Bursts as a unit of neural information:
	selective communication via
	resonance},
	journal = {TRENDS in Neurosciences},
	volume = {26},
	number = {3},
	year = {2003},
	pages = {161-167},
}

@article{HiddenMarkovModel:2004,
	author = {Eddy, S.},
	title = {{What is a hidden Markov model?}},
	journal = {Nature Biotechnology},
	volume = {22},
	year = {2004},
	pages = {1315-1316},
	doi = {10.1038/nbt1004-1315},
}

@book{SynapticOrganizationGordon:2004,
	editor = {Stepherd Gordon},
	title = {{The Synaptic Organization of the Brain}},
	publisher = {Oxford Academic, New York},
	edition = {5},
	doi = {10.1093/acprof:oso/9780195159560.001.1},
	year = {2006}
}


@article{BrainClocking:2005,
	author = {{Antle, M. C. and Silver, R.}},
	title = {{Orchestrating time: arrangements of the brain circadian clock}},
	journal = {{Trends Neurosci.}},
	volume = {28},
	year = {2015},
	pages = {145-151},
} 


@article {HalfNodeRanvier:2005,
	author = {Gina E. Sosinsky and Thomas J. Deerinck and Rocco Greco and Casey H. Buitenhuys and Thomas M. Bartol and Mark H. Ellisman},
	title = {{Small Nodes of Ranvier From Peripheral Nerves of MiceReconstructed by Electron Tomography}},
	subtitle = {{Development of a Model for Microphysiological Simulations}},
	journal = {Neuroinformatics},
	doi = {10.1385/NI:03:02:133},
	pages = {33–162},
	year = {2005},
	volume = {3},
}

@article{NonlinearMultivariatePEREDA:2005,
	title = {Nonlinear multivariate analysis of neurophysiological signals},
	journal = {Progress in Neurobiology},
	volume = {77},
	number = {1},
	pages = {1-37},
	year = {2005},
	issn = {0301-0082},
	doi = {https://doi.org/10.1016/j.pneurobio.2005.10.003},
	url = {https://www.sciencedirect.com/science/article/pii/S030100820500119X},
	author = {Ernesto Pereda and Rodrigo Quian Quiroga and Joydeep Bhattacharya},
	keywords = {Nonlinear analysis, Synchronization, Multivariate time series, Surrogate data, EEG, MEG, Spike trains},
	abstract = {Multivariate time series analysis is extensively used in neurophysiology with the aim of studying the relationship between simultaneously recorded signals. Recently, advances on information theory and nonlinear dynamical systems theory have allowed the study of various types of synchronization from time series. In this work, we first describe the multivariate linear methods most commonly used in neurophysiology and show that they can be extended to assess the existence of nonlinear interdependences between signals. We then review the concepts of entropy and mutual information followed by a detailed description of nonlinear methods based on the concepts of phase synchronization, generalized synchronization and event synchronization. In all cases, we show how to apply these methods to study different kinds of neurophysiological data. Finally, we illustrate the use of multivariate surrogate data test for the assessment of the strength (strong or weak) and the type (linear or nonlinear) of interdependence between neurophysiological signals.}
}

@article{LosonczyIntegrative:2006,
	author = {Losonczy, A. and Magee, J.C.},
	year = {2006},
	title = {{Integrative properties of radial oblique
	dendrites in hippocampal CA1 pyramidal neurons}},
	journal = {Neuron},
	volume = {50},
	pages = {291-307},
	doi = {10.1016/j.neuron.2006.03.016}
}


@book{BuzsakiRhythms:2006,
	author = {Gy\"orgy Buzs\'aki},
	title = {{Rhythms of the Brain}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-530106-9},
	edition = {1},
	year = {2006}
}


@article{SynchronizationCausality2007,
	title = {Causality detection based on information-theoretic approaches in time series analysis},
	journal = {Physics Reports},
	volume = {441},
	number = {1},
	pages = {1-46},
	year = {2007},
	issn = {0370-1573},
	doi = {https://doi.org/10.1016/j.physrep.2006.12.004},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157307000403},
	author = {Katerina Hlaváčková-Schindler and Milan Paluš and Martin Vejmelka and Joydeep Bhattacharya},
	keywords = {Causality, Entropy, Mutual information, Estimation},
	abstract = {Synchronization, a basic nonlinear phenomenon, is widely observed in diverse complex systems studied in physical, biological and other natural sciences, as well as in social sciences, economy and finance. While studying such complex systems, it is important not only to detect synchronized states, but also to identify causal relationships (i.e. who drives whom) between concerned (sub) systems. The knowledge of information-theoretic measures (i.e. mutual information, conditional entropy) is essential for the analysis of information flow between two systems or between constituent subsystems of a complex system. However, the estimation of these measures from a set of finite samples is not trivial. The current extensive literatures on entropy and mutual information estimation provides a wide variety of approaches, from approximation-statistical, studying rate of convergence or consistency of an estimator for a general distribution, over learning algorithms operating on partitioned data space to heuristical approaches. The aim of this paper is to provide a detailed overview of information theoretic approaches for measuring causal influence in multivariate time series and to focus on diverse approaches to the entropy and mutual information estimation.}
}


@Article{BretteSpikingSimulation:2007,
title = {Simulation of networks of spiking neurons: a review of tools and strategies},
pages = {349-98},
abstract ={ We review different aspects of the simulation of spiking neural networks. We 
start by reviewing the different types of simulation strategies and algorithms 
that are currently implemented. We next review the precision of those simulation 
strategies, in particular in cases where plasticity depends on the exact timing 
of the spikes. We overview different simulators and simulation environments 
presently available (restricted to those freely available, open source and 
documented). For each simulation tool, its advantages and pitfalls are reviewed, 
with an aim to allow the reader to identify which simulator is appropriate for a 
given task. Finally, we provide a series of benchmark simulations of different 
types of networks of spiking neurons, including Hodgkin-Huxley type, 
integrate-and-fire models, interacting with current-based or conductance-based 
synapses, using clock-driven or event-driven integration strategies. The same set 
of models are implemented on the different simulators, and the codes are made 
available. The ultimate goal of this review is to provide a resource to 
facilitate identifying the appropriate integration strategy and simulation tool 
to use for a given modeling problem related to spiking neural networks.
},
author = {Brette, Romain},
journal ={{J Comput. Neurosci.}},
doi ={10.1007/s10827-007-0038-6},
volume = {23},
issue = {3},
}

@Article{SynapticPlasticity:2008,
	author = {Citri, A and Malenka, RC},
	title = {Synaptic plasticity: multiple forms, functions, and mechanisms},
	journal = {Neuropsychopharmacology},
	year = {2008},
	issue = {33},
	volume ={1},
	pages = {18--41},
	doi = {10.1038/sj.npp.1301559}
}

@Article{SubmillisecondResolution:2008,
doi = {10.1371/journal.pcbi.1000025},
author = {Nemenman, Ilya AND Lewen, Geoffrey D. AND Bialek, William AND de Ruyter van Steveninck, Rob R.},
journal = {PLOS Computational Biology},
publisher = {Public Library of Science},
title = {{Neural Coding of Natural Stimuli: Information at Sub-Millisecond Resolution}},
year = {2008},
month = {03},
volume = {4},
pages = {1-12},
abstract = {Sensory information about the outside world is encoded by neurons in sequences of discrete, identical pulses termed action potentials or spikes. There is persistent controversy about the extent to which the precise timing of these spikes is relevant to the function of the brain. We revisit this issue, using the motion-sensitive neurons of the fly visual system as a test case. Our experimental methods allow us to deliver more nearly natural visual stimuli, comparable to those which flies encounter in free, acrobatic flight. New mathematical methods allow us to draw more reliable conclusions about the information content of neural responses even when the set of possible responses is very large. We find that significant amounts of visual information are represented by details of the spike train at millisecond and sub-millisecond precision, even though the sensory input has a correlation time of ∼55 ms; different patterns of spike timing represent distinct motion trajectories, and the absolute timing of spikes points to particular features of these trajectories with high precision. Finally, the efficiency of our entropy estimator makes it possible to uncover features of neural coding relevant for natural visual stimuli: first, the system's information transmission rate varies with natural fluctuations in light intensity, resulting from varying cloud cover, such that marginal increases in information rate thus occur even when the individual photoreceptors are counting on the order of one million photons per second. Secondly, we see that the system exploits the relatively slow dynamics of the stimulus to remove coding redundancy and so generate a more efficient neural code.},
number = {3},
}
%url = {https://doi.org/10.1371/journal.pcbi.1000025},


@article{HebbianLearningRule:2008,
	author = {Caporale, Natalia and Dan, Yang},
	title = {{Spike Timing–Dependent Plasticity: A Hebbian Learning Rule}},
	journal = {Annual Review of Neuroscience},
	volume = {31},
	number = {1},
	pages = {25-46},
	year = {2008},
	doi = {10.1146/annurev.neuro.31.060407.125639},
	note ={PMID: 18275283},
	
	URL = { 
	https://doi.org/10.1146/annurev.neuro.31.060407.125639
	
	},
	eprint = { 
	https://doi.org/10.1146/annurev.neuro.31.060407.125639
	
	}
	,
	abstract = { Spike timing–dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo. }
}

@article{UpAndDownStatesNeuron:2008,
	author  = {Wilson. C.},
	title = {{Up and down states}},
	journal = {Scholarpedia J},
	year = {2008},
	volume = {6},
	number = {3},
	page = {1410},
	doi = {10.4249/scholarpedia.1410},
}

@article{TransientResponses:2008,
	author = {{Khorsand P and Chance F }},
	title = {{Transient Responses to Rapid Changes in Mean and Variance in Spiking Models}},
	journal = {{PLoS ONE}},
	volume = {3},
	number = {11},
	year = {208},
	pages = {e3786},
	articleno = {19},
	numpages = {29},
	doi = {doi.org/10.1371/journal.pone.0003786},
} 

@ARTICLE{EnergyEfficientNeuralComputing:2010,
		author={Berger, Toby and Levy, William B},
	journal={IEEE Transactions on Information Theory}, 
	title={{A Mathematical Theory of Energy Efficient Neural Computation and Communication}}, 
	year={2010},
	volume={56},
	number={2},
	pages={852-874},
	doi={10.1109/TIT.2009.2037089}
}


@article{ConvergenceComputationalNeuroscience:2011,
  author ={McDonnell, Mark D. and Ikeda, Shiro and Manton, Jonathan H.},
  year = {2011},
  title = {An introductory review of information theory in the context of computational neuroscience},
  journal = {Biological Cybernetics},
  page = {55},
  volume = {105},
  issue = {1},
  abstract = {This article introduces several fundamental concepts in information theory from the perspective of their origins in engineering. Understanding such concepts is important in neuroscience for two reasons. Simply applying formulae from information theory without understanding the assumptions behind their definitions can lead to erroneous results and conclusions. Furthermore, this century will see a convergence of information theory and neuroscience; information theory will expand its foundations to incorporate more comprehensively biological processes thereby helping reveal how neuronal networks achieve their remarkable information processing abilities.},
  doi = {10.1007/s00422-011-0451-9},
}

@article{EffectiveConnectivity:2011,
	author = {Vicente, Raul and Wibral, Michael and Lindner, Michael and Pipa, Gordon},
	year = {2011},
	title = {{Transfer entropy -- a model-free measure of effective connectivity for the neurosciences}},
	journal = {Journal of Computational Neuroscience},
	volume = {30},
	pages = {45--67},
	doi = {10.1007/s10827-010-0262-3},
	abstract = {Understanding causal relationships, or effective connectivity, between parts of the brain is of utmost importance because a large part of the brain’s activity is thought to be internally generated and, hence, quantifying stimulus response relationships alone does not fully describe brain dynamics. Past efforts to determine effective connectivity mostly relied on model based approaches such as Granger causality or dynamic causal modeling. Transfer entropy (TE) is an alternative measure of effective connectivity based on information theory. TE does not require a model of the interaction and is inherently non-linear. We investigated the applicability of TE as a metric in a test for effective connectivity to electrophysiological data based on simulations and magnetoencephalography (MEG) recordings in a simple motor task. In particular, we demonstrate that TE improved the detectability of effective connectivity for non-linear interactions, and for sensor level MEG signals where linear methods are hampered by signal-cross-talk due to volume conduction.},
}

@article{BuzsakiGammaOscillations:2012,
	author = {{Gy\"orgy Buzs\'aki and Xiao-Jing Wang}},
	title = {{Mechanisms of Gamma Oscillations}},
	journal = {{Annual Reviews of Neurosciences}},
	volume = {3},
	number = {4},
	year = {2012},
	issn = {1936-7406},
	pages = {19:1--19:29},
	articleno = {19},
	numpages = {29},
	doi = {10.1146/annurev-neuro-062111-150444},
} 

@book{PrinciplesNeuralScience:2013,
	author = {Eric R. Kandel and James H. Schwartz and Thomas M. Jessell and Steven A. Siegelbaum abd A. J. Hudspeth },
	title = {{Principles of Neural Science}},
	publisher = {The McGraw-Hill},
	isbn = {978-0-07-18101-2},
	edition = {5},
	year = {2013}
}

@book{SpikeTimingBook:2013,
	author = {Patricia M. DiLorenzo and Jonathan D. Victor},
	title = {{Spike Timing: Mechanisms and Function}},
	publisher = {CRC Press},
	isbn = {978-1-4398-3815-0},
	edition = {1},
	year = {2013}
}

@article{MeaningOfSpikes:2014,
	author = {Fiorillo, C.D. and Kim, J.Kk and Hong, S.Z.}, 
	title={The meaning of spikes from the neuron's point of view: predictive homeostasis generates the appearance of randomness}, 
	journal={Front Comput Neurosci.},
	year={2014}, 
	volume={8},
	page={49},
	doi= {10.3389/fncom.2014.00049}
}


@article{RoleOfMyelinPlasticity:2014,
	title = {Role of myelin plasticity in oscillations and synchrony of neuronal activity},
	journal = {Neuroscience},
	volume = {276},
	pages = {135-147},
	year = {2014},
	note = {Secrets of the CNS White Matter},
	issn = {0306-4522},
	doi = {10.1016/j.neuroscience.2013.11.007},
	url = {https://www.sciencedirect.com/science/article/pii/S0306452213009470},
	author = {S. Pajevic and P.J. Basser and R.D. Fields},
	keywords = {activity-dependent myelination, white matter plasticity, synchronization, oscillations, conduction velocity and delays, coupled oscillators},
	abstract = {Conduction time is typically ignored in computational models of neural network function. Here we consider the effects of conduction delays on the synchrony of neuronal activity and neural oscillators, and evaluate the consequences of allowing conduction velocity (CV) to be regulated adaptively. We propose that CV variation, mediated by myelin, could provide an important mechanism of activity-dependent nervous system plasticity. Even small changes in CV, resulting from small changes in myelin thickness or nodal structure, could have profound effects on neuronal network function in terms of spike-time arrival, oscillation frequency, oscillator coupling, and propagation of brain waves. For example, a conduction delay of 5ms could change interactions of two coupled oscillators at the upper end of the gamma frequency range (∼100Hz) from constructive to destructive interference; delays smaller than 1ms could change the phase by 30°, significantly affecting signal amplitude. Myelin plasticity, as another form of activity-dependent plasticity, is relevant not only to nervous system development but also to complex information processing tasks that involve coupling and synchrony among different brain rhythms. We use coupled oscillator models with time delays to explore the importance of adaptive time delays and adaptive synaptic strengths. The impairment of activity-dependent myelination and the loss of adaptive time delays may contribute to disorders where hyper- and hypo-synchrony of neuronal firing leads to dysfunction (e.g., dyslexia, schizophrenia, epilepsy).}
}


@article{NonOhmicProperties:2015,
	title = {{Intracellular Impedance Measurements Reveal Non-ohmic Properties of the Extracellular Medium around Neurons}},
	journal = {Biophysical Journal},
	volume = {110},
	number = {1},
	pages = {234-246},
	year = {2016},
	issn = {0006-3495},
	doi = {https://doi.org/10.1016/j.bpj.2015.11.019},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349515011765},
	author = {Jean-Marie Gomes and Claude Bédard and Silvana Valtcheva and Matthew Nelson and Vitalia Khokhlova and Pierre Pouget and Laurent Venance and Thierry Bal and Alain Destexhe},
	abstract = {Determining the electrical properties of the extracellular space around neurons is important for understanding the genesis of extracellular potentials, as well as for localizing neuronal activity from extracellular recordings. However, the exact nature of these extracellular properties is still uncertain. Here, we introduce a method to measure the impedance of the tissue, one that preserves the intact cell-medium interface using whole-cell patch-clamp recordings in vivo and in vitro. We find that neural tissue has marked non-ohmic and frequency-filtering properties, which are not consistent with a resistive (ohmic) medium, as often assumed. The amplitude and phase profiles of the measured impedance are consistent with the contribution of ionic diffusion. We also show that the impact of such frequency-filtering properties is possibly important on the genesis of local field potentials, as well as on the cable properties of neurons. These results show non-ohmic properties of the extracellular medium around neurons, and suggest that source estimation methods, as well as the cable properties of neurons, which all assume ohmic extracellular medium, may need to be reevaluated.},
} 

@article{QuantificationOfBursting:2015,
	author = {
Eisenman, L.N. and Emnett,  C.M. and Mohan, J. and Zorumski, C.F. and Mennerick, S.},
    title = { Quantification of bursting and synchrony in cultured hippocampal neurons},
    journal = { Journal of Neurophysiology},
    volume = {114},
    year = {2015},
    issue = {2},
    page = {1059-71},
    doi = {10.1152/jn.00079.2015},
}

@article{RealisticNeuronalModeling:2016,
	author = {Almog, M and Korngreen, A},
	title = {Is realistic neuronal modeling realistic?},
	journal = {J Neurophysiol.},
	year = {2016},
	pages = {2180-2209},
	volume = {5},
	issue = {116},
	doi ={doi: 10.1152/jn.00360.2016},
}
	

@article{MyelinatedAxonPlasticity:2017,
	author = {Rafael G. Almeida and David A. Lyons},
	title = {{On Myelinated Axon Plasticity and Neuronal Circuit
	Formation and Function}},
	journal = {J. Neuroscience},
	year = {2017},
	volume = {37},
	issue = {42},
	pages ={10023--10034},
}


@book{RiekeBook:1997,
	author = {Rieke, Fred and Warland, David and de Ruyter van Steveninck, Rob and Bialek, William},
	title = {{Spikes: Exploring the Neural Code}},
	publisher = {The MIT Press},
	isbn = {978-0-262681087},
	edition = {2},
	year = {1997}
}


@book{SterlingPrinciples:2017,
	author = {Peter Sterling and Simon Laughlin},
	title = {{Principles of Neural Design}},
	publisher = {The MIT Press},
	isbn = {978-0-262-53468-0},
	edition = {1},
	year = {2017}
}
%	URL={https://www.frontiersin.org/article/10.3389/fninf.2017.00030},       
% DOI={10.3389/fninf.2017.00030},      
% ISSN={1662-5196},   


@ARTICLE{CodingMetaphor:2018,
	AUTHOR={Brette, R.},
	YEAR={2018},      
	 TITLE={{Is coding a relevant metaphor for the brain?}},
	 JOURNAL={The Behavioral and brain sciences},
	 VOLUME={42}, 
	 pages={e215},
	doi={10.1017/S0140525X19000049},
}

@ARTICLE{NeuralNetworkPerformance:2018,
	AUTHOR={van Albada, Sacha J. and Rowley, Andrew G. and Senk, Johanna and Hopkins, Michael and Schmidt, Maximilian and Stokes, Alan B. and Lester, David R. and Diesmann, Markus and Furber, Steve B.},   
	TITLE={{Performance Comparison of the Digital Neuromorphic Hardware SpiNNaker and the Neural Network Simulation Software NEST for a Full-Scale Cortical Microcircuit Model}},      
	JOURNAL={Frontiers in Neuroscience},      
	VOLUME={12},      
	PAGES={291},     
	YEAR={2018},      
	ABSTRACT={The digital neuromorphic hardware SpiNNaker has been developed with the aim of enabling
	large-scale neural network simulations in real time and with low power consumption. Real-time
	performance is achieved with 1 ms integration time steps, and thus applies to neural networks
	for which faster time scales of the dynamics can be neglected. By slowing down the simulation,
	shorter integration time steps and hence faster time scales, which are often biologically relevant,
	can be incorporated. We here describe the first full-scale simulations of a cortical microcircuit
	with biological time scales on SpiNNaker. Since about half the synapses onto the neurons arise
	within the microcircuit, larger cortical circuits have only moderately more synapses per neuron.
	Therefore, the full-scale microcircuit paves the way for simulating cortical circuits of arbitrary size.
	With approximately 80,000 neurons and 0.3 billion synapses, this model is the largest simulated on
	SpiNNaker to date. The scale-up is enabled by recent developments in the SpiNNaker software
	stack that allow simulations to be spread across multiple boards. Comparison with simulations
	using the NEST software on a high-performance cluster shows that both simulators can reach
	a similar accuracy, despite the fixed-point arithmetic of SpiNNaker, demonstrating the usability
	of SpiNNaker for computational neuroscience applications with biological time scales and large
	network size. The runtime and power consumption are also assessed for both simulators on the
	example of the cortical microcircuit model. To obtain an accuracy similar to that of NEST with
	0.1 ms time steps, SpiNNaker requires a slowdown factor of around 20 compared to real time.
	The runtime for NEST saturates around 3 times real time using hybrid parallelization with MPI
	and multi-threading. However, achieving this runtime comes at the cost of increased power and
	energy consumption. The lowest total energy consumption for NEST is reached at around 144
	parallel threads and 4.6 times slowdown. At this setting, NEST and SpiNNaker have a comparable
	energy consumption per synaptic event. Our results widen the application domain of SpiNNaker
	and help guide its development, showing that further optimizations such as synapse-centric
	network representation are necessary to enable real-time simulation of large biological neural
	networks.}
	
}



%%https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf
@InProceedings{SpatiotemporalLearning:2018,
	author="Xie, Saining
	and Sun, Chen
	and Huang, Jonathan
	and Tu, Zhuowen
	and Murphy, Kevin",
	editor="Ferrari, Vittorio
	and Hebert, Martial
	and Sminchisescu, Cristian
	and Weiss, Yair",
	title="Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
	booktitle="Computer Vision -- ECCV 2018",
	year="2018",
	publisher="Springer International Publishing",
	address="Cham",
	pages="318--335",
	abstract="Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level ``semantic'' features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).",
	isbn="978-3-030-01267-0"
}


@book{BuzsakiTheBrain:2019,
	author = {Gy\"orgy Buzs\'aki},
	title = {{The Brain from Inside Out}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-090538-5},
	edition = {1},
	year = {2019}
}


@article{SpikingSurvey:2013,
	author = {Stefan Schliebs and Nikola Kirilov Kasabov},
	year = {2013},
	title = {{Evolving spiking neural networks: A Survey}},
	journal = {Evolving Systems 4(2)},
	volume = {2},
	issue = {4},
	doi = {10.1007/s12530-013-9074-9}
}

@ARTICLE{CausalityNeuroscience:2016,
		author={Porta, Alberto and Faes, Luca},
	journal={Proceedings of the IEEE}, 
	title={{Wiener–Granger Causality in Network Physiology With Applications to Cardiovascular Control and Neuroscience}}, 
	year={2016},
	volume={104},
	number={2},
	pages={282-309},
	doi={10.1109/JPROC.2015.2476824}
}


%% Very good discussion of neuromorphic limitations
    
@INPROCEEDINGS{SpatiotemporalAction:2018,	
	author={D. {Tran} and H. {Wang} and L. {Torresani} and J. {Ray} and Y. {LeCun} and M. {Paluri}},	
	booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 	
	title={{A Closer Look at Spatiotemporal Convolutions for Action Recognition}}, 	
	year={2018},	
	volume={},	
	number={},
	pages={6450-6459},}


@book{NeuralInformationTheory:2018,
	author = {James V. Stone},
	title = {{Principles of Neural Information Theory}},
	publisher = {Sebtel Press, Sheffield, UK},
	isbn = {978-0-9933679-2-2},
	year = {2018}
}


@ARTICLE{SpikingNNLiquidSpace:2019,
	AUTHOR={Iranmehr, Ensieh and Shouraki, Saeed Bagheri and Faraji, Mohammad Mahdi and Bagheri, Nasim and Linares-Barranco, Bernabe},   
	TITLE={{Bio-Inspired Evolutionary Model of Spiking Neural Networks in Ionic Liquid Space}},      
	JOURNAL={Frontiers in Neuroscience},      
	VOLUME={13},      
	PAGES={1085},     
	YEAR={2019},      
	URL={https://www.frontiersin.org/article/10.3389/ fnins.2019.01085},       
	DOI={10.3389/fnins.2019.01085},      
	ISSN={1662-453X},   
	ABSTRACT={One of the biggest struggles while working with artificial neural networks is being able to come up with models which closely match biological observations. Biological neural networks seem to capable of creating and pruning dendritic spines, leading to synapses being changed, which results in higher learning capability. The latter forms the basis of the present study in which a new ionic model for reservoir-like networks, consisting of spiking neurons, is introduced. High plasticity of this model makes learning possible with a fewer number of neurons. In order to study the effect of the applied stimulus in an ionic liquid space through time, a diffusion operator is used which somehow compensates for the separation between spatial and temporal coding in spiking neural networks and therefore, makes the mentioned model suitable for spatiotemporal patterns. Inspired by partial structural changes in the human brain over the years, the proposed model evolves during the learning process. The effect of topological evolution on the proposed model's performance for some classification problems is studied in this paper. Several datasets have been used to evaluate the performance of the proposed model compared to the original LSM. Classification results via separation and accuracy values have shown that the proposed ionic liquid outperforms the original LSM.}
}


%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6276723/
%Two Forms of Electrical Transmission Between Neurons
%https://pubmed.ncbi.nlm.nih.gov/7480159/
%Nonsynaptic modulation of neuronal activity in the brain: electric currents and extracellular ions
https://onlinelibrary.wiley.com/doi/full/10.1002/jnr.24131
Energy‐efficient neural information processing in individual neurons and neuronal networks

@article{SynapsesInteraction:2014,
	author = {Alberto E. Pereda},
	title = {{Electrical synapses and their
	functional interactions with chemical
	synapses}},
	journal = {{Nature Reviews Neuroscience}},
	volume = {15},
	year = {2014},
	pages = {250--263},
} 


 (2014).  , 369(1644), 20130175. https://doi.org/10.1098/rstb.2013.0175


@ARTICLE{MirrorNeurons:2014,
	AUTHOR={Keysers, C.and Gazzola, V.},   
	TITLE={{Hebbian learning and predictive mirror neurons for actions, sensations and emotions.}},      
	JOURNAL={Phil. trans.  Royal Society of London. Series B, Biological sciences},      
	VOLUME={369},      
	PAGES={1644},     
	YEAR={2014},      
	DOI={ 10.1098/rstb.2013.0175},      
	ABSTRACT={Spike-timing-dependent plasticity is considered the neurophysiological basis of Hebbian learning and has been shown to be sensitive to both contingency and contiguity between pre- and postsynaptic activity. Here, we will examine how applying this Hebbian learning rule to a system of interconnected neurons in the presence of direct or indirect re-afference (e.g. seeing/hearing one's own actions) predicts the emergence of mirror neurons with predictive properties. In this framework, we analyse how mirror neurons become a dynamic system that performs active inferences about the actions of others and allows joint actions despite sensorimotor delays. We explore how this system performs a projection of the self onto others, with egocentric biases to contribute to mind-reading. Finally, we argue that Hebbian learning predicts mirror-like neurons for sensations and emotions and review evidence for the presence of such vicarious activations outside the motor system.}
}



@article{ActionPotentialTiming:2015,
	author = {Ford, M. and Alexandrova, O. and Cossell, L.},
	title = {{Node of Ranvier length as a potential regulator of myelinated axon conduction speed}},
	year = {2015},
	JOURNAL={Nat Commun},      
	VOLUME={6},      
	PAGES={8073},     
	url = {https://www.nature.com/articles/ncomms9073}
}



@MISC{RanvierLength:2017,
	author = {I. L. Arancibia-C\'arcamo and M. C. Ford and L. Cossell and K. Ishida and K. Tohyama and D. Attwell},
	title = {{Node of Ranvier length as a potential regulator of myelinated axon conduction speed}},
	year = {2017},
	DOI={doi:10.7554/eLife.23329},   
	JOURNAL={Elife},      
	VOLUME={6},      
	PAGES={e23329 },     
}
%	howpublished = {https://pubmed.ncbi.nlm.nih.gov/28130923/}


@ARTICLE{CoherentSpatioTemporal:2018,
	AUTHOR={Leandro M. Alonso1 and Marcelo O. Magnasco},   
	TITLE={{Complex spatiotemporal behavior and coherent excitations in critically-coupled chains of neural circuits}},      
	JOURNAL={ Chaos: An Interdisciplinary Journal of Nonlinear Science},      
	VOLUME={28},      
	PAGES={093102},     
	YEAR={2018},      
	URL={https://doi.org/10.1063/1.5011766},       
	DOI={ doi: 10.1063/1.5011766},      
	ABSTRACT={We investigate a critically-coupled chain of nonlinear oscillators, whose dynamics displays complex spatiotemporal patterns of activity, including regimes in which glider-like coherent excitations move about and interact. The units in the network are identical simple neural circuits whose dynamics is given by the Wilson-Cowan model and are arranged in space along a one-dimensional lattice with nearest neighbor interactions. The interactions follow an alternating sign rule, and hence the “synaptic matrix” M embodying them is tridiagonal antisymmetric and has purely imaginary (critical) eigenvalues. The model illustrates the interplay of two properties: circuits with a complex internal dynamics, such as multiple stable periodic solutions and period doubling bifurcations, and coupling with a “critical” synaptic matrix, i.e., having purely imaginary eigenvalues. In order to identify the dynamical underpinnings of these behaviors, we explored a discrete-time coupled-map lattice inspired by our system: the dynamics of the units is dictated by a chaotic map of the interval, and the interactions are given by allowing the critical coupling to act for a finite period τ, thus given by a unitary matrix U=exp(τ2M)
	. It is now explicit that such critical couplings are volume-preserving in the sense of Liouville’s theorem. We show that this map is also capable of producing a variety of complex spatiotemporal patterns including gliders, like our original chain of neural circuits. Our results suggest that if the units in isolation are capable of featuring multiple dynamical states, then local critical couplings lead to a wide variety of emergent spatiotemporal phenomena.
	}
}

 @article {WilliamsPhaseCoupling:1992,
 	author = {Williams, TL},
 	title = {Phase coupling by synaptic spread in chains of coupled neuronal oscillators},
 	volume = {258},
 	number = {5082},
 	pages = {662--665},
 	year = {1992},
 	doi = {10.1126/science.1411575},
 	publisher = {American Association for the Advancement of Science},
 	abstract = {Many neural systems behave as arrays of coupled oscillators, with characteristic phase coupling. For example, the rhythmic activation patterns giving rise to swimming in fish are characterized by a rostral-to-caudal phase delay in ventral root activity that is independent of the cycle duration. This produces a traveling wave of curvature along the body of the animal with a wavelength approximately equal to the body length. Here a simple mechanism for phase coupling in chains of equally activated oscillators is postulated: the synapses between the cells making up a "unit oscillator" are simply repeated in neighboring segments, with a reduced synaptic strength. If such coupling is asymmetric in the rostral and caudal directions, traveling waves of activity are produced. The intersegmental phase lag that develops is independent of the coupling strength over at least a tenfold range. Furthermore, for the unit oscillator believed to underlie central pattern generation in the lamprey spinal cord, such coupling can result in a phase lag that is independent of frequency.},
 	issn = {0036-8075},
 	eprint = {https://science.sciencemag.org/content/258/5082 /662.full.pdf},
 	journal = {Science}
 }
% 	URL = {https://science.sciencemag.org/content/258/5082/662},




@article{NeuralSelfReconfiguration:2017,
	title = "Shifting attention to dynamics: Self-reconfiguration of neural networks",
	journal = "Current Opinion in Systems Biology",
	volume = "3",
	pages = "132 - 140",
	year = "2017",
	issn = "2452-3100",
	doi = "https://doi.org/10.1016/j.coisb.2017.04.006",
	url = "http://www.sciencedirect.com/science/article/pii/ S2452310017300732",
	author = "Christoph Kirst and Carl D. Modes and Marcelo O. Magnasco",
}

@article {NeuronalAvalanches:2003,
	author = {Beggs, John M. and Plenz, Dietmar},
	title = {{Neuronal Avalanches in Neocortical Circuits}},
	volume = {23},
	number = {35},
	pages = {11167--11177},
	year = {2003},
	doi = {10.1523/JNEUROSCI.23-35-11167.2003},
	publisher = {Society for Neuroscience},
	abstract = {Networks of living neurons exhibit diverse patterns of activity, including oscillations, synchrony, and waves. Recent work in physics has shown yet another mode of activity in systems composed of many nonlinear units interacting locally. For example, avalanches, earthquakes, and forest fires all propagate in systems organized into a critical state in which event sizes show no characteristic scale and are described by power laws. We hypothesized that a similar mode of activity with complex emergent properties could exist in networks of cortical neurons. We investigated this issue in mature organotypic cultures and acute slices of rat cortex by recording spontaneous local field potentials continuously using a 60 channel multielectrode array. Here, we show that propagation of spontaneous activity in cortical networks is described by equations that govern avalanches. As predicted by theory for a critical branching process, the propagation obeys a power law with an exponent of -3/2 for event sizes, with a branching parameter close to the critical value of 1. Simulations show that a branching parameter at this value optimizes information transmission in feedforward networks, while preventing runaway network excitation. Our findings suggest that {\textquotedblleft}neuronal avalanches{\textquotedblright} may be a generic property of cortical networks, and represent a mode of activity that differs profoundly from oscillatory, synchronized, or wave-like network states. In the critical state, the network may satisfy the competing demands of information transmission and network stability.},
	issn = {0270-6474},
	eprint = {https://www.jneurosci.org/content/23/35/11167.full.pdf},
	journal = {Journal of Neuroscience}
}

@INbook{InformationTheoryIntersection:2008,
		author={Johnson, Don H.},
	booktitle={2008 IEEE Information Theory Workshop}, 
	title={Information theory and neuroscience: Why is the intersection so small?}, 
	year={2008},
	volume={},
	number={},
	pages={104-108},
	doi={10.1109/ITW.2008.4578631}
}

@article{PhenomenologicalSynapticPlasticity:2008,
	author={Morrison, A. and Diesmann, M. and Gerstner, W.},
	year ={2008},
	title = {{Phenomenological models of synaptic plasticity based on spike timing}},
	journal = {Biol Cybern},
	volume = {453/7191},
	number = {98},
	pages = {459--478},
	doi = {10.1007/s00422-008-0233-1},
}


@article{InformationNeuroscienceDimitrov:2011,
	author={Dimitrov, Alexander G. and
	Lazar, Aurel A. and Victor, Jonathan D.},
	year ={2011},
	title = {{Information theory in neuroscience}},
	journal = {Journal of Computational Neuroscience},
	volume = {30/1},
	pages = {1--5},
	doi={10.1007/s10827-011-0314-3},
}


@INPROCEEDINGS{PerformancePenaltySpinnaker:2013,
	author={Diehl, P.U. and Cook, M.},
	booktitle={In 2014 International Joint Conference on Neural Networks (IJCNN)}, 
	title={{Efficient implementation of STDP rules on SpiNNaker neuromorphic hardware}}, 
	year={2014},
	volume={},
	number={},	
	pages={4288–4295},
}
% https://doi.org/10.1109/IJCNN.2014.6889876.


@article{ConvertingNeuronalInformation:2014,
author = {Sengupta, B and Laughlin, SB and Niven, JE},
title = {{Consequences of Converting Graded to Action Potentials upon Neural Information Coding and Energy Efficiency}},
journal = { PLoS Comput Biol},
year = {2014},
doi = {10.1371/journal.pcbi.1003439},
volume = {1},
issue = {10},
}


@article{LogNormalBuzsaki:2014,
	author = {Buzs\'aki, Gy. and Mizuseki, K.},
	year = {2014},
	title = {The log-dynamic brain: how skewed distributions affect network operations},
	journal = {Nature Reviews Neuroscience},
	pages = {264-278},
	volume = {15},
	issue = {4},
	abstract = {At many physiological and anatomical levels in the brain, the distribution of numerous parameters is strongly skewed with a heavy tail and typically follows a lognormal distribution.The power and frequency relationship of brain oscillations is typically expressed in a log scale.Network synchrony, measured as a fraction of spiking neurons in a given time window, shows lognormal distribution in all brain states.Firing rates, spike bursts and synaptic weights follow a lognormal distribution. Importantly, these parameters remain correlated across brain states, environments and situations.The log-dynamic patterns of networks may be supported by the lognormal distribution of corticocortical connections strengths and axon diameters.A preconfigured, strongly connected minority of fast-firing neurons form the backbone of brain connectivity and serve as an ever-ready, fast-acting system. However, full performance of the brain also depends on the activity of very large numbers of weakly connected and slow-firing majority of neurons.},
	doi = {10.1038/nrn3687},
}


%	URL = {https://www.jneurosci.org/content/23/35/11167},

@INPROCEEDINGS{ReviewAhmed:2015,
	author={M. R. {Ahmed} and B. K. {Sujatha}},
	booktitle={2015 International Conference on Communications and Signal Processing (ICCSP)}, 
	title={A review on methods, issues and challenges in neuromorphic engineering}, 
	year={2015},
	volume={},
	number={},	
	pages={0899-0903},
}


@article{MechanicalWaves:2015,
	author = {El Hady, Ahmed and Machta, Benjamin B.},
	title = {{Mechanical surface waves accompany action potential propagation}},
	journal = {Nature Communications},
	volume = {6},
	issue = {1},
	year = {2019},
	pages = {6697},
	doi = "10.1038/ncomms7697",
	abstract = {Many diverse studies have shown that a mechanical displacement of the axonal membrane accompanies the electrical pulse defining the action potential (AP). We present a model for these mechanical displacements as arising from the driving of surface wave modes in which potential energy is stored in elastic properties of the neuronal membrane and cytoskeleton while kinetic energy is carried by the axoplasmic fluid. In our model, these surface waves are driven by the travelling wave of electrical depolarization characterizing the AP, altering compressive electrostatic forces across the membrane. This driving leads to co-propagating mechanical displacements, which we term Action Waves (AWs). Our model allows us to estimate the shape of the AW that accompanies any travelling wave of voltage, making predictions that are in agreement with results from several experimental systems. Our model can serve as a framework for understanding the physical origins and possible functional roles of these AWs.}
} 

@ARTICLE{NeuronalVariability:2017,
	author = {Li, M. and  Tsien, J.Z.},
	title = {{Neural Code-Neural Self-information Theory on How Cell-Assembly Code Rises from Spike Time and Neuronal Variability}},
	journal = {Frontiers in Cell Neuroscience},
	year = {2017},
	doi = {10.3389/fncel.2017.00236},
} 

@ARTICLE{BiologicalConservationLaw:2017,
	author = {Boris Podobnik and Marko Jusup  and Zoran Tiganj and Wen-Xu Wangi and Javier M. Buld and H. Eugene Stanley},
	title = {Biological conservation law as an emerging
	functionality in dynamical neuronal networks},
	journal = {Applied Physical Sciences},
	doi = {10.1073/pnas.1705704114},
	year = {2017},
	volume = {45},
	issue = {114},
	pages = {11826-11831},
}


@misc{NeuromorphicSchumanSurvey:2017,
	title={{A Survey of Neuromorphic Computing and Neural Networks in Hardware}},
	author={Catherine D. Schuman and Thomas E. Potok and Robert M. Patton and J. Douglas Birdwell and Mark E. Dean and Garrett S. Rose and James S. Plank},
	lab ={Computational  Data  Analytics  Group,  OakRidge   National   Laboratory},
	year={2017},
	howpublished={\url{https://arxiv.org/abs/1705.06963} (Accessed on Sep 10, 2022)},
}
%	author={Catherine D. Schuman and Thomas E. Potok and Robert M. Patton and J. Douglas Birdwell and Mark E. Dean and Garrett S. Rose and James S. Plank},


@ARTICLE{EstimatingInformationSingleSpikingNeuron:2017,	
	AUTHOR={Zeldenrust, Fleur and de Knecht, Sicco and Wadman, Wytse J. and Den\'eve, Sophie and Gutkin, Boris},   
	TITLE={{Estimating the Information Extracted by a Single Spiking Neuron from a Continuous Input Time Series}},      	
	JOURNAL={Frontiers in Computational Neuroscience},      	
	VOLUME={11},      	
	PAGES={49},     	
	YEAR={2017},      		
	DOI={10.3389/fncom.2017.00049},      	
	ABSTRACT={Understanding the relation between (sensory) stimuli and the activity of neurons (i.e., “the neural code”) lies at heart of understanding the computational properties of the brain. However, quantifying the information between a stimulus and a spike train has proven to be challenging. We propose a new (in vitro) method to measure how much information a single neuron transfers from the input it receives to its output spike train. The input is generated by an artificial neural network that responds to a randomly appearing and disappearing “sensory stimulus”: the hidden state. The sum of this network activity is injected as current input into the neuron under investigation. The mutual information between the hidden state on the one hand and spike trains of the artificial network or the recorded spike train on the other hand can easily be estimated due to the binary shape of the hidden state. The characteristics of the input current, such as the time constant as a result of the (dis)appearance rate of the hidden state or the amplitude of the input current (the firing frequency of the neurons in the artificial network), can independently be varied. As an example, we apply this method to pyramidal neurons in the CA1 of mouse hippocampi and compare the recorded spike trains to the optimal response of the “Bayesian neuron” (BN). We conclude that like in the BN, information transfer in hippocampal pyramidal cells is non-linear and amplifying: the information loss between the artificial input and the output spike train is high if the input to the neuron (the firing of the artificial network) is not very informative about the hidden state. If the input to the neuron does contain a lot of information about the hidden state, the information loss is low. Moreover, neurons increase their firing rates in case the (dis)appearance rate is high, so that the (relative) amount of transferred information stays constant.}
}
%	URL={https://www.frontiersin.org/article/10.3389/fncom.2017.00049}     



@ARTICLE{NeuralSelfInformationTheory:2017,	
	AUTHOR={Li, Meng and Tsien, Joe Z.},   	
	TITLE={{Neural Code—Neural Self-information Theory on How Cell-Assembly Code Rises from Spike Time and Neuronal Variability}},      	
	JOURNAL={Frontiers in Cellular Neuroscience},      	
	VOLUME={11},      	
	YEAR={2017},      	
	URL={https://www.frontiersin.org/article/10.3389/fncel.2017.00236},       	
	DOI={10.3389/fncel.2017.00236},      	
	ISSN={1662-5102},   	
	ABSTRACT={A major stumbling block to cracking the real-time neural code is neuronal variability - neurons discharge spikes with enormous variability not only across trials within the same experiments but also in resting states. Such variability is widely regarded as a noise which is often deliberately averaged out during data analyses. In contrast to such a dogma, we put forth the Neural Self-Information Theory that neural coding is operated based on the self-information principle under which variability in the time durations of inter-spike-intervals (ISI), or neuronal silence durations, is self-tagged with discrete information. As the self-information processor, each ISI carries a certain amount of information based on its variability-probability distribution; higher-probability ISIs which reflect the balanced excitation-inhibition state convey minimal information, whereas lower-probability ISIs which signify rare-occurrence surprisals in the form of extremely transient or prolonged silence carry most information. These variable silence durations are naturally coupled with intracellular biochemical cascades, energy equilibrium and dynamic regulation of protein and gene expression levels. As such, this silence variability-based self-information code is completely intrinsic to the neurons themselves, with no need for outside observers to set any reference point as typically used in the rate code, population code and temporal code models. Moreover, temporally coordinated ISI surprisals across cell population can inherently give rise to robust real-time cell-assembly codes which can be readily sensed by the downstream neural clique assemblies. One immediate utility of this self-information code is a general decoding strategy to uncover a variety of cell-assembly patterns underlying external and internal categorical or continuous variables in an unbiased manner.}
}



@article{BrainNetworkModels:2018,
	author = {Bassett, D. S. and Zurn, P. and Gold, J. I.},
	year ={ 2018},
	title = {On the nature and use of models in network neuroscience},
	journal = {Nature Reviews Neuroscience},
	pages = {566-578},
	volume = {19},
	issue =  {9},
	abstract ={ Network theory provides an intuitively appealing framework for studying relationships among interconnected brain mechanisms and their relevance to behaviour. As the space of its applications grows, so does the diversity of meanings of the term network model. This diversity can cause confusion, complicate efforts to assess model validity and efficacy, and hamper interdisciplinary collaboration. In this Review, we examine the field of network neuroscience, focusing on organizing principles that can help overcome these challenges. First, we describe the fundamental goals in constructing network models. Second, we review the most common forms of network models, which can be described parsimoniously along the following three primary dimensions: from data representations to first-principles theory; from biophysical realism to functional phenomenology; and from elementary descriptions to coarse-grained approximations. Third, we draw on biology, philosophy and other disciplines to establish validation principles for these models. We close with a discussion of opportunities to bridge model types and point to exciting frontiers for future pursuits.},
	doi = { 10.1038/s41583-018-0038-8}
}

@article{RecurrentExcitation:2018,
	author = {Seeman, SC and Campagnola, L and Davoudian, PA and Hoggarth, A and Hage, TA and Bosma-Moody, A and Baker, CA and Lee, JH and Mihalas, S and Teeter, C and Ko, AL and Ojemann, JG and Gwinn, RP and Silbergeld, DL and Cobbs, C and Phillips, J and  Lein, E and %Murphy, G and Koch, C and Zeng, H and Jarsky, T},
	title = {Sparse recurrent excitatory connectivity in the microcircuit of the adult mouse and human cortex},
	journal = {Elife},
	year = {2018},
	page = {e37349},
	doi = {10.7554/eLife.37349}
}

@article{FromCellsToCircuits:2018,
	author = {Richards, SEV and Van Hooser, SD},
	title = {{Neural architecture: from cells to circuits.}},
	journal = {J Neurophysiol.},
	year = {2018},
	volume = {2},
	issue = {120},
	pages = {854-866},
	doi = {10.1152/jn.00044.2018}
}

@article{RelativisticBuzsaki:2019,
	title={Preexisting hippocampal network dynamics constrain optogenetically induced place fields},
	author={Sam McKenzie and Roman Husz\'ar and Daniel F. English and Kanghwan Kim and Euisik Yoon and Gy\"orgy Buzs\'aki},
	journal={Neuron},
	volume={109},
	issue={6},
	year={2021},
	doi={10.1101/803577},
}

@misc{UnderstandNeuralNetwork:2019,
	title={What does it mean to understand a neural network?},
	author={Timothy P. Lillicrap and Konrad P. Kording},
	year={2019},
	eprint={1907.06374},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@Article{InformationTheoryNeuroscience:2019,
	AUTHOR = {Piasini, Eugenio and Panzeri, Stefano},
	TITLE = {Information Theory in Neuroscience},
	JOURNAL = {Entropy},
	VOLUME = {21},
	YEAR = {2019},
	NUMBER = {1},
	ARTICLE-NUMBER = {62},
	URL = {https://www.mdpi.com/1099-4300/21/1/62},
	ISSN = {1099-4300},
	ABSTRACT = {This is the Editorial article summarizing the scope and contents of the Special Issue, Information Theory in Neuroscience.},
	DOI = {10.3390/e21010062}
}

@article{IntelligenceBySpikes:2019,
	  author = {{Augusto, E and Gambino, F}},
	  title = {{Can NMDA Spikes Dictate Computations of Local Networks and Behavior?}},
	  journal = {Front Mol Neurosci.},
	  volume = {12},
	  issue = {238},
	  doi = {10.3389/fnmol.2019.00238},
}

@article{ChallengingPointNeuron:2019,
	author = {Tzilivaki, A and Kastellakis, G and Poirazi, P},
	title = {{Challenging the point neuron dogma: FS basket cells as 2-stage nonlinear integrators}},
	journal = { Nat Commun.},
	year = {2019},
	volume = {10},
	issue = {1},
	page = {3664},
	doi = {10.1038/s41467-019-11537-7}
}
	

@article{SynapticClustering:2019,
	author = {Kastellakis, G and Poirazi, P},
	title = {{Synaptic Clustering and Memory Formation}},
	journal = {{Front Mol Neurosci}},
	year = {2019},
	volume = {12},
	issue = {300},
	doi = {10.3389/fnmol.2019.00300},
}


@article{InformationTheoryAbused:2019,
	title={Information theory is abused in neuroscience},
	author={Nizami, Lance},
	journal={Cybernetics \& Human Knowing},
	volume={26},
	number={4},
	pages={47--97},
	year={2019},
	publisher={Imprint Academic}
}


@misc{LearningFrequencyDomain:2020,
	title={Learning in the Frequency Domain},
	author={Kai Xu and Minghai Qin and Fei Sun and Yuhao Wang and Yen-Kuang Chen and Fengbo Ren},
	year={2020},
		booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	archivePrefix={arXiv},
	pages = {(Accessed on Sep 10, 2022)},
	howpublished={\url{https://arxiv.org/abs/2002.12416} },
	primaryClass={cs.CV},
		note = {Accessed: 2023-08-30}
	
}
%	eprint={2002.12416},
% 
InProceedings{LearningFrequencyDomain:2020,
%	author = {Xu, Kai and Qin, Minghai and Sun, Fei and Wang, Yuhao and Chen, Yen-Kuang and Ren, Fengbo},
%	title = {Learning in the Frequency Domain},
%	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
%	month = {June},
%	year = {2020}
%} 


% https://link.springer.com/article/10.1007/s11265-020-01558-7
%Online learning can be implemented in software, which also results in a performance penalty [16]
% https://www.researchgate.net/publication/342408921_Inference_with_Artificial_Neural_Networks_on_the_Analog_BrainScaleS-2_Hardware


@misc{HowNeuronsCommunicate,
	title = {How neurons communicate},
	howpublished = {\url{https://openstax.org/books/biology/pages/35-2-how-neurons-communicate}},
		note = {Accessed: 2023-08-30}
}


@article{InformationTheorySystemsBiology:2018,
	author ={Uda, Shinsuke},
 year = {2020},
title = {Application of information theory in systems biology},
journal =  {Biophysical Reviews},
pages = {377-384},
volume = {12},
isue = {2},
abstract = {Over recent years, new light has been shed on aspects of information processing in cells. The quantification of information, as described by Shannon’s information theory, is a basic and powerful tool that can be applied to various fields, such as communication, statistics, and computer science, as well as to information processing within cells. It has also been used to infer the network structure of molecular species. However, the difficulty of obtaining sufficient sample sizes and the computational burden associated with the high-dimensional data often encountered in biology can result in bottlenecks in the application of information theory to systems biology. This article provides an overview of the application of information theory to systems biology, discussing the associated bottlenecks and reviewing recent work.},
doi={10.1007/s12551-020-00665-w}
}

@article{NeuronPreciseTiming:2021,
author = {Singh, P. and Sahoo, P. and Saxena, K. and  Manna,  J. S. and Ray, K. and  Kanad K. S.}, 
title = {{Cytoskeletal filaments deep inside a neuron are not
	silent: They regulate the precise timing of nerve spikes using a pair of
	vortices}},
 journal =  {Symmetry},
 volume = {13},
 year = {2021},
 doi = {10.3390/sym13050821},
} 


@article{BuzsakiActiveState:2021,
	title={Distinct ground state and activated state modes of spiking in forebrain neurons},
	author={Levenstein, Daniel and Girardeau, Gabrielle and Gornet, Jonathan and Grosmark, Andres and Huszar, Roman and Peyrache, Adrien and Senzai, Yuta and Watson, Brendon and Rinzel, John and Buzs{\'a}ki, Gy{\"o}rgy},
	journal={bioRxiv, \url{https://www.biorxiv.org/content/10.1101/2021.09.20.461152v3.full.pdf}},
	year={2021},
	publisher={Cold Spring Harbor Laboratory},
}

@article{SymmetryPerception:2021,
	title={Symmetry perception with spiking neural networks},
	author={George, Jonathan K and Soci, Cesare and Miscuglio, Mario and Sorger, Volker J},
	journal={Scientific Reports},
	volume={11},
	issue={1},
	page={5776},
	year={2021},
	doi={10.1038/s41598-021-85232-3},
	abstract={Mirror symmetry is an abundant feature in both nature and technology. Its successful detection is critical for perception procedures based on visual stimuli and requires organizational processes. Neuromorphic computing, utilizing brain-mimicked networks, could be a technology-solution providing such perceptual organization functionality, and furthermore has made tremendous advances in computing efficiency by applying a spiking model of information. Spiking models inherently maximize efficiency in noisy environments by placing the energy of the signal in a minimal time. However, many neuromorphic computing models ignore time delay between nodes, choosing instead to approximate connections between neurons as instantaneous weighting. With this assumption, many complex time interactions of spiking neurons are lost. Here, we show that the coincidence detection property of a spiking-based feed-forward neural network enables mirror symmetry. Testing this algorithm exemplary on geospatial satellite image data sets reveals how symmetry density enables automated recognition of man-made structures over vegetation. We further demonstrate that the addition of noise improves feature detectability of an image through coincidence point generation. The ability to obtain mirror symmetry from spiking neural networks can be a powerful tool for applications in image-based rendering, computer graphics, robotics, photo interpretation, image retrieval, video analysis and annotation, multi-media and may help accelerating the brain-machine interconnection. More importantly it enables a technology pathway in bridging the gap between the low-level incoming sensor stimuli and high-level interpretation of these inputs as recognized objects and scenes in the world.},
	publisher={Cold Spring Harbor Laboratory}
}


@InProceedings{NeuralChannelCapacity:2021,
	author="Ye, Gen and Lin, Tong",
	editor="Farka{\v{s}}, Igor
	and Masulli, Paolo
	and Otte, Sebastian
	and Wermter, Stefan",
	title="Channel Capacity of Neural Networks",
	booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2021",
	year="2021",
	publisher="Springer International Publishing",
	address="Cham",
	pages="253--265",
	abstract="Occam's Razor principle suggests preference for simpler models and triggers an enduring question: what is the proper definition of complexity of a model? In this work, we regard neural networks as communication channels and measure the complexity of neural networks by means of their channel capacity---the maximum information reserved in the output of a neural network. Furthermore, we show a connection between the L2-norm of the weight matrix of the linear model and its channel capacity through the singular values of the weight matrix. On image classification problems, we find regularizing different neural networks by constraining their channel capacity effectively boosts the generalization performance and outperforms other information-theoretic regularization methods.",
	isbn="978-3-030-86380-7"
}


@article{SpikingNeuralThreads:2021,
author = {Susi, Gianluca and Garc\'es, Pilar and Paracone, Emanuele and Cristini, Alessandro and
Salerno, Mario and Maestú, Fernando and Pereda, Ernesto},
year = {2021},
title = {{FNS allows efficient event-driven spiking neural network simulations based on a neuron model supporting spike latency}},
journal = {Nature Scientific Reports},
page = {12160},
volume =  {11},
issue = {1},
abstract = {Neural modelling tools are increasingly employed to describe, explain, and predict the human brain’s behavior. Among them, spiking neural networks (SNNs) make possible the simulation of neural activity at the level of single neurons, but their use is often threatened by the resources needed in terms of processing capabilities and memory. Emerging applications where a low energy burden is required (e.g. implanted neuroprostheses) motivate the exploration of new strategies able to capture the relevant principles of neuronal dynamics in reduced and efficient models. The recent Leaky Integrate-and-Fire with Latency (LIFL) spiking neuron model shows some realistic neuronal features and efficiency at the same time, a combination of characteristics that may result appealing for SNN-based brain modelling. In this paper we introduce FNS, the first LIFL-based SNN framework, which combines spiking/synaptic modelling with the event-driven approach, allowing us to define heterogeneous neuron groups and multi-scale connectivity, with delayed connections and plastic synapses. FNS allows multi-thread, precise simulations, integrating a novel parallelization strategy and a mechanism of periodic dumping. We evaluate the performance of FNS in terms of simulation time and used memory, and compare it with those obtained with neuronal models having a similar neurocomputational profile, implemented in NEST, showing that FNS performs better in both scenarios. FNS can be advantageously used to explore the interaction within and between populations of spiking neurons, even for long time-scales and with a limited hardware configuration.},
doi = {10.1038/s41598-021-91513-8},
}


@article{EnergyNeuralCommunication:2021,
	author = {William B Levy  and Victoria G. Calvert },
	title = {{Communication consumes 35 times more energy than computation in the human cortex, but both costs are needed to predict synapse number}},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {118},
	number = {18},
	pages = {e2008173118},
	year = {2021},
	doi = {10.1073/pnas.2008173118},
	abstract = {Darwinian evolution tends to produce energy-efficient outcomes. On the other hand, energy limits computation, be it neural and probabilistic or digital and logical. Taking a particular energy-efficient viewpoint, we define neural computation and make use of an energy-constrained computational function. This function can be optimized over a variable that is proportional to the number of synapses per neuron. This function also implies a specific distinction between adenosine triphosphate (ATP)-consuming processes, especially computation per se vs. the communication processes of action potentials and transmitter release. Thus, to apply this mathematical function requires an energy audit with a particular partitioning of energy consumption that differs from earlier work. The audit points out that, rather than the oft-quoted 20 W of glucose available to the human brain, the fraction partitioned to cortical computation is only 0.1 W of ATP [L. Sokoloff, Handb. Physiol. Sect. I Neurophysiol. 3, 1843–1864 (1960)] and [J. Sawada, D. S. Modha, “Synapse: Scalable energy-efficient neurosynaptic computing” in Application of Concurrency to System Design (ACSD) (2013), pp. 14–15]. On the other hand, long-distance communication costs are 35-fold greater, 3.5 W. Other findings include 1) a 108-fold discrepancy between biological and lowest possible values of a neuron’s computational efficiency and 2) two predictions of N, the number of synaptic transmissions needed to fire a neuron (2,500 vs. 2,000).}}

%	URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2008173118},
@article{NeuronGame:2022,
	author = {Brett J. Kagan et al},
	title = {{In vitro neurons learn and exhibit sentience when embodied in a simulated game-world}},
	volume = {110},
	journal = {Neuron},
	ISSUE = {23},
	pages = {3952-3969},
	year = {2022},
}

@article{BrainMasterPlan:2022,
	journal = {Nature},
	author =  {Mehonic, A. and Kenyon, A. J.},
	title = {{Brain-inspired computing needs a master plan}},
	volume= {604},
	issue = {7905},
	pages = {255-260},
	abstract = 
{ New computing technologies inspired by the brain promise fundamentally different ways to process information with extreme energy efficiency and the ability to handle the avalanche of unstructured and noisy data that we are generating at an ever-increasing rate. To realize this promise requires a brave and coordinated plan to bring together disparate research communities and to provide them with the funding, focus and support needed. We have done this in the past with digital technologies; we are in the process of doing it with quantum technologies; can we now do it for brain-inspired computing?
}, 
doi = {10.1038/s41586-021-04362-w},
}

@article{NewInformationStream:2022,
	author = {Hedrick, NG and Lu, Z and Bushong, E and Singhi, S and Nguyen, P and Maga\~na, Y and Jilani, S and Lim, BK and Ellisman, M and Komiyama, T.},
  title = {{Learning binds new inputs into functional synaptic clusters via spinogenesis}},
   journal = {Nat Neurosci},
   year = {2022},
    volume = {6},
    issue = {25},
    pages = {726-737},
     doi = {10.1038/s41593-022-01086-6},
}

@article{SynapticModificationsLearning:2022,
	author = {{Ma, S and Zuo, Y}},
	title = {{Synaptic modifications in learning and memory - A dendritic spine story}},
	journal = {Semin Cell Dev Biol.},
	year = {2022},
	volume = {125},
	page = {84-90},
	doi = {10.1016/j.semcdb.2021.05.015},
}
