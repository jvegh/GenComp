
%% The neuronal- related citations


@book{
	HebbBook:1949,
	author = {Hebb, D.O.},
	title = {{The Organization of Behavior}},
	publisher = {New York: Wiley and Sons},
	year = {1949}
}

@article{NeuromorphicSystems:1990,
	author = {Carver Mead},
	title = {{Neuromorphic electronic systems}},
	journal = {{Proc. IEEE}},
	volume = {78},
	year = {1990},
	issn = {1936-7406},
	pages = {1629--1636},
} 
%	doi = {10.1146/annurev-neuro-062111-150444},


@article {FireWireTogether:1992,
	author = {Lowel, S and Singer, W},
	title = {Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity},
	volume = {255},
	number = {5041},
	pages = {209--212},
	year = {1992},
	doi = {10.1126/science.1372754},
	publisher = {American Association for the Advancement of Science},
	abstract = {In the visual cortex of the brain, long-ranging tangentially oriented axon collaterals interconnect regularly spaced clusters of cells. These connections develop after birth and attain their specificity by pruning. To test whether there is selective stabilization of connections between those cells that exhibit correlated activity, kittens were raised with artificially induced strabismus (eye deviation) to eliminate the correlation between signals from the two eyes. In area 17, cell clusters were driven almost exclusively from either the right or the left eye and tangential intracortical fibers preferentially connected cell groups activated by the same eye. Thus, circuit selection depends on visual experience, and the selection criterion is the correlation of activity.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/255/5041/209},
	eprint = {https://science.sciencemag.org/content/255/5041/209.full.pdf},
	journal = {Science}
}

@article{SpatiotemporalPlenz:1996,
	author = {D. Plenz and A. Aertsen},
	title = {{Neural dynamics in cortex-striatum ci-cultures -- II. Spatiotemporal Characteristics of Neural Activity}},
	journal = {{Neuroscience}},
	volume = {70/4},
	year = {1996},
	pages = {893--924},
} 

@article{ConductanceBySynapticActivity:1998,
	author = {Tim A. Benke and Andreas L\"uthi and John T. R. Isaac and Graham L. Collingridge},
	title = {{Modulation of AMPA receptor
	unitary conductance
	by synaptic activity}},
	journal = {{Nature}},
	volume = {393},
	year = {1998},
	issn = {793-797},
	pages = {1629--1636},
} 

@article{SpatiotemporalPrut:1998,
	author = {Yifat Prut and Eilon Vaadia and Hagai Bergman and Iris Haalman and Hamutal Slovin and
	Moshe Abeles},
	title = {{Spatiotemporal Structure of Cortical Activity: Properties and
	Behavioral Relevance}},
	journal = {{J. Neurophysiol}},
	volume = {79},
	year = {1998},
	pages = {2857--2874},
} 



@book{
KochBiophysics:1999,
	author = {Christof Koch},
	title = {{Biophysics of Computation}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-518199-9},
	year = {1999}
}

@article{KochVoltageDependentConductance:1999,
	author = {Stemmler, M. and Koch, C.},
	title = {{How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate}},
	journal = {{Nat Neurosci}},
	volume = {2},
	year = {1999},
	pages = {521–527},
	howpublished = {https://doi.org/10.1038/9173}
} 

@book{
	JohstonWuNeurophysiology:1995,
	author = {Daniel Johnston and Samuel Miao-sin Wu},
	title = {{Foundations of Cellular Neurophysiology}},
	publisher = {Massachusetts Institute of Technology},
	isbn = {978-0-262-10053-3},
	year = {1995}
}


@article{PerturbationNeuralComputation:2002,
author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
journal = {Neural Computation},
volume = {14},
number = {11},
pages = {2531-2560},
year = {2002},
doi = {10.1162/089976602760407955},
	
URL = { 
https://doi.org/10.1162/089976602760407955},
eprint = { 
	https://doi.org/10.1162/089976602760407955
	},
abstract = { A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology. }
}

@article{BrainClocking:2005,
	author = {{Antle, M. C. and Silver, R.}},
	title = {{ Orchestrating time: arrangements of the brain circadian clock}},
	journal = {{Trends Neurosci.}},
	volume = {28},
	year = {2015},
	pages = {145–151},
} 


@article{FurberNeuralEngineering:2007,
	author = {{Steve Furber and Steve Temple}},
	title = {{Neural systems engineering}},
	journal = {{J. R. Soc. Interface}},
	volume = {4},
	year = {2007},
	pages = {193--206},
	numpages = {14},
	doi = {10.1098/rsif.2006.0177},
} 

@article{HebbianLearningRule:2008,
	author = {Caporale, Natalia and Dan, Yang},
	title = {Spike Timing–Dependent Plasticity: A Hebbian Learning Rule},
	journal = {Annual Review of Neuroscience},
	volume = {31},
	number = {1},
	pages = {25-46},
	year = {2008},
	doi = {10.1146/annurev.neuro.31.060407.125639},
	note ={PMID: 18275283},
	
	URL = { 
	https://doi.org/10.1146/annurev.neuro.31.060407.125639
	
	},
	eprint = { 
	https://doi.org/10.1146/annurev.neuro.31.060407.125639
	
	}
	,
	abstract = { Spike timing–dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo. }
}


@article{TransientResponses:2008,
	author = {{Khorsand P and Chance F }},
	title = {{Transient Responses to Rapid Changes in Mean and Variance in Spiking Models}},
	journal = {{PLoS ONE}},
	volume = {3},
	number = {11},
	year = {208},
	pages = {e3786},
	articleno = {19},
	numpages = {29},
	doi = {doi.org/10.1371/journal.pone.0003786},
} 


@article{BuzsakiGammaOscillations:2012,
	author = {{Gy\"orgy Buzs\'aki and Xiao-Jing Wang}},
	title = {{Mechanisms of Gamma Oscillations}},
	journal = {{Annual Reviews of Neurosciences}},
	volume = {3},
	number = {4},
	month = {nov},
	year = {2012},
	issn = {1936-7406},
	pages = {19:1--19:29},
	articleno = {19},
	numpages = {29},
	doi = {10.1146/annurev-neuro-062111-150444},
} 
%	keywords = {},

@INPROCEEDINGS{TrueNorth:2016, author={J. {Sawada~et~al}
	},
	 booktitle={SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	 title={{TrueNorth Ecosystem for Brain-Inspired Computing: Scalable Systems, Software, and Applications}}, year={2016}, volume={}, number={}, pages={130-141},
	 } 
	%and F. {Akopyan} and A. S. {Cassidy} and B. {Taba} and M. V. {Debole} and P. {Datta} and R. %{Alvarez-Icaza} and A. {Amir} and J. V. {Arthur} and A. {Andreopoulos} and R. {Appuswamy} and H. %{Baier} and D. {Barch} and D. J. {Berg} and C. {Di Nolfo} and S. K. {Esser} and M. {Flickner} %and T. A. {Horvath} and B. L. {Jackson} and J. {Kusnitz} and S. {Lekuch} and M. {Mastro} and T. %{Melano} and P. A. {Merolla} and S. E. {Millman} and T. K. {Nayak} and N. {Pass} and H. E. %{Penner} and W. P. {Risk} and K. {Schleupen} and B. {Shaw} and H. {Wu} and B. {Giera} and A. T. %{Moody} and N. {Mundhenk} and B. C. {Van Essen} and E. X. {Wang} and D. P. {Widemann} and Q. %{Wu} and W. E. {Murphy} and J. K. {Infantolino} and J. A. {Ross} and D. R. {Shires} and M. M. %{Vindiola} and R. {Namburu} and D. S. {Modha}


@article{IntelLoihi:2018,
	author = {{M. Davies, et al}},
	title = {{Loihi:
	{\small A Neuromorphic Manycore Processor with On-Chip Learning}}},
	journal = { IEEE Micro},
	year = {2018},
	volume = {38},
	issue = {1},
	pages ={82--99},
	keywords = {neuromorphic computing,machine learning,artificial intelligence},
}
%        doi = {https://doi.org/10.1109/MM.2018.112130359}


@article{MyelinatedAxonPlasticity:2017,
	author = {Rafael G. Almeida and David A. Lyons},
	title = {{On Myelinated Axon Plasticity and Neuronal Circuit
	Formation and Function}},
	journal = {J. Neuroscience},
	year = {2017},
	volume = {37},
	issue = {42},
	pages ={10023--10034},
}


@book{SterlingPrinciples:2017,
	author = {Peter Sterling and Simon Laughlin},
	title = {{Principles of Neural Design}},
	publisher = {The MIT Press},
	isbn = {978-0-262-53468-0},
	edition = {1},
	year = {2017}
}



%%https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf

@book{BuzsakiTheBrain:2019,
	author = {Gy\"orgy Buzs\'aki},
	title = {{The Brain from Inside Out}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-090538-5},
	edition = {1},
	year = {2019}
}

@book{BuzsakiRhythms:2006,
	author = {Gy\"orgy Buzs\'aki},
	title = {{Rhythms of the Brain}},
	publisher = {Oxford University Press},
	isbn = {978-0-19-530106-9},
	edition = {1},
	year = {2006}
}

@book{PrinciplesNeuralScience:2013,
	author = {Eric R. Kandel and James H. Schwartz and Thomas M. Jessell and Steven A. Siegelbaum abd A. J. Hudspeth },
	title = {{Principles of Neural Science}},
	publisher = {The McGraw-Hill},
	isbn = {978-0-07-18101-2},
	edition = {5},
	year = {2013}
}


@INPROCEEDINGS{ConditionalComputationBengion:2016,
	author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precu},
	booktitle={ICLR'16: },
	title={{Conditional Computation in Neural Networks for faster models}}, year={2016},
	eprint={1511.06297},
archivePrefix={arXiv},
primaryClass={cs.LG},
url={https://arxiv.org/pdf/1511.06297},
} 

@InProceedings{SpatiotemporalLearning:2018,
	author="Xie, Saining
	and Sun, Chen
	and Huang, Jonathan
	and Tu, Zhuowen
	and Murphy, Kevin",
	editor="Ferrari, Vittorio
	and Hebert, Martial
	and Sminchisescu, Cristian
	and Weiss, Yair",
	title="Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
	booktitle="Computer Vision -- ECCV 2018",
	year="2018",
	publisher="Springer International Publishing",
	address="Cham",
	pages="318--335",
	abstract="Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level ``semantic'' features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).",
	isbn="978-3-030-01267-0"
}

@article{LosonczyIntegrative:2006,
author = {Losonczy, A. and Magee, J.C.},
	year = {2006},
 title = {{Integrative properties of radial oblique
dendrites in hippocampal CA1 pyramidal neurons}},
 journal = {Neuron},
 volume = {50},
 pages = {291-307}
}

@article{SpikingSurvey:2013,
	author = {Stefan Schliebs and Nikola Kirilov Kasabov},
	year = {2013},
	title = {{Evolving spiking neural networks: A Survey}},
	journal = {Evolving Systems 4(2)},
	volume = {2},
	issue = {4},
	doi = {10.1007/s12530-013-9074-9}
}
    
@INPROCEEDINGS{SpatiotemporalAction:2018,	
	author={D. {Tran} and H. {Wang} and L. {Torresani} and J. {Ray} and Y. {LeCun} and M. {Paluri}},	
	booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 	
	title={A Closer Look at Spatiotemporal Convolutions for Action Recognition}, 	
	year={2018},	
	volume={},	
	number={},
	pages={6450-6459},}

@article{RecipeMemristor:2020,
	author = {Chicca,E.  and Indiveri,G. },
	title = {{A recipe for creating ideal hybrid memristive-CMOS neuromorphic processing systems}},
	journal = {Applied Physics Letters},
	volume = {116},
	number = {12},
	pages = {120501},
	year = {2020},
	doi = {10.1063/1.5142089},
	
	URL = { 
	https://doi.org/10.1063/1.5142089},
	eprint = { 
	https://doi.org/10.1063/1.5142089
	
	},
	ABSTRACT={The development of memristive device technologies has reached a level of maturity to enable the design and fabrication of complex and large-scale hybrid memristive-Complementary Metal-Oxide Semiconductor (CMOS) neural processing systems. These systems offer promising solutions for implementing novel in-memory computing architectures for machine learning and data analysis problems. We argue that they are also ideal building blocks for integration in neuromorphic electronic circuits suitable for ultra-low power brain-inspired sensory processing systems, therefore leading to innovative solutions for always-on edge-computing and Internet-of-Things applications. Here, we present a recipe for creating such systems based on design strategies and computing principles inspired by those used in mammalian brains. We enumerate the specifications and properties of memristive devices required to support always-on learning in neuromorphic computing systems and to minimize their power consumption. Finally, we discuss in what cases such neuromorphic systems can complement conventional processing ones and highlight the importance of exploiting the physics of both the memristive devices and the CMOS circuits interfaced to them.}
	
}

@article{NatureBuildingBrain:2020,
	title = {{Building brain-inspired computing}},
	journal = {Nature Communications},
	lab = {Nature Communications},
	volume = {10},
	number = {12},
	pages = {4838},
	year = {2019},
	URL = { 
	https://doi.org/10.1038/s41467-019-12521-x}
}
%doi = {10.1063/1.5142089},

@article{BrainInspiredBlocks:2020,
	author = {Jack D. Kendall and Suhas Kumar},
	title = {{The building blocks of a brain-inspired computer}},
	journal = {Appl. Phys. Rev.},
	year = {2020},
	volume = {7},
	pages ={011305},
	doi = {10.1063/1.5129306}
}


@article{AIcoreProgressStalled:2020,
	author = {Matthew Hutson},
	title = {{Core progress in AI has stalled in some fields}},
	journal = {Science},
	year = {2020},
	volume = {368},
	pages ={6494/927},
	doi = {10.1126/science.368.6494.927}
}

@ARTICLE{SpikingNNLiquidSpace:2019,
AUTHOR={Iranmehr, Ensieh and Shouraki, Saeed Bagheri and Faraji, Mohammad Mahdi and Bagheri, Nasim and Linares-Barranco, Bernabe},   
TITLE={Bio-Inspired Evolutionary Model of Spiking Neural Networks in Ionic Liquid Space},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={13},      
PAGES={1085},     
YEAR={2019},      
URL={https://www.frontiersin.org/article/10.3389/ fnins.2019.01085},       
DOI={10.3389/fnins.2019.01085},      
ISSN={1662-453X},   
ABSTRACT={One of the biggest struggles while working with artificial neural networks is being able to come up with models which closely match biological observations. Biological neural networks seem to capable of creating and pruning dendritic spines, leading to synapses being changed, which results in higher learning capability. The latter forms the basis of the present study in which a new ionic model for reservoir-like networks, consisting of spiking neurons, is introduced. High plasticity of this model makes learning possible with a fewer number of neurons. In order to study the effect of the applied stimulus in an ionic liquid space through time, a diffusion operator is used which somehow compensates for the separation between spatial and temporal coding in spiking neural networks and therefore, makes the mentioned model suitable for spatiotemporal patterns. Inspired by partial structural changes in the human brain over the years, the proposed model evolves during the learning process. The effect of topological evolution on the proposed model's performance for some classification problems is studied in this paper. Several datasets have been used to evaluate the performance of the proposed model compared to the original LSM. Classification results via separation and accuracy values have shown that the proposed ionic liquid outperforms the original LSM.}
}

%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6276723/
%Two Forms of Electrical Transmission Between Neurons
%https://pubmed.ncbi.nlm.nih.gov/7480159/
%Nonsynaptic modulation of neuronal activity in the brain: electric currents and extracellular ions
https://onlinelibrary.wiley.com/doi/full/10.1002/jnr.24131
Energy‐efficient neural information processing in individual neurons and neuronal networks

@article{SynapsesInteraction:2014,
	author = {Alberto E. Pereda},
	title = {{Electrical synapses and their
	functional interactions with chemical
	synapses}},
	journal = {{Nature Reviews | Neuroscience}},
	volume = {15},
	year = {2014},
	pages = {250--263},
} 


 (2014).  , 369(1644), 20130175. https://doi.org/10.1098/rstb.2013.0175


@ARTICLE{MirrorNeurons:2014,
	AUTHOR={Keysers, C.and Gazzola, V.},   
	TITLE={{Hebbian learning and predictive mirror neurons for actions, sensations and emotions.}},      
	JOURNAL={Phil. trans.  Royal Society of London. Series B, Biological sciences},      
	VOLUME={369},      
	PAGES={1644},     
	YEAR={2014},      
	DOI={ 10.1098/rstb.2013.0175},      
	ABSTRACT={Spike-timing-dependent plasticity is considered the neurophysiological basis of Hebbian learning and has been shown to be sensitive to both contingency and contiguity between pre- and postsynaptic activity. Here, we will examine how applying this Hebbian learning rule to a system of interconnected neurons in the presence of direct or indirect re-afference (e.g. seeing/hearing one's own actions) predicts the emergence of mirror neurons with predictive properties. In this framework, we analyse how mirror neurons become a dynamic system that performs active inferences about the actions of others and allows joint actions despite sensorimotor delays. We explore how this system performs a projection of the self onto others, with egocentric biases to contribute to mind-reading. Finally, we argue that Hebbian learning predicts mirror-like neurons for sensations and emotions and review evidence for the presence of such vicarious activations outside the motor system.}
}


@ARTICLE{RoleOfMyelinPlasticity:2014,
	AUTHOR={Pajevic S, Basser PJ, Fields RD.},   
	TITLE={{Role of myelin plasticity in oscillations and synchrony of neuronal activity}},      
	JOURNAL={Neuroscience},      
	VOLUME={13},      
	PAGES={135-147},     
	YEAR={2014},      
	DOI={ 10.1016/j.neuroscience.2013.11.007},      
	ABSTRACT={Conduction time is typically ignored in computational models of neural network function. Here we consider the effects of conduction delays on the synchrony of neuronal activity and neural oscillators, and evaluate the consequences of allowing conduction velocity (CV) to be regulated adaptively. We propose that CV variation, mediated by myelin, could provide an important mechanism of activity-dependent nervous system plasticity. Even small changes in CV, resulting from small changes in myelin thickness or nodal structure, could have profound effects on neuronal network function in terms of spike-time arrival, oscillation frequency, oscillator coupling, and propagation of brain waves. For example, a conduction delay of 5ms could change interactions of two coupled oscillators at the upper end of the gamma frequency range (∼100Hz) from constructive to destructive interference; delays smaller than 1ms could change the phase by 30°, significantly affecting signal amplitude. Myelin plasticity, as another form of activity-dependent plasticity, is relevant not only to nervous system development but also to complex information processing tasks that involve coupling and synchrony among different brain rhythms. We use coupled oscillator models with time delays to explore the importance of adaptive time delays and adaptive synaptic strengths. The impairment of activity-dependent myelination and the loss of adaptive time delays may contribute to disorders where hyper- and hypo-synchrony of neuronal firing leads to dysfunction (e.g., dyslexia, schizophrenia, epilepsy)}
}
%	URL={https://www.frontiersin.org/article/10.3389/ fnins.2019.01085},       



@MISC{ActionPotentialTiming:2015,
	author = {Ford, M. and Alexandrova, O. and Cossell, L.},
	title = {{Node of Ranvier length as a potential regulator of myelinated axon conduction speed}},
	year = {2015},
	JOURNAL={Nat Commun },      
	VOLUME={6},      
	PAGES={8073},     
	howpublished = {https://www.nature.com/articles/ncomms9073}
}

@MISC{RanvierLength:2017,
	author = {I. L. Arancibia-Cárcamo and M. C. Ford and L. Cossell and K. Ishida and K. Tohyama and D. Attwell},
	title = {{Node of Ranvier length as a potential regulator of myelinated axon conduction speed}},
	year = {2017},
	DOI={doi:10.7554/eLife.23329},   
	JOURNAL={Elife},      
	VOLUME={6},      
	PAGES={e23329 },     
	howpublished = {https://pubmed.ncbi.nlm.nih.gov/28130923/}
}


@ARTICLE{CoherentSpatioTemporal:2018,
	AUTHOR={Leandro M. Alonso1 and Marcelo O. Magnasco},   
	TITLE={{Complex spatiotemporal behavior and coherent excitations in critically-coupled chains of neural circuits}},      
	JOURNAL={ Chaos: An Interdisciplinary Journal of Nonlinear Science},      
	VOLUME={28},      
	PAGES={093102},     
	YEAR={2018},      
	URL={https://doi.org/10.1063/1.5011766},       
	DOI={ doi: 10.1063/1.5011766},      
	ABSTRACT={We investigate a critically-coupled chain of nonlinear oscillators, whose dynamics displays complex spatiotemporal patterns of activity, including regimes in which glider-like coherent excitations move about and interact. The units in the network are identical simple neural circuits whose dynamics is given by the Wilson-Cowan model and are arranged in space along a one-dimensional lattice with nearest neighbor interactions. The interactions follow an alternating sign rule, and hence the “synaptic matrix” M embodying them is tridiagonal antisymmetric and has purely imaginary (critical) eigenvalues. The model illustrates the interplay of two properties: circuits with a complex internal dynamics, such as multiple stable periodic solutions and period doubling bifurcations, and coupling with a “critical” synaptic matrix, i.e., having purely imaginary eigenvalues. In order to identify the dynamical underpinnings of these behaviors, we explored a discrete-time coupled-map lattice inspired by our system: the dynamics of the units is dictated by a chaotic map of the interval, and the interactions are given by allowing the critical coupling to act for a finite period τ, thus given by a unitary matrix U=exp(τ2M)
	. It is now explicit that such critical couplings are volume-preserving in the sense of Liouville’s theorem. We show that this map is also capable of producing a variety of complex spatiotemporal patterns including gliders, like our original chain of neural circuits. Our results suggest that if the units in isolation are capable of featuring multiple dynamical states, then local critical couplings lead to a wide variety of emergent spatiotemporal phenomena.
	}
}


 @article {HalfNodeRanvier:2005,
author = {Gina E. Sosinsky and Thomas J. Deerinck and Rocco Greco and Casey H. Buitenhuys and Thomas M. Bartol and Mark H. Ellisman},
title = {{Small Nodes of Ranvier From Peripheral Nerves of MiceReconstructed by Electron Tomography}},
subtitle = {{Development of a Model for MicrophysiologicalSimulations}},
journal = {Neuroinformatics},
doi = {10.1385/NI:03:02:133},
pages = {33–162},
year = {2005},
volume = {3},
}


 
 @article {WilliamsPhaseCoupling:1992,
 	author = {Williams, TL},
 	title = {Phase coupling by synaptic spread in chains of coupled neuronal oscillators},
 	volume = {258},
 	number = {5082},
 	pages = {662--665},
 	year = {1992},
 	doi = {10.1126/science.1411575},
 	publisher = {American Association for the Advancement of Science},
 	abstract = {Many neural systems behave as arrays of coupled oscillators, with characteristic phase coupling. For example, the rhythmic activation patterns giving rise to swimming in fish are characterized by a rostral-to-caudal phase delay in ventral root activity that is independent of the cycle duration. This produces a traveling wave of curvature along the body of the animal with a wavelength approximately equal to the body length. Here a simple mechanism for phase coupling in chains of equally activated oscillators is postulated: the synapses between the cells making up a "unit oscillator" are simply repeated in neighboring segments, with a reduced synaptic strength. If such coupling is asymmetric in the rostral and caudal directions, traveling waves of activity are produced. The intersegmental phase lag that develops is independent of the coupling strength over at least a tenfold range. Furthermore, for the unit oscillator believed to underlie central pattern generation in the lamprey spinal cord, such coupling can result in a phase lag that is independent of frequency.},
 	issn = {0036-8075},
 	eprint = {https://science.sciencemag.org/content/258/5082 /662.full.pdf},
 	journal = {Science}
 }
% 	URL = {https://science.sciencemag.org/content/258/5082/662},

@article{NeuralSelfReconfiguration:2017,
	title = "Shifting attention to dynamics: Self-reconfiguration of neural networks",
	journal = "Current Opinion in Systems Biology",
	volume = "3",
	pages = "132 - 140",
	year = "2017",
	issn = "2452-3100",
	doi = "https://doi.org/10.1016/j.coisb.2017.04.006",
	url = "http://www.sciencedirect.com/science/article/pii/ S2452310017300732",
	author = "Christoph Kirst and Carl D. Modes and Marcelo O. Magnasco",
}

@article {NeuronalAvalanches:2003,
	author = {Beggs, John M. and Plenz, Dietmar},
	title = {Neuronal Avalanches in Neocortical Circuits},
	volume = {23},
	number = {35},
	pages = {11167--11177},
	year = {2003},
	doi = {10.1523/JNEUROSCI.23-35-11167.2003},
	publisher = {Society for Neuroscience},
	abstract = {Networks of living neurons exhibit diverse patterns of activity, including oscillations, synchrony, and waves. Recent work in physics has shown yet another mode of activity in systems composed of many nonlinear units interacting locally. For example, avalanches, earthquakes, and forest fires all propagate in systems organized into a critical state in which event sizes show no characteristic scale and are described by power laws. We hypothesized that a similar mode of activity with complex emergent properties could exist in networks of cortical neurons. We investigated this issue in mature organotypic cultures and acute slices of rat cortex by recording spontaneous local field potentials continuously using a 60 channel multielectrode array. Here, we show that propagation of spontaneous activity in cortical networks is described by equations that govern avalanches. As predicted by theory for a critical branching process, the propagation obeys a power law with an exponent of -3/2 for event sizes, with a branching parameter close to the critical value of 1. Simulations show that a branching parameter at this value optimizes information transmission in feedforward networks, while preventing runaway network excitation. Our findings suggest that {\textquotedblleft}neuronal avalanches{\textquotedblright} may be a generic property of cortical networks, and represent a mode of activity that differs profoundly from oscillatory, synchronized, or wave-like network states. In the critical state, the network may satisfy the competing demands of information transmission and network stability.},
	issn = {0270-6474},
	eprint = {https://www.jneurosci.org/content/23/35/11167.full.pdf},
	journal = {Journal of Neuroscience}
}

%	URL = {https://www.jneurosci.org/content/23/35/11167},

@misc{LearningFrequencyDomain:2020,
	title={Learning in the Frequency Domain},
	author={Kai Xu and Minghai Qin and Fei Sun and Yuhao Wang and Yen-Kuang Chen and Fengbo Ren},
	year={2020},
	eprint={2002.12416},
	archivePrefix={arXiv},
	howpublished={https://arxiv.org/abs/2002.12416},
	primaryClass={cs.CV}
}

@article{NeuromorphicSchumanSurvey:2017,
	title={{A Survey of Neuromorphic Computing and Neural Networks in Hardware}},
	author={{C.D. Schuman et.al}},
	lab ={Computational  Data  Analytics  Group,  OakRidge   National   Laboratory},
	year={2017},
	howpublished={https://arxiv.org/abs/1705.06963},
	url={https://arxiv.org/abs/1705.06963},
	primaryClass={cs.NE}
}
%	author={Catherine D. Schuman and Thomas E. Potok and Robert M. Patton and J. Douglas Birdwell and Mark E. Dean and Garrett S. Rose and James S. Plank},

@misc{UnderstandNeuralNetwork:2019,
	title={What does it mean to understand a neural network?},
	author={Timothy P. Lillicrap and Konrad P. Kording},
	year={2019},
	eprint={1907.06374},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{AddressEvent:1999,
	author = {K.   Boahen},
	title = {{Point-to-Point   Connectivity   Between   Neuromorphic   Chips   UsingAddress Events}},
	journal = {IEEE Trans. on Circuits and Systems Part},
	volume = {47},
	year = {2000},
	pages = {416-434},
}



@INPROCEEDINGS{PerformancePenaltySpinnaker:2013,
	author={Diehl, P.U. and Cook, M.},
	booktitle={In 2014 International Joint Conference on Neural Networks (IJCNN)}, 
	title={Efficient implementation of STDP rules on SpiNNaker neuromorphic hardware}, 
	year={2014},
	volume={},
	number={},	
	pages={4288–4295},
}
% https://doi.org/10.1109/IJCNN.2014.6889876.

@article{NeuromorphicSpikesAI:2019,
	author = {Roy, K. and Jaiswal, A. and Panda, P.},
	title = {{Towards spike-based machine intelligence with neuromorphic computing.}},
	journal = {Nature},
	volume = {575},
	year = {2019},
	pages = {607–617},
	doi = "https://doi.org/10.1038/s41586-019-1677-2",
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.}
} 

@INPROCEEDINGS{ReviewAhmed:2015,
		author={M. R. {Ahmed} and B. K. {Sujatha}},
		booktitle={2015 International Conference on Communications and Signal Processing (ICCSP)}, 
		title={A review on methods, issues and challenges in neuromorphic engineering}, 
		year={2015},
		volume={},
		number={},	
	pages={0899-0903},
}


@misc{RelativisticBuzsaki:2019,
	title={Preexisting hippocampal network dynamics constrain optogenetically induced place fields},
	author={Sam McKenzie and Roman Husz\'ar and Daniel F. English and Kanghwan Kim and Euisik Yoon and György Buzsáki},
	year={2019},
	doi={10.1101/803577},
	howpublished={10.1101/803577},
}

@article{MissingMemristor:2008,
	author={Strukov, Dmitri B. and Snider, Gregory S. and Stewart, Duncan R. and Williams, R. Stanley},
	year ={2008},
	title = {{The missing memristor found}},
	journal = {Natures},
volume = {453/7191},
pages = {80–83},
}

@article{PhysicsForNeuromorhicComputing:2020,
	author = {Danijela Markovic and Alice Mizrahi and Damien Querlioz and Julie Grollier},
	title = {{Physics for neuromorphic computing}},
	journal = {Nature Reviews Physics},
	volume = {2},
	year = {2020},
	pages = {499--510},
	doi = "https://www.nature.com/articles/s42254-020-0208-2.pdf"
}

@article{VerificationBrainScaleS:2020,
	author = {Andreas Grubl and Sebastian Billaudelle and Benjamin Cramer and Vitali Karasenko and Johannes Schemmel 
	},
	title = {{Verification and Design Methods for the BrainScaleS Neuromorphic Hardware System}},
	journal = {Journal of Signal Processing Systems},
	volume = {92},
	year = {2020},
	pages = {1277--1292},
	doi = "https://www.nature.com/articles/s42254-020-0208-2.pdf"
}
% https://link.springer.com/article/10.1007/s11265-020-01558-7
%Online learning can be implemented in software, which also results in a performance penalty [16]
% https://www.researchgate.net/publication/342408921_Inference_with_Artificial_Neural_Networks_on_the_Analog_BrainScaleS-2_Hardware


@article{EuroBrainImplosion:2020,
	author = {Alison Abbott},
	title = {{Documentary follows implosion of billion-euro brain project}},
	journal = {Nature},
	volume = {588},
	year = {2020},
	pages = {215-216},
	doi = "10.1038/d41586-020-03462-3"
}

@misc{HowNeuronsCommunicate,
  title = {How neurons communicate},
  howpublished = {\url{https://openstax.org/books/biology/pages/35-2-how-neurons-communicate}},
  note = {Accessed: 2021-03-20}
}

@MISC{stop-calling-everything-ai:2020,
	author = {Kathy Pretz},
	title = {{Stop Calling Everything AI, Machine-Learning Pioneer Says}},
	year = {2021},
	JOURNAL={IEEE Spectrum},      
	howpublished = {https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says}
}

@MISC{RevolutionHasnotHappenedYet:2019,
	author = { Jordan, M. I.},
	title = {{Artificial Intelligence—The Revolution Hasn’t Happened Yet}},
	year = {2019},
	issues = {1},
	JOURNAL={Harvard Data Science Review},      
	howpublished = {https://doi.org/10.1162/99608f92.f06c6e61}
}

%https://www.nature.com/articles/s41598-019-54215-w

%doi: https://doi.org/10.1038/d41586-020-03462-3

% https://www.ncbi.nlm.nih.gov/books/NBK92848/